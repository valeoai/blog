<style type="text/css">
h1 {
    margin-top:0.5em !important;
    margin-bottom:0.25em !important;
}

h3 {
    /*line-height: normal;*/
    line-height: 100%;
    /*margin-top:0.2em !important;*/
    margin-bottom:0.2em !important;
}


p.b {
  font-size: 95%;
  /*font-style: italic;*/
  color: #808080;
  /*margin-top: 0.2em;
  margin-bottom: 0.2em;*/
  margin-top: 0;
  margin-bottom: 0;
  /*line-height: 40%;*/
  /*line-height: 98%;*/
  line-height: normal;
}

p.c {
  font-size: 95%;
  font-style: italic;
  color: #808080;
  /*margin-top: 0.2em;
  margin-bottom: 0.2em;*/
  margin-top: 0;
  margin-bottom: 0;
  /*line-height: 40%;*/
  /*line-height: 100%;*/
  line-height: normal;
}

</style>

<h1 id="projects">Projects</h1>

<div class="row">
    <div class="col-md-4">
        <div class="pubteaserbs">
            <a href="https://valeoai.github.io/blog/projects/multi-sensor">
                <img class="media-object" src="../images/projects/valeo_sensors.jpg" alt="images/projects/valeo_sensors.jpg project teaser" />
           </a>
        </div>
    </div>
    <div class="col-md-8">
        <div class="pubitembs">
          <h3><a href="https://valeoai.github.io/blog/projects/multi-sensor">Multi-sensor perception</a></h3>
          <p>Automated driving relies first on a diverse range of sensors, like Valeo’s <a href="https://www.valeo.com/en/360-vue/">fish-eye cameras</a>, <a href="https://www.valeo.com/en/valeo-scala/">LiDARs</a>, radars and <a href="https://www.valeo.com/en/ultrasonic-parking-sensors/">ultrasonics</a>. Exploiting at best the outputs of each of these sensors at any instant is fundamental to understand the complex environment of the vehicle and gain robustness. To this end, we explore various machine learning approaches where sensors are considered either in isolation (as radar in <a href="https://arxiv.org/abs/2005.01456">Carrada</a> at ICPR’20) or collectively (as in <a href="https://valeoai.github.io/blog/publications/xmuda/">xMUDA</a> at CVPR’20).</p>
        </div>
</div>
</div>

<hr />

<div class="row">
    <div class="col-md-4">
        <div class="pubteaserbs">
            <a href="https://valeoai.github.io/blog/projects/3d-perception">
                <img class="media-object" src="../images/projects/3d_perception.jpg" alt="images/projects/3d_perception.jpg project teaser" />
           </a>
        </div>
    </div>
    <div class="col-md-8">
        <div class="pubitembs">
          <h3><a href="https://valeoai.github.io/blog/projects/3d-perception">3D perception</a></h3>
          <p>Each sensor delivers information about the 3D world around the vehicle. Making sense of this information in terms of drivable space and important objects (road users, curb, obstacles, street furnitures) in 3D is required for the driving system to plan and act in the safest and most confortable way. This encompasses several challenging tasks, in particular detection and segmentation of objects in point clouds as in <a href="https://valeoai.github.io/blog/publications/fkaconv/">FKAConv</a> at ACCV’20.</p>
        </div>
</div>
</div>

<hr />

<div class="row">
    <div class="col-md-4">
        <div class="pubteaserbs">
            <a href="https://valeoai.github.io/blog/projects/limited-supervision">
                <img class="media-object" src="../images/projects/limited_supervision.jpg" alt="images/projects/limited_supervision.jpg project teaser" />
           </a>
        </div>
    </div>
    <div class="col-md-8">
        <div class="pubitembs">
          <h3><a href="https://valeoai.github.io/blog/projects/limited-supervision">Frugal learning</a></h3>
          <p>Collecting diverse enough data, and annotating it precisely, is complex, costly and time-consuming. To reduce dramatically these needs, we explore various alternatives to fully-supervised learning, e.g, training that is unsupervised (as <a href="https://valeoai.github.io/blog/publications/rosd/">rOSD</a> at ECCCV’20), self-supervised (as <a href="https://valeoai.github.io/blog/publications/bownet/">BoWNet</a> at CVPR’20), semi-supervised, active, zero-shot (as <a href="https://valeoai.github.io/blog/publications/zs3/">ZS3</a> at NeurIPS’19) or few-shot. We also investigate training with fully-synthetic data (in combination with unsupervised domain adaptation) and with GAN-augmented data (as <a href="https://valeoai.github.io/blog/publications/semanticpalette/">Semantic Palette</a> at CVPR’21).</p>
        </div>
</div>
</div>

<hr />

<div class="row">
    <div class="col-md-4">
        <div class="pubteaserbs">
            <a href="https://valeoai.github.io/blog/projects/domain-adaptation">
                <img class="media-object" src="../images/projects/domain_adaptation.jpg" alt="images/projects/domain_adaptation.jpg project teaser" />
           </a>
        </div>
    </div>
    <div class="col-md-8">
        <div class="pubitembs">
          <h3><a href="https://valeoai.github.io/blog/projects/domain-adaptation">Domain adaptation</a></h3>
          <p>Deep learning and reinforcement learning are key technologies for autonomous driving. One of the challenges they face is to adapt to conditions which differ from those met during training. To improve systems’ performance in such situations, we explore so-called “domain adaptation” techniques, as in <a href="https://valeoai.github.io/blog/publications/advent/">AdvEnt</a> at CVPR’19 and <a href="https://valeoai.github.io/blog/publications/dada/">DADA</a> its extension at ICCV’19. We propose new solutions to more practical DA scenarios in <a href="https://valeoai.github.io/blog/publications/mtaf/">MTAF</a> (ICCV'21) to handle multiple target domains and in <a href="https://valeoai.github.io/blog/publications/buda/">BUDA</a> (CVIU'21) to handle new target classes. In <a href="https://valeoai.github.io/blog/publications/xmuda/">xMUDA</a> (CVPR'20), we introduce a new framework to tackle the challenging adaptation problem on both 2D image and 3D point-cloud spaces.</p>
        </div>
</div>
</div>

<hr />

<div class="row">
    <div class="col-md-4">
        <div class="pubteaserbs">
            <a href="https://valeoai.github.io/blog/projects/reliability">
                <img class="media-object" src="../images/projects/uncertainty.jpg" alt="images/projects/uncertainty.jpg project teaser" />
           </a>
        </div>
    </div>
    <div class="col-md-8">
        <div class="pubitembs">
          <h3><a href="https://valeoai.github.io/blog/projects/reliability">Reliability</a></h3>
          <p>When the unexpected happens, when the weather badly degrades, when a sensor gets blocked, the embarked perception system should diagnose the situation and react accordingly, <em>e.g.,</em> by calling an alternative system or the human driver. With this in mind, we investigate ways to improve the robustness of neural nets to input variations, including to adversarial attacks, and to predict automatically the performance and the confidence of their predictions as in <a href="https://valeoai.github.io/blog/publications/confidnet">ConfidNet</a> at NeurIPS’19.</p>
        </div>
</div>
</div>

<hr />

<div class="row">
    <div class="col-md-4">
        <div class="pubteaserbs">
            <a href="https://valeoai.github.io/blog/projects/driving">
                <img class="media-object" src="../images/projects/driving.jpg" alt="images/projects/driving.jpg project teaser" />
           </a>
        </div>
    </div>
    <div class="col-md-8">
        <div class="pubitembs">
          <h3><a href="https://valeoai.github.io/blog/projects/driving">Driving in action</a></h3>
          <p>Getting from sensory inputs to car control goes either through a modular stack (perception &gt; localization &gt; forecast &gt; planning &gt; actuation) or, more radically, through a single end-to-end model. We work on both strategies, more specificaly on action forecasting, automatic interpretation of decisions taken by a driving system, and reinforcement / imitation learning for end-to-end systems (as in <a href="https://valeoai.github.io/blog/publications/e2e-rl-driving/">RL work</a> at CVPR’20).</p>
        </div>
</div>
</div>

<hr />

<div class="row">
    <div class="col-md-4">
        <div class="pubteaserbs">
            <a href="https://valeoai.github.io/blog/projects/deep-learning">
                <img class="media-object" src="../images/projects/deep_learning.jpg" alt="images/projects/deep_learning.jpg project teaser" />
           </a>
        </div>
    </div>
    <div class="col-md-8">
        <div class="pubitembs">
          <h3><a href="https://valeoai.github.io/blog/projects/deep-learning">Core Deep Learning</a></h3>
          <p>Deep learning being now a key component of AD systems, it is important to get a better understanding of its inner workings, in particular the link between the specifics of the learning optimization and the key properities (performance, regularity, robustness, generalization) of the trained models. Among other things, we investigate the impact of popular batch normalization on standard learning procedures and the ability to learn through unsupervised distillation.</p>
        </div>
</div>
</div>

<hr />

<div class="row">
    <div class="col-md-4">
        <div class="pubteaserbs">
            <a href="https://valeoai.github.io/blog/projects/explainability">
                <img class="media-object" src="../images/projects/logo_explainable.png" alt="images/projects/logo_explainable.png project teaser" />
           </a>
        </div>
    </div>
    <div class="col-md-8">
        <div class="pubitembs">
          <h3><a href="https://valeoai.github.io/blog/projects/explainability">Interpretability and Explainability of Deep Models</a></h3>
          <p>The concept of explainability has several facets and the need for explainability is strong in safety-critical applications such as autonomous driving where deep learning models are now widely used. As the underlying mechanisms of these models remain opaque, explainability and trustworthiness have become major concerns. Among other things, we investigate methods providing explanations to a black-box visual-based systems in a post-hoc fashion, as well as approaches that aim at building more interpretable self-driving systems by design.</p>
        </div>
</div>
</div>


