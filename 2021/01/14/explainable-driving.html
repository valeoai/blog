<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>How can we make driving systems explainable? | valeo.ai blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="How can we make driving systems explainable?" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Hedi Ben younes, Eloi Zablocki" />
<meta property="og:description" content="Hedi Ben younes, Eloi Zablocki" />
<link rel="canonical" href="https://valeoai.github.io/blog/2021/01/14/explainable-driving.html" />
<meta property="og:url" content="https://valeoai.github.io/blog/2021/01/14/explainable-driving.html" />
<meta property="og:site_name" content="valeo.ai blog" />
<meta property="og:image" content="https://valeoai.github.io/blog/images/posts/advent/qualitative_results.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-14T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Hedi Ben younes, Eloi Zablocki","headline":"How can we make driving systems explainable?","dateModified":"2021-01-14T00:00:00-06:00","datePublished":"2021-01-14T00:00:00-06:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://valeoai.github.io/blog/2021/01/14/explainable-driving.html"},"image":"https://valeoai.github.io/blog/images/posts/advent/qualitative_results.png","url":"https://valeoai.github.io/blog/2021/01/14/explainable-driving.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://valeoai.github.io/blog/feed.xml" title="valeo.ai blog" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-179942144-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>How can we make driving systems explainable? | valeo.ai blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="How can we make driving systems explainable?" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Hedi Ben younes, Eloi Zablocki" />
<meta property="og:description" content="Hedi Ben younes, Eloi Zablocki" />
<link rel="canonical" href="https://valeoai.github.io/blog/2021/01/14/explainable-driving.html" />
<meta property="og:url" content="https://valeoai.github.io/blog/2021/01/14/explainable-driving.html" />
<meta property="og:site_name" content="valeo.ai blog" />
<meta property="og:image" content="https://valeoai.github.io/blog/images/posts/advent/qualitative_results.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-14T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Hedi Ben younes, Eloi Zablocki","headline":"How can we make driving systems explainable?","dateModified":"2021-01-14T00:00:00-06:00","datePublished":"2021-01-14T00:00:00-06:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://valeoai.github.io/blog/2021/01/14/explainable-driving.html"},"image":"https://valeoai.github.io/blog/images/posts/advent/qualitative_results.png","url":"https://valeoai.github.io/blog/2021/01/14/explainable-driving.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://valeoai.github.io/blog/feed.xml" title="valeo.ai blog" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-179942144-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">valeo.ai blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/projects/">Projects</a><a class="page-link" href="/blog/publications/">Publications</a><a class="page-link" href="/blog/about/">About</a><a class="page-link" href="/blog/categories/">Tags</a><a class="page-link" href="/blog/search/">Search</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <!-- <style type="text/css">
.post-title {
 font-size: 42px !important; 
}
</style>
 -->

<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">How can we make driving systems explainable?</h1><p class="page-description">Hedi Ben younes, Eloi Zablocki</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-01-14T00:00:00-06:00" itemprop="datePublished">
        Jan 14, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      29 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#explainability">explainability</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#interpretability">interpretability</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#survey">survey</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#self-driving">self-driving</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#contextual-elements">Contextual elements</a>
<ul>
<li class="toc-entry toc-h3"><a href="#why-do-we-need-explainable-driving-models">Why do we need explainable driving models?</a></li>
<li class="toc-entry toc-h3"><a href="#explainability-">Explainability ?</a></li>
<li class="toc-entry toc-h3"><a href="#driving-system-">Driving system ?</a></li>
<li class="toc-entry toc-h3"><a href="#the-challenges-of-explainability-of-neural-driving-systems">The challenges of explainability of neural driving systems</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#post-hoc-explanation">Post-hoc explanation</a>
<ul>
<li class="toc-entry toc-h3"><a href="#local-explanations">Local explanations</a>
<ul>
<li class="toc-entry toc-h4"><a href="#saliency-methods">Saliency methods</a></li>
<li class="toc-entry toc-h4"><a href="#local-approximations">Local approximations</a></li>
<li class="toc-entry toc-h4"><a href="#counterfactual-explanations">Counterfactual explanations</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#global-explanations">Global explanations</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#desigining-an-explainable-driving-model">Desigining an explainable driving model</a>
<ul>
<li class="toc-entry toc-h3"><a href="#input-level-explanations">Input-level explanations</a>
<ul>
<li class="toc-entry toc-h4"><a href="#attention-based-models">Attention-based models</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#use-case-generating-natural-language-explanations">Use-case: generating natural language explanations</a></li>
<li class="toc-entry toc-h2"><a href="#conclusion">Conclusion</a></li>
<li class="toc-entry toc-h2"><a href="#references">References</a></li>
</ul><!-- Research on autonomous vehicles is blooming thanks to recent advances in deep learning and computer vision \citep{imagenet_classification,deeplearning_nature}, as well as the development of autonomous driving datasets and simulators \citep{kitti,carla,YuCWXCLMD20}.
The number of academic publications on this subject is rising in most machine learning, computer vision, robotics and transportation conferences, and journals.
On the industry side, several manufacturers are already producing cars equipped with advanced computer vision technologies for automatic lane following, assisted parking, or collision detection among other things. Meanwhile, constructors are working on and designing prototypes with level 4 and 5 autonomy.
The development of autonomous vehicles has the potential to reduce congestions, fuel consumption, and crashes, and it can increase personal mobility and save lives given that nowadays the vast majority of car crashes are caused by human error \citep{anderson2014autonomous}.

The first steps in the development of autonomous driving systems are taken with the collaborative European project PROMETHEUS (Program for a European Traffic with Highest Efficiency and Unprecedented Safety) \citep{prometheus} at the end of the '80s and the Grand DARPA Challenges in the late 2000s.
At these times, systems are heavily-engineered pipelines \citep{urmson2008autonomous,thrun2006stanley} and their modular aspect decomposes the task of driving into several smaller tasks --- from perception to planning --- which has the advantage to offer interpretability and transparency to the processing. Nevertheless, modular pipelines have also known limitations such as the lack of flexibility, the need for handcrafted representations, and the risk of error propagation.
In the 2010s, we observe an interest in approaches aiming to \emph{train} driving systems, usually in the form of neural networks, either by leveraging large quantities of expert recordings \citep{pilotnet,codevilla2018end,imitation_overview} or through simulation \citep{Espi2005TORCSTO,marinaffordance,carla}.
In both cases, these systems learn a highly complex transformation that operates over input sensor data and produce end-commands (steering angle, throttle). 
While these neural driving models overcome some of the limitations of the modular pipeline stack, they are sometimes described as \emph{black-boxes} for their critical lack of transparency and interpretability.
 -->
<h2 id="contextual-elements">
<a class="anchor" href="#contextual-elements" aria-hidden="true"><span class="octicon octicon-link"></span></a>Contextual elements</h2>

<h3 id="why-do-we-need-explainable-driving-models">
<a class="anchor" href="#why-do-we-need-explainable-driving-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Why do we need explainable driving models?</h3>

<h3 id="explainability-">
<a class="anchor" href="#explainability-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Explainability ?</h3>

<p>Many terms are related to the concept of explainability and several definitions have been proposed for each of these terms. The boundaries between concepts are fuzzy and constantly evolving. 
In human-machine interactions, explainability is defined as the ability for the human user to understand the agent’s logic <a class="citation" href="#RosenfeldR19">(Rosenfeld &amp; Richardson, 2019)</a>. 
The explanation is based on how the human user understands the connections between inputs and outputs of the model. 
According to (missing reference), an explanation is a human-interpretable description of the process by which a decision-maker took a particular set of inputs and reached a particular conclusion. They state that in practice, an explanation should answer at least one of the three following questions:</p>
<ul>
  <li><em>What were the main factors in the decision?</em></li>
  <li><em>Would changing a certain factor have changed the decision?</em></li>
  <li><em>Why did two similar-looking cases get different decisions, or vice versa?</em></li>
</ul>

<p>The term explainability often co-occurs with the concept of interpretability.
Some recent work of <a class="citation" href="#beaudouin2020identifying">(Beaudouin et al., 2020)</a> simply advocate that explainability and interpretability are synonyms.
However, <a class="citation" href="#GilpinBYBSK18">(Gilpin et al., 2018)</a> provide a nuance between these terms that we find interesting. According to them, interpretability designate to which extent an explanation is understandable by a human. 
They state that an explanation should be designed and assessed in a trade-off between its interpretability and its completeness, which measures how accurate the explanation is as it describes the inner workings of the system. 
For example, an exhaustive and completely faithful explanation is a description of the system itself and all its processing: this is a complete explanation although the exhaustive description of the processing may be incomprehensible.
The whole challenge in explaining neural networks is to provide explanations that are both interpretable and complete.</p>

<p>The relation with autonomous vehicles differs a lot depending who is interacting with the system. 
Indeed, expected explanations will be of varying nature, form and should convey different types of information regarding who is the explanation geared towards:</p>
<ul>
  <li>
<strong>End-users</strong> and citizens need to trust the autonomous system and to be reassured. They put their life in the hands of the driving system and thus need to gain trust in it. 
There is a long and dense line of research trying to define, characterize, evaluate, and increase the trust between an individual and a machine. 
It appears that user trust is heavily impacted by the system transparency <a class="citation" href="#trusthci20">(Zhang et al., 2020)</a>: providing information that helps the user understand how the system functions foster his or her trust in the system. Interestingly, research on human-computer interactions argues that the timing of explanations is important for trust: they should be provided before the vehicle takes an action, in a formulation which is concise and direct. <a class="citation" href="#RosenfeldR19">(Rosenfeld &amp; Richardson, 2019)</a>,<a class="citation" href="#haspiel2018explanations">(Haspiel et al., 2018)</a>,<a class="citation" href="#du2019look">(Du et al., 2019)</a>
</li>
  <li>
<strong>Designers</strong> of self-driving models need to understand their limitations to validate them and improve future versions.
The concept of Operational Design Domain (ODD) is often used by carmakers to designate the conditions under which the car is expected to behave safely.
Thus, whenever a machine learning model is built to address the task of driving, it is crucial to know and understand its failure modes, and to verify that these situations do not overlap with the ODD. 
A common practice is to stratify the evaluation into situations, as is done by the European New Car Assessment Program (Euro NCAP) to test and assess assisted driving functionalities in new vehicles.
But even if these in-depth performance analysis are helpful to improve the model’s performance, it is not possible to exhaustively list and evaluate every situation the model may possibly encounter. 
As a fallback solution, explainability can help delving deeper into the inner workings of the model and to understanding why it makes these errors and correct the model/training data accordingly.</li>
  <li>
<strong>Legal and regulator bodies</strong> are interested in explanations for <em>liability</em> and <em>accountability</em> purposes, especially when a self-driving system is involved in a car accident. 
 Notably, explanations generated for legal or regulatory institutions are likely to be different from those addressed to the end-user. 
 As noted in <a class="citation" href="#beaudouin2020identifying">(Beaudouin et al., 2020)</a>, detailed explanations of all aspects of the decision process could be required to identify the reasons for a malfunction.
These explanations are directed towards experts who will likely spend large amounts of time studying the system, and who are thus inclined to receive rich explanations with great amounts of detail.</li>
</ul>

<h3 id="driving-system-">
<a class="anchor" href="#driving-system-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Driving system ?</h3>

<p>The history of autonomous driving systems started in the late ’80s and early ’90s with the European Eureka project called Prometheus.
This has later been followed by driving challenges proposed by the Defense Advanced Research Projects Agency (DARPA). The vast majority of autonomous systems competing in these challenges is characterized by their modularity.
Leveraging strong suites of sensors, these systems are composed of several sub-modules, each completing a very specific task. 
Broadly speaking, these sub-tasks deal with sensing the environment, forecasting future events, planning, taking high-level decisions, and controlling the vehicle.</p>

<p>As pipeline architectures split the driving task into easier-to-solve problems, they offer somewhat interpretable processing of sensor data through specialized modules (perception, planning, decision, control).
However, these approaches have several drawbacks:</p>
<ul>
  <li>First, they rely on human heuristics and manually-chosen intermediate representations, which are not proven to be optimal for the driving task.</li>
  <li>Second, they lack flexibility to account for real-world uncertainties and to generalize to unplanned scenarios.
Moreover, from an engineering point of view, these systems are hard to scale and to maintain as the various modules are entangled together.</li>
  <li>Finally, they are prone to error propagation between the multiple sub-modules.</li>
</ul>

<p>To circumvent these issues, and nurtured by the deep learning revolution, researchers put more and more efforts on machine learning-based driving systems, and in particular on deep neural networks which can leverage large quantities of data.</p>

<p>We can distinguish four key elements involved in the design of a neural driving system: input sensors, input representations, output type, and learning paradigm</p>

<p><img src="/blog/images/posts/explainable_driving/driving_architecture.png" alt="driving_architecture" width="100%"></p>
<div class="caption"><b>Figure 1. Overview of neural network-based autonomous driving systems.</b></div>

<ul>
  <li>
<strong>Sensors</strong>. They are the hardware interface through which the neural network perceives its environment.
Typical neural driving systems rely on sensors from two families: <em>proprioceptive</em> sensors and <em>exteroceptive</em> sensors. <em>Proprioceptive</em> sensors provide information about the internal vehicle state such as speed, acceleration, yaw, change of position, and velocity. They are measured through tachometers, inertial measurement units (IMU), and odometers.  All these sensors communicate through the controller area network (CAN) bus, which allows signals to be easily accessible. In contrast, <em>exteroceptive</em> sensors acquire information about the surrounding environment. They include cameras, radars, LiDARs, and GPS. For a more thorough review of driving sensors, we refer the reader to <a class="citation" href="#survey_sensors">(Yurtsever et al., 2020)</a>.</li>
  <li>
<strong>Input representation</strong>. Once sensory inputs are acquired by the system, they are processed by computer vision models to build a structured representation, before being passed to the neural driving system. In the <em>mediated perception</em> approach, several perception systems provide their understanding of the world, and their outputs are aggregated to build an input for the driving model.
An example of such vision tasks is object detection and semantic segmentation, which aim at finding and classifying relevant objects in a scene (cars, bicycles, pedestrians, stop signs, <em>etc.</em>), tracking objects accross time, extracting depth information (<em>i.e.</em> knowing the distance that separates the vehicle from each point in the space), recognition of pedestrian intent…
Mediated perception contrasts with the <em>direct perception</em> approach, which instead extracts visual affordances from an image.
Affordances are scalar indicators that describe the road situation such as curvature, deviation to neighboring lanes, or distances between ego and other vehicles.
These human-interpretable features are usually recognized using neural networks as in <a class="citation" href="#deepdrivingaffordance">(Chen et al., 2015)</a>.
Then, they are passed at the input of a driving controller which is usually hard-coded, even if some recent approaches use affordance recognition to provide compact inputs to learning-based driving systems <a class="citation" href="#marinaffordance">(Toromanoff et al., 2020)</a>.</li>
  <li>
<strong>Outputs</strong>. Ultimately, the goal is to generate vehicle controls. Some approaches, called end-to-<em>end</em>, tackle this problem by training the deep network to directly output the commands.
However, in practice most methods instead predict the future trajectory of the autonomous vehicle; they are called end-to-<em>mid</em> methods. The trajectory is then expected to be followed by a low-level controller, such as the proportional–integral–derivative (PID) controller.</li>
  <li>
<strong>Learning</strong>.
Two families of methods coexist for training self-driving neural models: <em>behavior cloning</em> approaches, which leverage datasets of human driving sessions, and <em>reinforcement learning</em> approaches, which train models through trial-and-error simulation.
    <ul>
      <li>Behavior cloning (BC) approaches leverage huge quantities of recorded human driving sessions to learn the input-output driving mapping by imitation. 
  In this setting, the network is trained to mimic the commands applied by the expert driver (end-to-end models), or the future trajectory (end-to-mid models), in a supervised fashion. 
  Initial attempt to behavior cloning of vehicle controls was made in <a class="citation" href="#Pomerleau88">(Pomerleau, 1988)</a>, and continued later in <a class="citation" href="#pilotnet">(Bojarski et al., 2016)</a>.</li>
      <li>Reinforcement learning (RL) was alternatively explored by researchers to train neural driving systems. This paradigm learns a policy by balancing self-exploration and reinforcement.
  This training paradigm does not require a training set of expert driving but relies instead on a simulator such as CARLA <a class="citation" href="#carla">(Dosovitskiy et al., 2017)</a>.</li>
    </ul>
  </li>
</ul>

<h3 id="the-challenges-of-explainability-of-neural-driving-systems">
<a class="anchor" href="#the-challenges-of-explainability-of-neural-driving-systems" aria-hidden="true"><span class="octicon octicon-link"></span></a>The challenges of explainability of neural driving systems</h3>

<p>Introducing explainability in the design of learning-based self-driving systems is a challenging task.
These concerns arise from two aspects:</p>
<ul>
  <li>From an <strong>ML perspective</strong>, explainability hurdles of self-driving models are shared with most deep learning models, across many application domains. Indeed, decisions of deep systems are intrinsically hard to explain as the functions these systems represent, mapping from inputs to outputs, are not transparent. 
In particular, although it may be possible for an expert to broadly understand the structure of the model, the parameter values, which have been learned, are yet to be explained.
There are several factors giving rise to interpretability problems for self-driving systems. First, the dataset used for training brings interpretability issues, as a finite training dataset cannot exhaustively cover all possible driving situations. It will likely under- and over-represent some specific cases, and questions such as <em>Has the model encounter situations like X?</em> are legitimate. Moreover, datasets contain numerous biases of various nature (omitted variable bias, cause-effect bias, sampling bias), which also gives rise to explainability issues related to fairness.
Second, the trained model, and the mapping function it represents, is poorly understood and is considered as a <em>black-box</em>. The model is highly non-linear and does not provide any robustness guarantee as small input changes may dramatically change the output behavior. Explainability issues thus occur regarding the generalizability and robustness aspects: <em>How will the model behave under these new scenarios?</em> Third, the learning phase is not perfectly understood. Among other things, there are no guarantees that the model will settle at a minimum point that generalizes well to new situations, and that the model does not underfit on some situations and overfit on others. Besides, the model may learn to ground its decisions on spurious correlations during training instead of leveraging causal signals. We aim at finding answers to questions like <em>Which factors caused this decision to be taken?</em>
</li>
  <li>From a <strong>driving perspective</strong>, it has been shown that humans tackle this task by solving many intermediate sub-problems, at different levels of hierarchy <a class="citation" href="#michon1984critical">(Michon, 1984)</a>.
In the effort towards building an autonomous driving system, researchers aim at providing the machine with these intermediate capabilities. Thus, explaining the general behavior of an autonomous vehicle inevitably requires understanding how each of these intermediate steps is carried and how it interacts with others. We can categorize these capabilities into three types:
    <ul>
      <li>
<em>Perception</em>: information about the system’s understanding of its local environment. This includes the objects that have been recognized and assigned to a semantic label (persons, cars, urban furniture, driveable area, crosswalks, traffic lights), their localization, properties of their motion (velocity, acceleration), intentions of other agents, <em>etc</em>.;</li>
      <li>
<em>Reasoning</em>: information about how the different components of the perceived environment are organized and assembled by the system. This includes global explanations about the rules that are learned by the model, instance-wise explanation showing which objects are relevant in a given scene, traffic pattern recognition, object occlusion reasoning, <em>etc.</em>;</li>
      <li>
<em>Decision</em>: information about how the system processes the perceived environment and its associated reasoning to produce a decision. This decision can be a high-level goal stating that the car should turn right, a prediction of the ego vehicle’s trajectory, its low-level relative motion or even the raw controls, <em>etc</em>.
While the separation between perception, reasoning, and decision is clear in modular driving systems, some recent end-to-end neural networks such as <a class="citation" href="#pilotnet">(Bojarski et al., 2016)</a> blur the lines and perform these simultaneously. Indeed, when an explanation method is developed for a neural driving system, it is often not clear whether it attempts to explain the perception, the reasoning, or the decision step.
Considering the nature of neural networks architecture and training, disentangling perception, reasoning, and decision in neural driving systems constitutes a non-trivial challenge.</li>
    </ul>
  </li>
</ul>

<h2 id="post-hoc-explanation">
<a class="anchor" href="#post-hoc-explanation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Post-hoc explanation</h2>

<p>When a deep learning model in general — or a self-driving model more specifically — comes as an opaque black-box as it has not been designed with a specific explainability constraint, <em>post-hoc</em> methods have been proposed to gain interpretability from the network processing and its representations.
Post-hoc explanations have the advantage of giving an interpretation to black-box models without conceding any predictive performance.
In this section, we assume that we have a model $f$ which is already trained.
Two main categories of post-hoc methods can be distinguished to explain $f$: <em>local</em> methods which explain the prediction of the model for a specific instance, and <em>global</em> methods that seek to explain the model in its entirety, <em>i.e.</em> by gaining a finer understanding on learned representations and activations.</p>

<h3 id="local-explanations">
<a class="anchor" href="#local-explanations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Local explanations</h3>

<p>Given an input image $x$, a local explanation aims at justifying why the model $f$ gives its specific prediction $y=f(x)$.
We distinguish three types of approaches: saliency methods which determine regions of image $x$ influencing the most the decision,
local approximations which approach the behavior of the black-box model $f$ locally around the instance $x$,
and counterfactual analysis which aims to find the cause in $x$ that made the model predict $f(x)$.</p>

<h4 id="saliency-methods">
<a class="anchor" href="#saliency-methods" aria-hidden="true"><span class="octicon octicon-link"></span></a>Saliency methods</h4>
<p>A saliency method aims at explaining which input image’s regions influence the most the output of the model.
These methods produce a heat map that highlights regions on which the model relied the most for its decision.
By doing so, these methods mostly explain the perception part of the driving architectures.
The first saliency method to visualize the input influence in the context of autonomous driving has been developed by <a class="citation" href="#visualbackprop">(Bojarski et al., 2018)</a>.
The VisualBackprop method they propose identifies sets of pixels by backpropagating activations from both late layers, which contain relevant information for the task but have a coarse resolution, and early layers which have a finer resolution. The algorithm runs in real-time and can be embedded in a self-driving car. 
This method has been used to explain PilotNet <a class="citation" href="#pilotnet">(Bojarski et al., 2016)</a>, a deep end-to-end control prediction model.
As seen in <strong>Figure 2.</strong>, they qualitatively validate that the model correctly grounds its decisions on lane markings, edges of the road (delimited with grass or parked cars), and surrounding cars.</p>

<p><img src="/blog/images/posts/explainable_driving/visualbackprop.png" alt="visualbackprop" width="100%"></p>
<div class="caption"><b>Figure 2. Example of salient pixels obtained by VisualBackprop. Credits to <a class="citation" href="#explaining_pilotnet">(Bojarski et al., 2017)</a></b></div>

<p>Recently, <a class="citation" href="#conditionalaffordance">(Sauer et al., 2018)</a> propose to condition the saliency visualization on a variety of driving affordances. They employ the Grad-CAM saliency technique <a class="citation" href="#gradcam">(Selvaraju et al., 2020)</a> on a deep model trained to predict driving affordances on a dataset recorded from the CARLA simulator. They argue that saliency methods are particularly well suited for this type of architecture on the contrary to end-to-end models, as those models map all of the perception (<em>e.g.</em> detection of speed limits, red lights, cars, <em>etc.</em>.) to a single control output. Instead, in their case, they can analyze the saliency in the input image for each affordance.</p>

<p>While saliency methods enable visual explanations for deep black-box models, they come with some limitations. First, they are hard to evaluate. For example, human evaluation can be employed, but this comes with the risk of selecting methods which are more <em>persuasive</em> than <em>faithful</em>. Second, recent work of <a class="citation" href="#sanity_check_saliency">(Adebayo et al., 2018)</a> tend to show that some saliency methods can provide heat maps that are independrnt both of the model and the data. Indeed, they show that some saliency methods behave like edge-detectors even when they are applied to a model with random weights. Lastly, different saliency methods produce different results and it is not obvious to know which one is correct, or better than others. In that respect, a potential research direction is to learn to combine explanations coming from various explanation methods.</p>

<h4 id="local-approximations">
<a class="anchor" href="#local-approximations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Local approximations</h4>

<p>The idea of a local approximation method is to approach the behavior of the black-box model in the vicinity of the instance to be explained, with a simpler model.
In practice, a separate model, inherently interpretable, is built to act as a proxy for the input/output mapping of the main model locally around the instance.
Such methods include the Local Interpretable Model-agnostic Explanations (LIME) approach <a class="citation" href="#lime">(Ribeiro et al., 2016)</a>, which learns an interpretable-by-design input/output mapping, mimicking the behavior of the main model in the neighborhood of an input.
In practice, such mapping can be instantiated by a decision tree or a linear model.
Note that in the case of LIME, the interpretable student model does not necessarily use the raw instance data but rather an interpretable input, such as a binary vector indicating the presence or absence of a superpixel in an image.
The SHapley Additive exPlanations (SHAP) approach <a class="citation" href="#shap">(Lundberg &amp; Lee, 2017)</a> has later been introduced to generalize LIME, as well as other additive feature attribution methods, and provides more consistent results.</p>

<p>In the autonomous driving literature, we are not aware of any work that aims to explain a self-driving model by locally approximating it with an interpretable model. 
This is likely due to the cost of local approximation strategies, as a set of perturbed inputs are sampled and forwarded in the main model to collect their corresponding labels.
Besides, these methods operate on a simplified input representation instead of the raw input. This interpretable semantic basis should be chosen wisely, as it constitutes the vocabulary that can be used by the explanation system.
Finally, these techniques were shown to be highly sensitive to hyper-parameter choices.</p>

<h4 id="counterfactual-explanations">
<a class="anchor" href="#counterfactual-explanations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Counterfactual explanations</h4>

<p>Recently, a lot of attention has been put on counterfactual analysis, a field from the causal inference literature.
A counterfactual analysis aims at finding features $X$ within the input $x$ that <em>caused</em> the decision $y=f(x)$ to be taken, by imagining a new input instance $x’$ where $X$ is changed and a different outcome $y’$ is observed. 
The new imaginary scenario $x’$ is called a <em>counterfactual example</em> and the different output $y’$ is a contrastive class. 
The new counterfactual example, and the change in $X$ between $x$ and $x’$, constitute <em>counterfactual explanations</em>.
In other words, a counterfactual example is a modified version of the input, in a minimal way, that changes the prediction of the model to the predefined output $y’$. 
In an autonomous driving context, it corresponds to questions like ‘‘What should be different in this scene, such that the car would have stopped instead of moving forward?’’</p>

<p>Several requirements should be imposed to find counterfactual examples.
First, the prediction $f(x’)$ of the counterfactual example must be close to the desired contrastive class $y’$.
Second, the counterfactual change must be <em>minimal</em>, <em>i.e.</em> the new counterfactual example $x’$ must be as similar as possible to $x$, either by making sparse changes or in the sense of some distance.
Third, the counterfactual change must be <em>relevant</em>, <em>i.e.</em> new counterfactual instances must be likely in the underlying input data distribution.</p>

<p>Regarding the autonomous driving literature, there only exists a limited number of approaches involving counterfactual interventions.
When the input space has semantic dimensions and can thus be easily manipulated, the causal impace of input factors can be measured by intervening on them (removing or adding).
In <a class="citation" href="#bansal2018chauffeutnet">(Bansal et al., 2018)</a>, causal factors of the driving model are studied as they test their model under various hand-designed inputs where some objects have been removed.
More recently, <a class="citation" href="#whomakedriversstop">(Li et al., 2020)</a> introduce a causal inference strategy for the identification of ‘‘risk-objects’’, <em>i.e.</em> objects that have a causal impact on the driver’s behavior (see <strong>Figure 3</strong>. In this work, decisions are binary (‘go’ or ‘stop’).
The idea is that removing non-causal objects from a scene will not affect the decision.
They propose a training algorithm with interventions, where some objects are randomly removed in scenes where the output is ‘go’. At inference, in a sequence where the model predicts ‘stop’, the risk-object is found as the one which gives the higher score to the ‘go’ class when removed.</p>

<p><img src="/blog/images/posts/explainable_driving/whomakesdriverstop.PNG" alt="whomakesdriverstop" width="100%"></p>
<div class="caption">
<b>Figure 3. Counterfactual intervention to measure the causal impact of an input region.</b> Removing a pedestrian induces a change in the driver's decision from 'stop' to 'go', which indicates that the pedestrian is a risk-object. Credits to <a class="citation" href="#whomakedriversstop">(Li et al., 2020)</a>.</div>

<p>We call the reader’s attention to the fact that analyzing driving scenes and building driving models using causality is far from trivial as it requires the capacity to <em>intervene</em> on the model’s inputs. This, in the context of driving, is a highly complex problem to solve for three main reasons.</p>
<ul>
  <li>First, the data is composed of high-dimensional tensors of raw sensor inputs (such as the camera or LiDAR signals) and scalar-valued signals that represent the current physical state of the vehicle (velocity, yaw rate, acceleration, <em>etc</em>.). Performing controlled interventions requires the capacity to perform realistic alterations of these raw high-dimensional inputs.
Even though some recent works explore realistic alterations of visual content <a class="citation" href="#flow_edge_guided">(Gao et al., 2020)</a>, this is yet to be applied in the context of self-driving.
Interestingly, more and more neural driving systems rely on semantic representations. Alterations of the input space are simplified as the realism requirement is removed.</li>
  <li>Second, modified inputs must be coherent and respect the underlying causal structure of the data generation process. As an example, we may be provided with a driving scene that depicts a green light, pedestrians waiting and vehicles passing. A simple intervention consisting of changing the state of the light to red would imply massive changes on the other variables to be \emph{coherent}: pedestrians should start crossing the street and vehicles should stop at the red light. The very recent and promising work of <a class="citation" href="#li2020causal">(Li et al., 2020)</a> tackles the issue of unsupervised <em>causal discovery</em> in videos. We believe that the adaptation of this type of approach to real driving data is crucial for the development of causal explainability.</li>
  <li>Finally, we would need to have annotations for these new examples. Indeed, whether we use these altered examples to train a driving model on or to perform exhaustive and controlled evaluations, expert annotations would be required. Considering the nature of the driving data, it might be hard for a human to provide these annotations: they would need to imagine the decision they would have taken (control values or future trajectory) in this newly generated situation.</li>
</ul>

<h3 id="global-explanations">
<a class="anchor" href="#global-explanations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Global explanations</h3>

<p>Global explanations contrast with local explanation methods as they attempt to explain the behavior of a model in general by summarizing the information it contains. 
We distinguish three families of methods to provide global explanations.</p>
<ul>
  <li>
<strong>Model translation</strong>. These techniques aim at transfering the knowledge contained in the main opaque model into a separate machine learning model that is inherently interpretable. Concretely, this involves training a simple explainable model (such as a decision tree or a rule-based system) to mimic the input-output mapping of the black-box function.</li>
  <li>
<strong>Explaining representations</strong>. The goal of these methods is to provide insights into what is captured by the internal data structures of the model (neurons, vectors, layers, <em>etc.</em>), at different granularities.</li>
  <li>
<strong>Prototypes</strong>. They are specific data instances that represent well the data. Prototypes are chosen simultaneously to represent the data distribution in a non-redundant way. Global explanations can be obtained by computing and aggregating local explanations of these prototypes.</li>
</ul>

<p>To the best of our knowledge, global explanation techniques have only been used scarcely in the autonomous driving literature. Please refer to the survey for more details.</p>

<h2 id="desigining-an-explainable-driving-model">
<a class="anchor" href="#desigining-an-explainable-driving-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Desigining an explainable driving model</h2>

<p>We remark that tools for post-hoc analysis operate on models whose design may have completely ignored the requirement of explainability. 
A good example of such models is PilotNet <a class="citation" href="#pilotnet">(Bojarski et al., 2016)</a> which consists in a convolutional neural network operating over a raw video stream and producing the vehicle controls at every time step. 
Understanding the behavior of this system is only possible through external tools, but cannot be done directly by observing the model itself.</p>

<p>Drawing inspiration from modular systems, recent architectures place a particular emphasis on conveying understandable information about their inner workings, in addition to their performance imperatives.
The modularity of pipelined architectures allows for forensic analysis, by studying the quantities that are transferred between modules (<em>e.g.</em> semantic and depth maps, forecasts of surrounding agent’s future trajectories, <em>etc</em>.). 
These modularity-inspired models exhibit some forms of interpretability, which can be enforced at three different levels in the design of the driving system: input level, intermediate level and output level.</p>

<h3 id="input-level-explanations">
<a class="anchor" href="#input-level-explanations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Input-level explanations</h3>
<p>Input-level explanations aim at enlightening the user on which perceptual information is used by the model to take its decisions.
We identified two families of approaches that ease interpretation at the input level: attention-based models and models that use semantic inputs.</p>

<h4 id="attention-based-models">
<a class="anchor" href="#attention-based-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Attention-based models</h4>

<p>Attention mechanisms learn a function that scores different regions of the input depending on whether or not they should be considered in the decision process. 
This scoring is often performed based on some contextual information that helps the model decide which part of the input is relevant to the task at hand. 
<a class="citation" href="#show_attend_tell">(Xu et al., 2015)</a> are the first to use an attention mechanism for a computer vision problem, namely, image captioning. Many of such attention models were developed for other applications since then, for example in Visual Question Answering (VQA). Intuitively, the question tells the VQA model where to look to answer the question correctly.
Not only do attention mechanisms boost the performance of machine learning models, but also they provide insights into the inner workings of the system. By visualizing the attention weight associated with each input region, it is possible to know which part of the image was deemed relevant to make the decision.</p>

<p>Attention-based models recently stimulated interest in the self-driving community, as they supposedly give a hint about the internal reasoning of the neural network. 
In <a class="citation" href="#causal_attention">(Kim &amp; Canny, 2017)</a>, an attention mechanism is used to weight each region of an image, using information about previous frames as a context. 
Visual attention can also be used to select objects defined by bounding boxes, as in <a class="citation" href="#deep_object_centric">(Wang et al., 2019)</a>. 
In this work, a pre-trained object detector provides regions of interest (RoIs), which are weighted using the global visual context, and aggregated to decide which action to take.
Recently, <a class="citation" href="#attentional_bottleneck">(Kim &amp; Bansal, 2020)</a> extended the ChauffeurNet architecture by building a visual attention module that operates on a bird-eye view semantic scene representation. Interestingly, as shown in <strong>Figure 4</strong>, combining visual attention with information bottleneck results in sparser saliency maps, making them more interpretable.</p>

<p><img src="/blog/images/posts/explainable_driving/attentionbottleneck.PNG" alt="attentionbottleneck" width="100%"></p>
<div class="caption">
<b>Figure 4. Comparison of attention maps from classical visual attention and from attention bottleneck</b> Attention bottleneck seems to provide tighter modes, focused on objects of interest. Credits to <a class="citation" href="#attentional_bottleneck">(Kim &amp; Bansal, 2020)</a>.</div>

<p>While these attention mechanisms are often thought to make neural networks more transparent, the recent work of <a class="citation" href="#attention_not_explanation">(Jain &amp; Wallace, 2019)</a> mitigates this assumption. Indeed, they show, in the context of natural language, that learned attention weights poorly correlate with multiple measures of feature importance. They even show that it is possible to find adversarial attention weights that keep the same prediction while weighting the input words very differently. All these findings cast some doubts on the faithfulness of explanations based on attention maps.</p>

<h2 id="use-case-generating-natural-language-explanations">
<a class="anchor" href="#use-case-generating-natural-language-explanations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Use-case: generating natural language explanations</h2>

<h2 id="conclusion">
<a class="anchor" href="#conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h2>

<h2 id="references">
<a class="anchor" href="#references" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h2>

<ol class="bibliography">
<li><span id="RosenfeldR19">Rosenfeld, A., &amp; Richardson, A. (2019). Explainability in human-agent systems. <i>Auton. Agents Multi Agent Syst.</i></span></li>
<li><span id="beaudouin2020identifying">Beaudouin, V., Bloch, I., Bounie, D., Clémençon, S., d’Alché-Buc, F., Eagan, J., Maxwell, W., Mozharovskyi, P., &amp; Parekh, J. (2020). Flexible and Context-Specific AI Explainability: A Multidisciplinary
               Approach. <i>CoRR</i>.</span></li>
<li><span id="GilpinBYBSK18">Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., &amp; Kagal, L. (2018). Explaining Explanations: An Overview of Interpretability of Machine
               Learning. <i>DSSA</i>.</span></li>
<li><span id="trusthci20">Zhang, Q., Yang, X. J., &amp; Robert, L. P. (2020). Expectations and Trust in Automated Vehicles. <i>CHI</i>.</span></li>
<li><span id="haspiel2018explanations">Haspiel, J., Du, N., Meyerson, J., Jr., L. P. R., Tilbury, D. M., Yang, X. J., &amp; Pradhan, A. K. (2018). Explanations and Expectations: Trust Building in Automated Vehicles. <i>HRI</i>.</span></li>
<li><span id="du2019look">Du, N., Haspiel, J., Zhang, Q., Tilbury, D., Pradhan, A. K., Yang, X. J., &amp; Robert Jr, L. P. (2019). Look who’s talking now: Implications of AV’s explanations on driver’s trust, AV preference, anxiety and mental workload. <i>Transportation Research Part C: Emerging Technologies</i>.</span></li>
<li><span id="survey_sensors">Yurtsever, E., Lambert, J., Carballo, A., &amp; Takeda, K. (2020). A Survey of Autonomous Driving: Common Practices and Emerging Technologies. <i>IEEE Access</i>.</span></li>
<li><span id="deepdrivingaffordance">Chen, C., Seff, A., Kornhauser, A. L., &amp; Xiao, J. (2015). DeepDriving: Learning Affordance for Direct Perception in Autonomous
               Driving. <i>ICCV</i>.</span></li>
<li><span id="marinaffordance">Toromanoff, M., Émilie Wirbel, &amp; Moutarde, F. (2020). End-to-End Model-Free Reinforcement Learning for Urban Driving Using
               Implicit Affordances. <i>CVPR</i>.</span></li>
<li><span id="Pomerleau88">Pomerleau, D. (1988). ALVINN: An Autonomous Land Vehicle in a Neural Network. <i>NIPS</i>.</span></li>
<li><span id="pilotnet">Bojarski, M., Testa, D. D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., Jackel, L. D., Monfort, M., Muller, U., Zhang, J., Zhang, X., Zhao, J., &amp; Zieba, K. (2016). End to End Learning for Self-Driving Cars. <i>CoRR</i>.</span></li>
<li><span id="carla">Dosovitskiy, A., Ros, G., Codevilla, F., López, A., &amp; Koltun, V. (2017). CARLA: An Open Urban Driving Simulator. <i>CoRL</i>.</span></li>
<li><span id="michon1984critical">Michon, J. A. (1984). <i>A Critical View of Driver Behavior Models: What Do We Know, what Should We Do?</i> Human behavior and traffic safety.</span></li>
<li><span id="visualbackprop">Bojarski, M., Choromanska, A., Choromanski, K., Firner, B., Ackel, L. J., Muller, U., Yeres, P., &amp; Zieba, K. (2018). VisualBackProp: Efficient Visualization of CNNs for Autonomous Driving. <i>ICRA</i>.</span></li>
<li><span id="explaining_pilotnet">Bojarski, M., Yeres, P., Choromanska, A., Choromanski, K., Firner, B., Jackel, L. D., &amp; Muller, U. (2017). Explaining How a Deep Neural Network Trained with End-to-End Learning
               Steers a Car. <i>CoRR</i>.</span></li>
<li><span id="conditionalaffordance">Sauer, A., Savinov, N., &amp; Geiger, A. (2018). Conditional Affordance Learning for Driving in Urban Environments. <i>CoRL</i>.</span></li>
<li><span id="gradcam">Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., &amp; Batra, D. (2020). Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based
               Localization. <i>Int. J. Comput. Vis.</i></span></li>
<li><span id="sanity_check_saliency">Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I. J., Hardt, M., &amp; Kim, B. (2018). Sanity Checks for Saliency Maps. <i>NeurIPS</i>.</span></li>
<li><span id="lime">Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2016). "Why Should I Trust You?": Explaining the Predictions of Any Classifier. <i>SIGKDD</i>.</span></li>
<li><span id="shap">Lundberg, S. M., &amp; Lee, S.-I. (2017). A Unified Approach to Interpreting Model Predictions. <i>NIPS</i>.</span></li>
<li><span id="bansal2018chauffeutnet">Bansal, M., Krizhevsky, A., &amp; Ogale, A. S. (2018). ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst. <i>CoRR</i>.</span></li>
<li><span id="whomakedriversstop">Li, C., Chan, S. H., &amp; Chen, Y.-T. (2020). Who Make Drivers Stop? Towards Driver-centric Risk Assessment: Risk
               Object Identification via Causal Inference. <i>IROS</i>.</span></li>
<li><span id="flow_edge_guided">Gao, C., Saraf, A., Huang, J.-B., &amp; Kopf, J. (2020). Flow-edge Guided Video Completion. <i>ECCV</i>.</span></li>
<li><span id="li2020causal">Li, Y., Torralba, A., Anandkumar, A., Fox, D., &amp; Garg, A. (2020). Causal discovery in physical systems from videos. <i>NeurIPS</i>.</span></li>
<li><span id="show_attend_tell">Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A. C., Salakhutdinov, R., Zemel, R. S., &amp; Bengio, Y. (2015). Show, Attend and Tell: Neural Image Caption Generation with Visual
               Attention. <i>ICML</i>.</span></li>
<li><span id="causal_attention">Kim, J., &amp; Canny, J. F. (2017). Interpretable Learning for Self-Driving Cars by Visualizing Causal
               Attention. <i>ICCV</i>.</span></li>
<li><span id="deep_object_centric">Wang, D., Devin, C., Cai, Q.-Z., Yu, F., &amp; Darrell, T. (2019). Deep Object-Centric Policies for Autonomous Driving. <i>ICRA</i>.</span></li>
<li><span id="attentional_bottleneck">Kim, J., &amp; Bansal, M. (2020). Attentional Bottleneck: Towards an Interpretable Deep Driving Network. <i>CVPR Workshops</i>.</span></li>
<li><span id="attention_not_explanation">Jain, S., &amp; Wallace, B. C. (2019). Attention is not Explanation. <i>NAACL</i>.</span></li>
</ol>

  </div><a class="u-url" href="/blog/2021/01/14/explainable-driving.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>valeo.ai research blog</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
