<style type="text/css">

/*h1 {
    margin-top:2.5em !important;
    color: #4a788c;
    font-size: 120%;
}

h2 {
    margin-top:1.5em !important;      
    color: #4a788c;

}

h3, h4 {
    margin-top:0.5em !important;      
    color: #4a788c;

}*/

h1 {
    margin-top:0.5em !important;
    margin-bottom:0.25em !important;
}

h3 {
    margin-bottom:0.25em !important;
    margin-top:0.25em !important;
}

p {
    margin-top:1rem !important; 
    margin-bottom:1rem !important;  
    font-size: 16px;

}

/*h1, h2, h3, h4 {
    font-weight: normal !important;
    margin-bottom:0.5em !important;  
    code {
      font-size: 100%;
    }
}   
*/

 .bibtex-box {
        background-color: #eee;     
        border: 1px solid #eeeeee;
        /*border-radius: 10px ;*/
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
        margin-top:1rem; 
        word-break: normal;
    }
</style>

<h1 align="center"> PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving </h1>
<!-- Simple call of authors -->
<!-- <h3 align="center"> Thibault Buhet, Emilie Wirbel, Andrei Bursuc and Xavier Perrotton </h3> -->
<!-- Alternatively you can add links to author pages -->
<h3 align="center"> <a href="https://scholar.google.com/citations?user=TLRo9J0AAAAJ&amp;hl=en&amp;oi=ao">Thibault Buhet</a> &nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=Z7wRy_cAAAAJ&amp;hl=en">Emilie Wirbel</a>&nbsp;&nbsp; <a href="https://abursuc.github.io/">Andrei Bursuc</a>&nbsp;&nbsp; <a href="https://scholar.google.com/citations?hl=en&amp;user=PBfow1oAAAAJ">Xavier Perrotton</a></h3>

<h3 align="center"> CoRL 2020 </h3>

<div align="center">
  <p>
    
    <a href="https://arxiv.org/pdf/2003.08744.pdf"><i class="far fa-file-pdf"></i> Paper</a>&nbsp;&nbsp;
    
    
    
    
    
  </p>
</div>

<div class="publication-teaser">
    <img src="../../images/publications/plop/plop.png" alt="project teaser" />
</div>

<hr />

<h2 align="center"> Abstract</h2>

<p align="justify">To navigate safely in urban environments, an autonomous vehicle (*ego vehicle*) must understand and anticipate its surroundings, in particular the behavior and intents of other road users (*neighbors*). Most of the times, multiple decision choices are acceptable for all road users (e.g., turn right or left, or different ways of avoiding an obstacle), leading to a highly uncertain and multi-modal decision space. We focus here on predicting multiple feasible future trajectories for both ego vehicle and neighbors through a probabilistic framework. We rely on a conditional imitation learning algorithm, conditioned by a navigation command for the ego vehicle (e.g., *turn right*). Our model processes ego vehicle front-facing camera images and bird-eye view grid, computed from Lidar point clouds, with detections of past and present objects, in order to generate multiple trajectories for both ego vehicle and its neighbors. Our approach is computationally efficient and relies only on on-board sensors. We evaluate our method offline on the publicly available dataset nuScenes, achieving state-of-the-art performance, investigate the impact of our architecture choices on online simulated experiments and show preliminary insights for real vehicle control.</p>

<p><br /></p>
<hr />

<h2 align="center"> Results</h2>

<div class="publication-teaser">
  <img src="../../images/publications/plop/results_nuscenes.png" alt="project teaser" />
</div>

<div class="caption"><b>Comparison on nuScenes 4s prediction</b> with published results of DESIRE-plan, ESP and PRECOG from <a class="citation" href="#rhinehart2019precog">(Rhinehart et al., 2019)</a> (results from their Table II, with a fixed 5 agents training), over minMSD metric.
</div>

<p><img src="../../images/publications/plop/comparison_online.png" alt="" /></p>
<div class="caption"><b>Closed loop error locations for urban and track test data (internal data)</b>, visualized for PLOP and constant velocity baseline. We note that braking behind a vehicle can induce multiple high speed errors and stack multiple red dots on the same location. Points of interest (traffic lights, roundabout, stop signs) are highlighted on the map.</div>

<hr />

<h2 align="center"> Video</h2>

<p align="center">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/94FwahFmc5A" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" align="center"></iframe>
</p>

<p><br /></p>
<hr />

<h2 align="center">BibTeX</h2>
<left>
  <pre class="bibtex-box">
  @article{buhet2020plop,
    title={PLOP: Probabilistic poLynomial Objects trajectory Planning for autonomous driving},
    author={Buhet, Thibault and Wirbel, Emilie and Bursuc, Andrei and Perrotton, Xavier},
    journal={Conference on Robot Learning (CoRL)},
    year={2020}
  }</pre>
</left>

<p><br /></p>

<hr />

<h2 align="center">References</h2>

<ol class="bibliography"><li><span id="rhinehart2019precog">Rhinehart, N., McAllister, R., Kitani, K., &amp; Levine, S. (2019). Precog: Prediction conditioned on goals in visual multi-agent settings. <i>Iccv</i>.</span></li></ol>
