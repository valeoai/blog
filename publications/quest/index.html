<style type="text/css">

/*h1 {
    margin-top:2.5em !important;
    color: #4a788c;
    font-size: 120%;
}

h2 {
    margin-top:1.5em !important;      
    color: #4a788c;

}

h3, h4 {
    margin-top:0.5em !important;      
    color: #4a788c;

}*/

h1 {
    margin-top:0.5em !important;
    margin-bottom:0.25em !important;
}

h3 {
    margin-bottom:0.25em !important;
    margin-top:0.25em !important;
}

p {
    margin-top:1rem !important; 
    margin-bottom:1rem !important;  
    font-size: 16px;

}

/*h1, h2, h3, h4 {
    font-weight: normal !important;
    margin-bottom:0.5em !important;  
    code {
      font-size: 100%;
    }
}   
*/

 .bibtex-box {
        background-color: #eee;     
        border: 1px solid #eeeeee;
        /*border-radius: 10px ;*/
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
        margin-top:1rem; 
        word-break: normal;
    }
</style>

<h1 align="center"> QuEST: Quantized Embedding Space for Transferring Knowledge </h1>
<!-- Simple call of authors -->
<!-- <h3 align="center"> Himalaya Jain, Spyros Gidaris, Nikos Komodakis, Patrick Pérez, and Matthieu Cord </h3> -->
<!-- Alternatively you can add links to author pages -->
<h3 align="center"> <a href="https://himalayajain.github.io/">Himalaya Jain</a>&nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=7atfg7EAAAAJ&amp;hl=en">Spyros Gidaris</a>&nbsp;&nbsp; <a href="https://www.csd.uoc.gr/~komod/">Nikos Komodakis</a>&nbsp;&nbsp; <a href="https://ptrckprz.github.io/">Patrick Pérez</a>&nbsp;&nbsp; <a href="http://webia.lip6.fr/~cord/">Matthieu Cord</a></h3>

<h3 align="center"> ECCV 2020 </h3>

<div align="center">
  <p>
    
    <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660171.pdf"><i class="far fa-file-pdf"></i> Paper</a>&nbsp;&nbsp;
    
    
    
    
    
  </p>
</div>

<div class="publication-teaser">
    <img src="../../images/publications/quest/quest.png" alt="project teaser" />
</div>

<hr />

<h2 align="center"> Abstract</h2>

<p align="justify">Knowledge distillation refers to the process of training a student network to achieve better accuracy by learning from a pre-trained teacher network.
Most of the existing knowledge distillation methods direct the student to follow the teacher by matching the teacher's output, feature maps or their distribution.
In this work, we propose a novel way to achieve this goal: by distilling the knowledge through a quantized visual words space.
According to our method, the teacher's feature maps are first quantized to represent the main visual concepts (i.e., visual words) encompassed in these maps and then the student is asked to predict those visual word representations.
Despite its simplicity, we show that our approach is able to yield results that improve the state of the art on knowledge distillation for model compression and transfer learning scenarios.
To that end, we provide an extensive evaluation across several network architectures and most commonly used benchmark datasets.</p>

<p><br /></p>
<hr />

<h2 align="center"> Video</h2>

<p align="center">
  <iframe width="660" height="395" src="https://www.youtube.com/embed/bJyJkAhjp88" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="" align="center"></iframe>
</p>

<p><br /></p>
<hr />

<h2 align="center">BibTeX</h2>
<left>
  <pre class="bibtex-box">
@article{jain2019quest,
  title={QUEST: Quantized embedding space for transferring knowledge},
  author={Jain, Himalaya and Gidaris, Spyros and Komodakis, Nikos and P{\'e}rez, Patrick and Cord, Matthieu},
  journal={arXiv preprint arXiv:1912.01540},
  year={2019}
}</pre>
</left>

<p><br /></p>
