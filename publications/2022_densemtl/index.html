<style type="text/css">

/*h1 {
    margin-top:2.5em !important;
    color: #4a788c;
    font-size: 120%;
}

h2 {
    margin-top:1.5em !important;      
    color: #4a788c;

}

h3, h4 {
    margin-top:0.5em !important;      
    color: #4a788c;

}*/

h1 {
    margin-top:0.5em !important;
    margin-bottom:0.25em !important;
}

h3 {
    margin-bottom:0.25em !important;
    margin-top:0.25em !important;
}

p {
    margin-top:1rem !important; 
    margin-bottom:1rem !important;  
    font-size: 16px;

}

/*h1, h2, h3, h4 {
    font-weight: normal !important;
    margin-bottom:0.5em !important;  
    code {
      font-size: 100%;
    }
}   
*/

 .bibtex-box {
        background-color: #eee;     
        border: 1px solid #eeeeee;
        /*border-radius: 10px ;*/
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
        margin-top:1rem; 
        word-break: normal;
    }
</style>

<h1 align="center"> Cross-task Attention Mechanism for Dense Multi-task Learning </h1>
<!-- Simple call of authors -->
<!-- <h3 align="center"> Ivan Lopes , Tuan-Hung Vu and Raoul de Charette </h3> -->
<!-- Alternatively you can add links to author pages -->
<h3 align="center"> <a href="https://wonjunior.github.io/">Ivan Lopes</a>&nbsp;&nbsp; <a href="https://tuanhungvu.github.io/">Tuan-Hung Vu</a>&nbsp;&nbsp; <a href="https://team.inria.fr/rits/membres/raoul-de-charette/">Raoul de Charette</a> </h3>

<h3 align="center"> WACV 2023 </h3>

<div align="center">
  <p>
    
    <a href="https://arxiv.org/abs/2206.08927"><i class="far fa-file-pdf"></i> Paper</a>&nbsp;&nbsp;
    
    
    <a href="https://github.com/cv-rits/DenseMTL"><i class="fab fa-github"></i> Code</a> &nbsp;&nbsp;
    
    
    
    
  </p>
</div>

<div class="publication-teaser">
    <img src="../../images/publications/2022_densemtl/teaser.png" alt="project teaser" />
</div>

<hr />

<h2 align="center"> Abstract</h2>

<p align="justify">Multi-task learning has recently become a promising solution for a comprehensive understanding of complex scenes. Not only being memory-efficient, multi-task models with an appropriate design can favor exchange of complementary signals across tasks. In this work, we jointly address 2D semantic segmentation, and two geometry-related tasks, namely dense depth, surface normal estimation as well as edge estimation showing their benefit on indoor and outdoor datasets. We propose a novel multi-task learning architecture that exploits pair-wise cross-task exchange through correlation-guided attention and self-attention to enhance the average representation learning for all tasks. We conduct extensive experiments considering three multi-task setups, showing the benefit of our proposal in comparison to competitive baselines in both synthetic and real benchmarks. We also extend our method to the novel multi-task unsupervised domain adaptation setting. Our code is available at https://github.com/cv-rits/DenseMTL.
</p>

<hr />

<h2 align="center">BibTeX</h2>
<left>
  <pre class="bibtex-box">
@inproceedings{lopes2023densemtl,
    title={Cross-task Attention Mechanism for Dense Multi-task Learning},
    author={Lopes, Ivan and Vu, Tuan-Hung and De Charette, Raoul},
    booktitle={Proceedings of IEEE/CVF Winter Conference on Applications of Computer Vision},
    year={2023}
}</pre>
</left>

<p><br /></p>
