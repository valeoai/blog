<style type="text/css">

/*h1 {
    margin-top:2.5em !important;
    color: #4a788c;
    font-size: 120%;
}

h2 {
    margin-top:1.5em !important;      
    color: #4a788c;

}

h3, h4 {
    margin-top:0.5em !important;      
    color: #4a788c;

}*/

h1 {
    margin-top:0.5em !important;
    margin-bottom:0.25em !important;
}

h3 {
    margin-bottom:0.25em !important;
    margin-top:0.25em !important;
}

p {
    margin-top:1rem !important; 
    margin-bottom:1rem !important;  
    font-size: 16px;

}

/*h1, h2, h3, h4 {
    font-weight: normal !important;
    margin-bottom:0.5em !important;  
    code {
      font-size: 100%;
    }
}   
*/

 .bibtex-box {
        background-color: #eee;     
        border: 1px solid #eeeeee;
        /*border-radius: 10px ;*/
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
        margin-top:1rem; 
        word-break: normal;
    }
</style>

<h1 align="center"> TRADI: Tracking deep neural network weight distributions for uncertainty estimation </h1>
<!-- Simple call of authors -->
<!-- <h3 align="center"> Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson, and Isabelle Bloch </h3> -->
<!-- Alternatively you can add links to author pages -->
<h3 align="center"> Gianni Franchi&nbsp;&nbsp; <a href="https://abursuc.github.io/">Andrei Bursuc</a>&nbsp;&nbsp; <a href="http://hebergement.u-psud.fr/emi/">Emanuel Aldea</a>&nbsp;&nbsp; Severine Dubuisson&nbsp;&nbsp; <a href="https://perso.telecom-paristech.fr/bloch/">Isabelle Bloch</a> </h3>

<h3 align="center"> ECCV 2020 </h3>

<div align="center">
  <p>
    
    <a href="https://arxiv.org/abs/1912.11316"><i class="far fa-file-pdf"></i> Paper</a>&nbsp;&nbsp;
    
    
    <a href="https://github.com/giannifranchi/TRADI_Tracking_DNN_weights"><i class="fab fa-github"></i> Code</a> &nbsp;&nbsp;
    
    
    
    
  </p>
</div>

<div class="publication-teaser">
    <img src="../../images/publications/tradi/tradi_teaser.jpg" alt="project teaser" />
</div>

<hr />

<h2 align="center"> Abstract</h2>

<p align="justify">During training, the weights of a Deep Neural Network (DNN) are optimized from a random initialization towards a nearly optimum value minimizing a loss function. Only this final state of the weights is typically kept for testing, while the wealth of information on the geometry of the weight space, accumulated over the descent towards the minimum is discarded. In this work we propose to make use of this knowledge and leverage it for computing the distributions of the weights of the DNN. This can be further used for estimating the epistemic uncertainty of the DNN by sampling an ensemble of networks from these distributions. To this end we introduce a method for tracking the trajectory of the weights during optimization, that does not require any changes in the architecture nor on the training procedure. We evaluate our method on standard classification and regression benchmarks, and on out-of-distribution detection for classification and semantic segmentation. We achieve competitive results, while preserving computational efficiency in comparison to other popular approaches.</p>

<p><br /></p>

<hr />

<h2 align="center"> Results</h2>

<p><img src="../../images/publications/tradi/synthetic_regression.jpg" alt="" /></p>
<div class="caption"><b>Results on a synthetic regression task comparing MC dropout, Deep Ensembles, and TRADI.</b> $x$-axis: spatial coordinate of the Gaussian process. Black lines: ground
truth curve. Blue points: training points. Orange areas: estimated variance.
</div>

<p><img src="../../images/publications/tradi/table_results.jpg" alt="" /></p>
<div class="caption"><b>Distinguishing in- and out-of-distribution data for semantic segmentation
(CamVid, StreetHazards, BDD Anomaly) and image classification (MNIST/notMNIST).</b>
</div>

<p><img src="../../images/publications/tradi/camvid_qualitative.jpg" alt="" /></p>
<div class="caption"><b>Qualitative results on CamVid-OOD.</b> <b>Columns:</b> $(a)$ input image and ground
truth; $(b)-(d)$ predictions and confidence scores by MC Dropout, Deep Ensembles, and
TRADI. <b>Rows:</b> $(1)$ input and confidence maps; $(2)$ class predictions; $(3)$ zoomed-in area
on input and confidence maps
</div>

<hr />

<h2 align="center">BibTeX</h2>
<left>
  <pre class="bibtex-box">
@article{franchi2019tradi,
  title={TRADI: Tracking deep neural network weight distributions for uncertainty estimation},
  author={Franchi, Gianni and Bursuc, Andrei and Aldea, Emanuel and Dubuisson, S{\'e}verine and Bloch, Isabelle},
  journal={arXiv preprint arXiv:1912.11316},
  year={2019}
}</pre>
</left>

<p><br /></p>
