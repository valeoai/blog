<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>valeo.ai at NeurIPS 2023 | valeo.ai blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="valeo.ai at NeurIPS 2023" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Victor Letzelter, Antonin Vobecky, Mickael Chen, Matthieu Cord, Andrei Bursuc" />
<meta property="og:description" content="Victor Letzelter, Antonin Vobecky, Mickael Chen, Matthieu Cord, Andrei Bursuc" />
<link rel="canonical" href="https://valeoai.github.io/blog/2023/12/08/valeoai-at-neurips-2023.html" />
<meta property="og:url" content="https://valeoai.github.io/blog/2023/12/08/valeoai-at-neurips-2023.html" />
<meta property="og:site_name" content="valeo.ai blog" />
<meta property="og:image" content="https://valeoai.github.io/blog/images/posts/2023_neurips/logo_neurips.svg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-12-08T00:00:00-06:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://valeoai.github.io/blog/images/posts/2023_neurips/logo_neurips.svg" />
<meta property="twitter:title" content="valeo.ai at NeurIPS 2023" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-12-08T00:00:00-06:00","datePublished":"2023-12-08T00:00:00-06:00","description":"Victor Letzelter, Antonin Vobecky, Mickael Chen, Matthieu Cord, Andrei Bursuc","headline":"valeo.ai at NeurIPS 2023","image":"https://valeoai.github.io/blog/images/posts/2023_neurips/logo_neurips.svg","mainEntityOfPage":{"@type":"WebPage","@id":"https://valeoai.github.io/blog/2023/12/08/valeoai-at-neurips-2023.html"},"url":"https://valeoai.github.io/blog/2023/12/08/valeoai-at-neurips-2023.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://valeoai.github.io/blog/feed.xml" title="valeo.ai blog" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-179942144-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>valeo.ai at NeurIPS 2023 | valeo.ai blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="valeo.ai at NeurIPS 2023" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Victor Letzelter, Antonin Vobecky, Mickael Chen, Matthieu Cord, Andrei Bursuc" />
<meta property="og:description" content="Victor Letzelter, Antonin Vobecky, Mickael Chen, Matthieu Cord, Andrei Bursuc" />
<link rel="canonical" href="https://valeoai.github.io/blog/2023/12/08/valeoai-at-neurips-2023.html" />
<meta property="og:url" content="https://valeoai.github.io/blog/2023/12/08/valeoai-at-neurips-2023.html" />
<meta property="og:site_name" content="valeo.ai blog" />
<meta property="og:image" content="https://valeoai.github.io/blog/images/posts/2023_neurips/logo_neurips.svg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-12-08T00:00:00-06:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://valeoai.github.io/blog/images/posts/2023_neurips/logo_neurips.svg" />
<meta property="twitter:title" content="valeo.ai at NeurIPS 2023" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-12-08T00:00:00-06:00","datePublished":"2023-12-08T00:00:00-06:00","description":"Victor Letzelter, Antonin Vobecky, Mickael Chen, Matthieu Cord, Andrei Bursuc","headline":"valeo.ai at NeurIPS 2023","image":"https://valeoai.github.io/blog/images/posts/2023_neurips/logo_neurips.svg","mainEntityOfPage":{"@type":"WebPage","@id":"https://valeoai.github.io/blog/2023/12/08/valeoai-at-neurips-2023.html"},"url":"https://valeoai.github.io/blog/2023/12/08/valeoai-at-neurips-2023.html"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://valeoai.github.io/blog/feed.xml" title="valeo.ai blog" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-179942144-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">valeo.ai blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/projects/">Projects</a><a class="page-link" href="/blog/publications/">Publications</a><a class="page-link" href="/blog/about/">About</a><a class="page-link" href="/blog/categories/">Tags</a><a class="page-link" href="/blog/search/">Search</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <!-- <style type="text/css">
.post-title {
 font-size: 42px !important; 
}
</style>
 -->

<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">valeo.ai at NeurIPS 2023</h1><p class="page-description">Victor Letzelter, Antonin Vobecky, Mickael Chen, Matthieu Cord, Andrei Bursuc</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2023-12-08T00:00:00-06:00" itemprop="datePublished">
        Dec 8, 2023
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#multi-sensor">multi-sensor</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#limited supervision">limited supervision</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#reliability">reliability</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#deep-learning">deep-learning</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#resilient-multiple-choice-learning-a-learned-scoring-scheme-with-application-to-audio-scene-analysis">Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis</a></li>
<li class="toc-entry toc-h2"><a href="#pop-3d-open-vocabulary-3d-occupancy-prediction-from-images">POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images</a></li>
<li class="toc-entry toc-h2"><a href="#rewarded-soups-towards-pareto-optimal-alignment-by-interpolating-weights-fine-tuned-on-diverse-rewards">Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards</a></li>
<li class="toc-entry toc-h2"><a href="#unifying-gans-and-score-based-diffusion-as-generative-particle-models">Unifying GANs and Score-Based Diffusion as Generative Particle Models</a></li>
</ul><p>The <a href="https://neurips.cc/">Neural Information Porcessing Systems Conference (NeurIPS)</a> is a major inter-disciplinary event that brings together researchers and practicioners in machine learning, computer vision, natural language processing, optimization, statistics, but also neuroscience, natural sciences, social sciences, etc. This year, at the thirty-seventh edition of NeurIPS, the <a href="https://ptrckprz.github.io/valeoai/">valeo.ai</a> team will present 4 papers in the main conference. We will be happy to discuss more about these projects and ideas, and share our exciting ongoing research. Take a quick view of our papers in the conference and come meet us at the posters, at our booth or in the hallway. Take a quick view of our papers and come meet us at the posters or catch us for a coffer in the hallways.</p>

<h2 id="resilient-multiple-choice-learning-a-learned-scoring-scheme-with-application-to-audio-scene-analysis">
<a class="anchor" href="#resilient-multiple-choice-learning-a-learned-scoring-scheme-with-application-to-audio-scene-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis</h2>
<h4 id="authors-victor-letzelter-mathieu-fontaine-mickaÃ«l-chen-patrick-pÃ©rez-slim-essid-gaÃ«l-richard">Authors: Victor Letzelter, Mathieu Fontaine, MickaÃ«l Chen, Patrick PÃ©rez, Slim Essid, GaÃ«l Richard</h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2311.01052">Paper</a>] Â Â  [<a href="https://github.com/Victorletzelter/code-rMCL">Code</a>] Â Â  [<a href="https://valeoai.github.io/blog/publications/rmcl/">Project page</a>]</h4>

<p>In this work, we tackle ambiguous machine learning tasks, where single predictions donâ€™t suffice due to the taskâ€™s nature or inherent uncertainties.</p>

<p>We introduce a robust multi-hypotheses framework that is capable of deterministically offering a range of plausible predictions at inference time. 
Our experiments on both synthetic data and real-world audio data affirm the potential and versatility of our method. Check out the paper and the code for more details.</p>

<p><img src="/blog/images/posts/2023_neurips/training_dynamics.gif" alt="rmcl_overview" height="100%" width="100%"></p>

<p>This problem involves estimating a conditional distribution that is dependent on the input. The accompanying animation illustrates the early stages in the evolution of our modelâ€™s learning process, highlighting how it progressively refines its predictions (represented by shaded blue points) to the actual data distribution (indicated by green points), which varies with the input â€˜tâ€™.</p>

<hr>

<h2 id="pop-3d-open-vocabulary-3d-occupancy-prediction-from-images">
<a class="anchor" href="#pop-3d-open-vocabulary-3d-occupancy-prediction-from-images" aria-hidden="true"><span class="octicon octicon-link"></span></a>POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images</h2>

<h4 id="authors-antonin-vobecky-oriane-simÃ©oni-david-hurych-spyros-gidaris-andrei-bursuc-patrick-pÃ©rez-josef-sivic">Authors: Antonin Vobecky, Oriane SimÃ©oni, David Hurych, Spyros Gidaris, Andrei Bursuc, Patrick PÃ©rez, Josef Sivic</h4>

<h4 align="center"> [<a href="https://openreview.net/forum?id=eBXM62SqKY">Paper</a>] Â Â  [<a href="https://github.com/vobecant/POP3D">Code</a>] Â Â  [<a href="https://vobecant.github.io/POP3D">Project page</a>]</h4>

<p>POP-3D is an approach to predict open-vocabulary 3D semantic voxel occupancy map from input 2D images to enable 3D grounding, segmentation, and retrieval of free-form language queries.</p>

<p><img src="/blog/images/posts/2023_neurips/pop3d-overview.png" alt="pop3d_overview" height="100%" width="100%"></p>
<div class="caption">Given surround-view images on the input, our POP-3D outputs voxel occupancy with 3D-language features, which one can query using text, e.g., to obtain zero-shot semantic segmentation.
</div>

<p>We design a new model architecture for open-vocabulary 3D semantic occupancy prediction. The architecture consists of a 2D-3D encoder together with occupancy prediction and 3D-language heads. The output is a dense voxel map of 3D grounded language embeddings enabling a range of open-vocabulary tasks. Next, we develop a tri-modal self-supervised learning algorithm that leverages three modalities: (i) images, (ii) language, and (iii) LiDAR point clouds and enables training the proposed architecture using a strong pre-trained vision-language model without the need for any 3D manual language annotations.</p>

<p><img src="/blog/images/posts/2023_neurips/pop3d-model.png" alt="pop3d_model" height="100%" width="100%"></p>
<div class="caption">Overview of POP-3D architecture and training approach.</div>

<p>Finally, we demonstrate the strengths of the proposed model quantitatively on several open-vocabulary tasks: Zero-shot 3D semantic segmentation using existing datasets; 3D grounding, and retrieval of free-form language queries, using a small dataset that we propose as an extension of nuScenes.</p>

<p><img src="/blog/images/posts/2023_neurips/pop3d-qualitative.png" alt="pop3d_example" height="100%" width="100%"></p>

<hr>

<h2 id="rewarded-soups-towards-pareto-optimal-alignment-by-interpolating-weights-fine-tuned-on-diverse-rewards">
<a class="anchor" href="#rewarded-soups-towards-pareto-optimal-alignment-by-interpolating-weights-fine-tuned-on-diverse-rewards" aria-hidden="true"><span class="octicon octicon-link"></span></a>Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards</h2>
<h4 id="authors-alexandre-ramÃ©-guillaume-couairon-mustafa-shukor-corentin-dancette-jean-baptiste-gaya-laure-soulier-matthieu-cord">Authors: Alexandre RamÃ©, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste Gaya, Laure Soulier, Matthieu Cord</h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2306.04488">Paper</a>] Â Â  [<a href="https://github.com/alexrame/rewardedsoups">Code</a>] Â Â  [<a href="https://huggingface.co/spaces/alexrame/rewardedsoups">Project page</a>]</h4>

<p>Foundation models are first pre-trained on vast unsupervised datasets and then fine-tuned on labeled data. Reinforcement learning, notably from human feedback (RLHF), can further align the network with the intended usage. Yet the imperfections in the proxy reward may hinder the training and lead to suboptimal results; the diversity of objectives in real-world tasks and human opinions exacerbate the issue. This paper proposes embracing the heterogeneity of diverse rewards by following a multi-policy strategy. Rather than focusing on a single a priori reward, we aim for Pareto-optimal generalization across the entire space of preferences. To this end, we propose rewarded soup, first specializing multiple networks independently (one for each proxy reward) and then interpolating their weights linearly. This succeeds empirically because we show that the weights remain linearly connected when fine-tuned on diverse rewards from a shared pre-trained initialization. We demonstrate the effectiveness of our approach for text-to-text (summarization, Q&amp;A, helpful assistant, review), text-image (image captioning, text-to-image generation, visual grounding, VQA), and control (locomotion) tasks. We hope to enhance the alignment of deep models, and how they interact with the world in all its diversity.</p>

<p><img src="/blog/images/posts/2023_neurips/rewarded-soups.png" alt="rs_overview" height="100%" width="100%"></p>
<div class="caption">
<b>Illustration of the different steps of our proposed rewarded soup (RS).</b>  After unsupervised pre-training and supervised fine-tuning, we launch $N$ independent RL fine-tunings on the proxy rewards $\{R_i\}^{N}_{i=1}$. Then we combine the trained networks by interpolation in the weight space. The final weights are adapted at test time by selecting the coefficient $\lambda$.</div>

<hr>

<h2 id="unifying-gans-and-score-based-diffusion-as-generative-particle-models">
<a class="anchor" href="#unifying-gans-and-score-based-diffusion-as-generative-particle-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Unifying GANs and Score-Based Diffusion as Generative Particle Models</h2>
<h4 id="authors-jean-yves-franceschi-mike-gartrell-ludovic-dos-santos-thibaut-issenhuth-emmanuel-de-bÃ©zenac-mickaÃ«l-chen-alain-rakotomamonjy">Authors: Jean-Yves Franceschi, Mike Gartrell, Ludovic Dos Santos, Thibaut Issenhuth, Emmanuel de BÃ©zenac, MickaÃ«l Chen, Alain Rakotomamonjy</h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2305.16150">Paper</a>] Â Â  [Code (soon)]</h4>

<p>By describing the trajectories of GAN outputs during training with particle evolution equations, we propose an unifying framework for GAN and Diffusion Models. We provide a new insights on the role of the generator network, and as proof of concept validating our theories, we propose methods to train a generator with score-based gradient instead of a discriminator, or to use a discriminatorâ€™s gradient flow to generate instead of training a generator.</p>

<p><img src="/blog/images/posts/2023_neurips/unify-gan.png" alt="unigan_overview" height="70%" width="70%"></p>


  </div><a class="u-url" href="/blog/2023/12/08/valeoai-at-neurips-2023.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="https://valeoai.github.io/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>valeo.ai research blog</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
