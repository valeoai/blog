<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://valeoai.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://valeoai.github.io/blog/" rel="alternate" type="text/html" /><updated>2020-10-21T10:52:08-05:00</updated><id>https://valeoai.github.io/blog/feed.xml</id><title type="html">valeo.ai blog</title><subtitle>valeo.ai research blog</subtitle><entry><title type="html">Is Deep Reinforcement Learning Really Superhuman on Atari?</title><link href="https://valeoai.github.io/blog/2020/10/19/saber.html" rel="alternate" type="text/html" title="Is Deep Reinforcement Learning Really Superhuman on Atari?" /><published>2020-10-19T00:00:00-05:00</published><updated>2020-10-19T00:00:00-05:00</updated><id>https://valeoai.github.io/blog/2020/10/19/saber</id><content type="html" xml:base="https://valeoai.github.io/blog/2020/10/19/saber.html">&lt;p&gt;&lt;em&gt;This post describes our recent &lt;a href=&quot;https://arxiv.org/pdf/1908.04683.pdf&quot;&gt;work&lt;/a&gt; on Deep Reinforcement Learning (DRL) on the Atari benchmark. DRL appears today as one of the closest paradigm to Artificial General Intelligence and large progress in this field has been enabled by the popular Atari benchmark. However, training and evaluation protocols on Atari vary across papers leading to biased comparisons, difficulty in reproducing results and in estimating the true contributions of the methods. Here we attempt to mitigate this problem with SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms. SABER allows us to compare multiple methods under the same conditions against a human baseline and to note that previous claims of superhuman performance on DRL do not hold. Finally, we propose a new state-of-the-art algorithm R-IQN combining Rainbow with Implicit Quantile Networks (IQN). Our code is &lt;a href=&quot;https://github.com/valeoai/rainbow-iqn-apex&quot;&gt;available&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Deep Reinforcement Learning is a learning scheme based on trial-and-error in which an agent learns an optimal policy from its own experiments and a reward signal. The goal of the agent is to maximize the sum of future accumulated rewards and thus the agent needs to think about sequences of actions rather than instantaneous ones. The Atari benchmark is valuable for evaluating general AI algorithms as it includes more than 50 games displaying high variability in the task to solve, ranging from simple paddle control in the ball game Pong to complex labyrinth exploration in Montezuma’s Revenge which remains unsolved by general algorithms up to today.&lt;/p&gt;

&lt;p&gt;We notice however that training and evaluation procedures on Atari can be different from paper to paper and thus leading to bias in comparison. Moreover this leads to difficulties to reproduce results of published works as some training or evaluation parameters are barely explained or sometimes not mentioned. In order to facilitate reproducible and comparable DRL, we introduce SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms. Furthermore, we introduce a human world record baseline and argue that previous claims of superhuman performance of DRL might not hold. Finally, we propose a new state-of-the-art algorithm R-IQN by combining the current state-of-the-art &lt;strong&gt;Rainbow&lt;/strong&gt; &lt;a class=&quot;citation&quot; href=&quot;#hessel2017rainbow&quot;&gt;(Hessel et al., 2018)&lt;/a&gt; along with Implicit Quantile Networks (&lt;strong&gt;IQN&lt;/strong&gt; &lt;a class=&quot;citation&quot; href=&quot;#dabney2018implicit&quot;&gt;(Dabney et al., 2018)&lt;/a&gt;). We release an open-source &lt;a href=&quot;https://github.com/valeoai/rainbow-iqn-apex&quot;&gt;implementation&lt;/a&gt; of distributed R-IQN following the ideas from Ape-X!&lt;/p&gt;

&lt;h2 id=&quot;dqns-human-baseline-vs-human-world-record-on-atari-games&quot;&gt;DQN’s human baseline vs human world record on Atari Games&lt;/h2&gt;

&lt;p&gt;A common way to evaluate AI for games is to let agents compete against the best humans. Recent examples for DRL include the victory of &lt;a href=&quot;https://deepmind.com/alphago-korea&quot;&gt;AlphaGo versus Lee Sedol&lt;/a&gt; for Go, &lt;a href=&quot;https://openai.com/blog/openai-five/&quot;&gt;OpenAI Five&lt;/a&gt; on Dota 2 or &lt;a href=&quot;https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii&quot;&gt;AlphaStar versus Mana&lt;/a&gt; for StarCraft 2. For this reason one of the most used metrics for evaluating RL agents on Atari is to compare them to the human baseline introduced in DQN.&lt;/p&gt;

&lt;p&gt;Previous works use the normalized human score, &lt;em&gt;i.e&lt;/em&gt;., 0% is the score of a random player and 100% is the score of the human baseline, which allows one to summarize the performance on the whole Atari set in one number, instead of individually comparing raw scores for each of the 61 games. However we argue that this human baseline is far from being representative of the best human player, which means that using it to claim superhuman performance is misleading.&lt;/p&gt;

&lt;p&gt;The current world records are available online for 58 of the 61 evaluated Atari games. For example, on VideoPinball, the world record is 50,000 times higher than the human baseline of DQN. Evaluating these world records scores using the usual human normalized score has a median of 4,400% and a mean of 99,300% (see Figure below for details on each game), to be compared to 200% and 800% of the current state-of-the-art Rainbow!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/saber/pro_vs_beginner.png&quot; alt=&quot;pro_vs_beginner&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 1: World record scores vs. the usual beginner human baseline&lt;/b&gt; (log scale) &lt;a class=&quot;citation&quot; href=&quot;#dabney2018implicit&quot;&gt;(Dabney et al., 2018)&lt;/a&gt; baseline can be up to 50k times lower than registered world records. Beating that baseline does not necessarily make the agent superhuman.&lt;/div&gt;

&lt;p&gt;We estimate that evaluating the algorithms under the world record baseline instead of the DQN human baseline will give a better view of the gap remaining between best human players and DRL agents.
Results are confirming this, as Rainbow reaches only a median human-normalized score of 3% (see Figure 2 below) meaning that for half of Atari games, the agent doesn’t even reach 3% of the way from random to best human run.&lt;/p&gt;

&lt;p&gt;In the video below we analyze agents previously claimed as above human-level but far from the world record. By taking a closer look at the AI playing, we discovered that on most of Atari games DRL agents fail to understand the goal of the game. Sometimes they just don’t explore other levels than the initial one, sometimes they are stuck in a loop giving small amount of reward, etc. A compelling example of this is the game Riverraid. In this game, the agent must shoot everything and take fuel to survive: the agent dies if there is a collision with an enemy or if out of fuel. But as shooting fuel actually gives points, the agent doesn’t understand that he could play way longer and win even more points by actually taking this fuel bonus and not shooting them!&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/oH6P3ksYLek&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot; align=&quot;center&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;h2 id=&quot;saber-a-standardized-atari-benchmark-for-general-reinforcement-learning-algorithms&quot;&gt;SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms&lt;/h2&gt;

&lt;p&gt;In our work we extend the recommendations proposed by &lt;a class=&quot;citation&quot; href=&quot;#machado2018revisiting&quot;&gt;(Machado et al., 2018)&lt;/a&gt; They also point out divergences in training and evaluating agents on Atari. Consequently they compile and propose a set of recommendations for more reproducible and comparable RL including sticky actions, ignoring life signal and using full action set.&lt;/p&gt;

&lt;p&gt;There is however one major parameter that is left out in &lt;a class=&quot;citation&quot; href=&quot;#machado2018revisiting&quot;&gt;(Machado et al., 2018)&lt;/a&gt;: the maximum number of frames allowed per episode. This parameter ends the episode after a fixed number of time steps even if the game is not over. In most of the recent works, this is set to 30 min of game play (i.e., 108k frames) and only to 5 min in some others (i.e.,18k frames). This means that the reported scores can not be compared fairly. For example, in easy games (e.g., Atlantis), the agent never dies and the score is more or less linear with the allowed time: the reported score will be 6 times higher if capped at 30 minutes instead of 5 minutes.&lt;/p&gt;

&lt;p&gt;Another issue with this time cap comes from the fact that some games are designed for much longer gameplay than 5 or 30 minutes. On those games (e.g., Atlantis, Video Pinball, Enduro) the scores reported of Ape-X, Rainbow and IQN are almost the same. This is due to all agents reaching the time limit and getting the maximum possible score in 30 minutes: the difference in scores is due to minor variations, not algorithmic difference and thus the comparison is not significant. As a consequence, the more successful agents are, the more games are incomparable because they reach the maximum possible score in the time cap while still being far behind human world record.&lt;/p&gt;

&lt;p&gt;This parameter can also be a source of ambiguity and error. The best score on Atlantis (2,311,815) is reported by Proximal Policy Optimization by &lt;a class=&quot;citation&quot; href=&quot;#schulman2017proximal&quot;&gt;(Schulman et al., 2017)&lt;/a&gt;, however this score is likely wrong: it seems impossible to reach in 30 minutes only! The first distributional paper by (&lt;a href=&quot;http://proceedings.mlr.press/v70/bellemare17a.html&quot;&gt;Bellemare et al.&lt;/a&gt;) also did this mistake and reported wrong results before adding an erratum in a later version on &lt;em&gt;ArXiv&lt;/em&gt; &lt;a class=&quot;citation&quot; href=&quot;#bellemare2017distributional&quot;&gt;(Bellemare et al., 2017)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/saber/saber_params.png&quot; alt=&quot;saber_params&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Table 1:&lt;/b&gt; Game parameters of SABER.&lt;/div&gt;

&lt;p&gt;We first re-implemented the current state-of-the-art Rainbow and evaluated it on SABER. We noticed that the same algorithm under different evaluation settings can lead to significantly different results. This showed again the necessity of a common and standardized benchmark, more details can be found in the paper.&lt;/p&gt;

&lt;p&gt;Then we implemented a new algorithm, R-IQN, by replacing the C51 algorithm (which is one of the 6 components of Rainbow) by Implicit Quantile Network (IQN). Both C51 and IQN belong to the field of Distributional RL which aims to predict the full distribution of the Q-value function instead of just predicting the mean of it. The fundamental difference between these two algorithms is how they parametrize the Q-value distribution. C51, which is the first algorithm of Distributional RL, approximates the Q-value as a categorical distribution with fixed support and just learns the mass to attribute to each category. On the other hand, IQN approximates the Q-value with quantile regression and both the support and the mass arelearned resulting in a major improvement in performance and data-efficiency over C51. As IQN arises much better performance than C51 while still designed for the same goal (predict the full distribution of the Q-function), combining Rainbow with IQN is relevant and natural.&lt;/p&gt;

&lt;p&gt;As shown in the graph below, R-IQN outperforms Rainbow and thus becomes the new state-of-the-art on Atari. However, we acknowledge that in order to make a more confident state-of-the-art claim we should run multiple times with different seeds. Testing an increased number of random seeds across the 60 Atari games is a computationally costly endeavor and is beyond the scope of this study. We test the stability of the performances of R-IQN across 5 random seeds on a subset of 14 games. We compare against Rainbow and report similar results in Figure 3.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/saber/median_human_normalised_Rainbow_vs_Rainbow_IQN.png&quot; alt=&quot;median_rainbow_vs_us&quot; height=&quot;90%&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 2: Comparison of Rainbow and Rainbow-IQN on SABER.&lt;/b&gt; We report median normalized scores w.r.t training steps.&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/saber/median_human_normalised_5_seeds_both_rainbow_riqn.png&quot; alt=&quot;median_rainbow_vs_us_seeds&quot; height=&quot;90%&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 3: Comparison of Rainbow and Rainbow-IQN on a subset of 14 games using 5 seeds.&lt;/b&gt; We report median normalized scores w.r.t training steps.&lt;/div&gt;

&lt;h2 id=&quot;open-source-implementation-of-distributed-rainbow-iqn-r-iqn-ape-x&quot;&gt;Open-source implementation of distributed Rainbow-IQN: R-IQN Ape-X&lt;/h2&gt;

&lt;p&gt;We release our code of a distributed version of Rainbow-IQN following ideas from Ape-X &lt;a class=&quot;citation&quot; href=&quot;#horgan2018distributed&quot;&gt;(Horgan et al., 2018)&lt;/a&gt;. The distributed part is our main practical advantage over some existing DRL repositories (particularly &lt;a href=&quot;https://github.com/google/dopamine&quot;&gt;Dopamine&lt;/a&gt; a popular open-source implementation of DQN, C51, IQN and a &lt;em&gt;small-Rainbow&lt;/em&gt; but in which all algorithms are single worker).
Indeed, when using DRL algorithms for other tasks (other than Atari and MuJoCO) a major bottleneck is the &lt;em&gt;speed&lt;/em&gt; of the environment. DRL algorithms often need a huge amount of data before reaching reasonable performance. This amount may be practically impossible to reach if the environment is real-time and if collecting data from multiple environments at the same time is not possible.&lt;/p&gt;

&lt;p&gt;Building upon ideas from Ape-X, we use REDIS as a side server to store our replay memory. Multiple actors will act in their own instances of the environment to fill as fast as they can the replay memory. Finally, the learner will sample from this replay memory (the learner is actually completely independent of the environment) for backprop. The learner will also periodically update the weight of each actor as shown in the schema below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/saber/appex_arch.png&quot; alt=&quot;apex_arch&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 4: Ape-X architecture.&lt;/b&gt; image taken from &lt;a class=&quot;citation&quot; href=&quot;#horgan2018distributed&quot;&gt;(Horgan et al., 2018)&lt;/a&gt;&lt;/div&gt;

&lt;p&gt;This scheme allowed us to use R-IQN Ape-X for the task of autonomous driving using &lt;a href=&quot;http://carla.org/&quot;&gt;CARLA&lt;/a&gt; as environment. This enabled us to win the &lt;a href=&quot;https://carlachallenge.org/results-challenge-2019/&quot;&gt;CARLA Challenge&lt;/a&gt; on Track 2 &lt;em&gt;Cameras Only&lt;/em&gt; showing the strength of R-IQN Ape-X as a general algorithm.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this work, we confirm the impact of standardized guidelines for DRL evaluation, and build a consolidated benchmark, SABER. In order to provide a more significant comparison, we build a new baseline based on human world records and show that the state-of-the-art Rainbow agent is in fact far from human world record performance. In the paper we share possible reasons for this failure. We hope that SABER will facilitate better comparisons and enable new exciting methods to prove their effectiveness without ambiguity.&lt;/p&gt;

&lt;p&gt;Check out our paper to find out more about intuitions, experiments and interpretations.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;hessel2017rainbow&quot;&gt;Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., &amp;amp; Silver, D. (2018). Rainbow: Combining improvements in deep reinforcement learning. &lt;i&gt;AAAI&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;dabney2018implicit&quot;&gt;Dabney, W., Ostrovski, G., Silver, D., &amp;amp; Munos, R. (2018). Implicit quantile networks for distributional reinforcement learning. &lt;i&gt;ICML&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;machado2018revisiting&quot;&gt;Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., &amp;amp; Bowling, M. (2018). Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. &lt;i&gt;Journal of Artificial Intelligence Research&lt;/i&gt;, &lt;i&gt;61&lt;/i&gt;, 523–562.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;schulman2017proximal&quot;&gt;Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp;amp; Klimov, O. (2017). Proximal policy optimization algorithms. &lt;i&gt;ArXiv Preprint ArXiv:1707.06347&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bellemare2017distributional&quot;&gt;Bellemare, M. G., Dabney, W., &amp;amp; Munos, R. (2017). A distributional perspective on reinforcement learning. &lt;i&gt;ArXiv Preprint ArXiv:1707.06887&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;horgan2018distributed&quot;&gt;Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., Van Hasselt, H., &amp;amp; Silver, D. (2018). Distributed prioritized experience replay. &lt;i&gt;ArXiv Preprint ArXiv:1803.00933&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name></name></author><summary type="html">This post describes our recent work on Deep Reinforcement Learning (DRL) on the Atari benchmark. DRL appears today as one of the closest paradigm to Artificial General Intelligence and large progress in this field has been enabled by the popular Atari benchmark. However, training and evaluation protocols on Atari vary across papers leading to biased comparisons, difficulty in reproducing results and in estimating the true contributions of the methods. Here we attempt to mitigate this problem with SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms. SABER allows us to compare multiple methods under the same conditions against a human baseline and to note that previous claims of superhuman performance on DRL do not hold. Finally, we propose a new state-of-the-art algorithm R-IQN combining Rainbow with Implicit Quantile Networks (IQN). Our code is available.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io/blog/images/posts/saber/space_invaders.jpg" /><media:content medium="image" url="https://valeoai.github.io/blog/images/posts/saber/space_invaders.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">ADVENT - Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation</title><link href="https://valeoai.github.io/blog/2020/07/07/advent-domain-adaptation.html" rel="alternate" type="text/html" title="ADVENT - Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation" /><published>2020-07-07T00:00:00-05:00</published><updated>2020-07-07T00:00:00-05:00</updated><id>https://valeoai.github.io/blog/2020/07/07/advent-domain-adaptation</id><content type="html" xml:base="https://valeoai.github.io/blog/2020/07/07/advent-domain-adaptation.html">&lt;p&gt;&lt;em&gt;This post describes our &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/html/Vu_ADVENT_Adversarial_Entropy_Minimization_for_Domain_Adaptation_in_Semantic_Segmentation_CVPR_2019_paper.html&quot;&gt;recent work&lt;/a&gt; on unsupervised domain adaptation for semantic segmentation presented at &lt;a href=&quot;http://cvpr2019.thecvf.com/&quot;&gt;CVPR 2019&lt;/a&gt;. ADVENT is a flexible technique for bridging the gap between two different domains through entropy minimization. Our work builds upon a simple observation: models trained only on source domain tend to produce over-confident, i.e., low-entropy, predictions on source-like images and under-confident, i.e., high-entropy, predictions on target-like ones. Consequently by minimizing the entropy on the target domain, we make the feature distributions from the two domains more similar. We show that our approach achieves competitive performances on standard semantic segmentation benchmarks and that it can be successfully extended to other tasks such as object detection.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Visual perception is a remarkable ability that human drivers leverage for understanding their surroundings and for supporting the multiple micro-decisions needed in traffic. Since many years, researchers have been working on mimicking this human capability by means of computer algorithms. This research field is known as computer vision and it has seen impressive progress and wide adoption. Most of the modern &lt;em&gt;computer vision&lt;/em&gt; systems rely on Deep Neural Networks (DNNs) which are powerful and widely employed tools able to learn from large amounts of data and make accurate predictions. In autonomous driving, DNN-based visual perception is also at the heart of the complex architectures under intelligent cars, and supports downstream decisions of the vehicle, &lt;em&gt;e.g.,&lt;/em&gt; steering, braking, signaling, etc.&lt;/p&gt;

&lt;p&gt;The diversity and complexity of the situations encountered in real-world driving is tremendous. Unlike humans who can extrapolate effortlessly from previous experience in order to adapt to new environments and conditions, the scope of DNNs beyond the types of conditions and scenes seen during training is limited. For instance a model trained on data from a sunny country, would have a hard time delivering the same performance on streets with mixed weather conditions in a different country (with different urban architecture, furniture, vegetation, types of cars and pedestrian appearance and clothing). Similarly a model trained on a particular type of camera, is expected to see a drop in performance with images coming from a camera with different specifications. This difference between environments that leads to performance drops is referred to as &lt;em&gt;domain gap&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;bridging-domains&quot;&gt;Bridging domains&lt;/h2&gt;

&lt;p&gt;We can resort to two options for narrowing the domain gap: (i) annotate more data; (ii) leverage the experience acquired on an initial environment and transfer it to the new environment. More annotated data has been shown to always improve performance of DNNs &lt;a class=&quot;citation&quot; href=&quot;#sun2017revisiting&quot;&gt;(Sun et al., 2017)&lt;/a&gt;. However the labeling process brings a significant financial and temporal burden. The time required for a high-quality annotation, such as the ones from the popular Cityscapes dataset is ∼90 minutes per image &lt;a class=&quot;citation&quot; href=&quot;#cordts2016cityscapes&quot;&gt;(Cordts et al., 2016)&lt;/a&gt;. The amount of images required to train high performance DNNs typically counts in hundreds of thousands. The acquisition of diverse data across seasons and weather conditions adds up even more time. It makes then sense to look for a solution elsewhere and the second option seems now more appealing, though achieving it remains technically challenging. This is actually the area of research of domain adaptation (DA) which addresses the domain-gap problem by transferring knowledge from a source domain (with full annotations) to a target domain (with fewer annotations if any), aiming to reach good performances on target samples. DA has consistently attracted interest from different communities across years &lt;a class=&quot;citation&quot; href=&quot;#csurka2017domain&quot;&gt;(Csurka, 2017)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here we are working on &lt;em&gt;Unsupervised DA&lt;/em&gt; (UDA), which is a more challenging task where we have access to labeled source samples and only unlabeled target samples. We use as source, data generated by a simulator or video game engine, while for target we consider real-data from car-mounted cameras. In &lt;em&gt;Figure 1&lt;/em&gt; we illustrate the difficulty of this task and the impact of our UDA technique, ADVENT.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/advent/advent_teaser.png&quot; alt=&quot;advent_teaser&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 1. Proposed entropy-based unsupervised domain adaptation for semantic segmentation.&lt;/b&gt; The top two rows show results on source and target domain scenes of the model trained without adaptation. The bottom row shows the result on the same target domain scene of the model trained with entropy-based adaptation. The left and right columns visualize respectively the semantic segmentation outputs and the corresponding prediction entropy maps.&lt;/div&gt;

&lt;p&gt;The main approaches for UDA include discrepancy minimization between source and target feature distributions usually achieved via adversarial training  &lt;a class=&quot;citation&quot; href=&quot;#ganin2015unsupervised&quot;&gt;(Ganin &amp;amp; Lempitsky, 2015)&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#tzeng2017adversarial&quot;&gt;(Tzeng et al., 2017)&lt;/a&gt;, self-training with pseudo-labels &lt;a class=&quot;citation&quot; href=&quot;#zou2018unsupervised&quot;&gt;(Zou et al., 2018)&lt;/a&gt; and generative approaches &lt;a class=&quot;citation&quot; href=&quot;#hoffman2018cycada&quot;&gt;(Hoffman et al., 2018)&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#wu2018dcan&quot;&gt;(Wu et al., 2018)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Entropy minimization&lt;/em&gt; has been shown to be useful for semi-supervised learning &lt;a class=&quot;citation&quot; href=&quot;#grandvalet2005semi&quot;&gt;(Grandvalet &amp;amp; Bengio, 2005)&lt;/a&gt;, clustering &lt;a class=&quot;citation&quot; href=&quot;#jain2018learning&quot;&gt;(Jain et al., 2018)&lt;/a&gt; and more recently to domain adaptation for classification &lt;a class=&quot;citation&quot; href=&quot;#long2016unsupervised&quot;&gt;(Long et al., 2016)&lt;/a&gt;. We chose to explore entropy based UDA training to obtain competitive performance on semantic segmentation.&lt;/p&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;p&gt;We present our two proposed approaches for entropy minimization using (i) an unsupervised entropy loss and (ii) adversarial training. To build our models, we start from existing semantic segmentation frameworks and add an additional network branch used for domain adaptation. &lt;em&gt;Figure 2&lt;/em&gt; illustrates our architectures.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/advent/advent_approach.jpg&quot; alt=&quot;&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 2. Approach overview.&lt;/b&gt; First, direct entropy minimization decreases the entropy of the target $P_{x_t}$, which is equivalent to minimizing the sum of weighted self-information maps $I_{x_t}$. In the second approach, we use adversarial training to enforce the consistency in $P_x$ across domains. Red arrows are used for target domain, blue arrows for source.
&lt;/div&gt;

&lt;h3 id=&quot;direct-entropy-minimization&quot;&gt;Direct entropy minimization&lt;/h3&gt;

&lt;p&gt;On the source domain we train our model, denoted as $F$, as usual using a supervised loss. For the target domain, we do not have annotations and we can no longer use the segmentation loss to train $F$. We notice that models trained only on source domain tend to produce over-confident predictions on source-like images and under-confident predictions on target-like ones. Motivated by this observation, we propose a supervision signal that could leverage visual information from the target samples, in spite of the lack of annotations. The objective is to constrain $F$ to produce high-confident predictions on target samples similarly to source samples. To this effect, we introduce the entropy loss $\mathcal{L}_{ent}$​ to maximize directly the prediction confidence in the target domain. Here we consider the Shannon Entropy (&lt;a href=&quot;http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf&quot;&gt;Shannon&lt;/a&gt;). During training, we jointly optimize the supervised segmentation loss $\mathcal{L}_{seg}$ on source samples and the unsupervised entropy loss $\mathcal{L}_{ent}$​​ on target samples.&lt;/p&gt;

&lt;h3 id=&quot;entropy-minimization-by-adverarial-learning&quot;&gt;Entropy minimization by adverarial learning&lt;/h3&gt;

&lt;p&gt;A limitation of the entropy loss is related to the absence of structural dependencies between local semantics. This is caused by the aggregation of the pixel-wise prediction entropies by summation. We address this through a unified adversarial training framework which minimizes indirectly the entropy of target data, by encouraging it to become similar to the source one. This allows the exploitation of the structural consistency between the domains. To this end, we formulate the UDA task as minimizing distribution distance between source and target on the &lt;em&gt;weighted self-information&lt;/em&gt; space. Since the trained model produces naturally low-entropy predictions on source-like images, by aligning weighted self-information distributions of target and source domains, we reach the same behavior on target-like data.&lt;/p&gt;

&lt;p&gt;We perform the adversarial adaptation on weighted self-information maps using a fully-convolutional discriminator network $D$. The discriminator produces domain classification outputs, &lt;em&gt;i.e.,&lt;/em&gt; class label $1$ (resp. $0$) for the source (resp. target) domain. We train $D$ to discriminate outputs coming from source and target images, and at the same time, train the segmentation network to fool the discriminator.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;We evaluate our approaches on the challenging &lt;em&gt;synthetic-2-real&lt;/em&gt; unsupervised domain adaptation set-ups. Models are trained on fully-annotated synthetic data and are validated on real-world data. In such set-ups, the models have access to some unlabeled real images during training.&lt;/p&gt;

&lt;h3 id=&quot;semantic-segmentation&quot;&gt;Semantic Segmentation&lt;/h3&gt;

&lt;p&gt;To train our models, we use either GTA5 &lt;a class=&quot;citation&quot; href=&quot;#richter2016playing&quot;&gt;(Richter et al., 2016)&lt;/a&gt; or SYNTHIA &lt;a class=&quot;citation&quot; href=&quot;#ros2016synthia&quot;&gt;(Ros et al., 2016)&lt;/a&gt; as source synthetic data, along with the training split of Cityscapes dataset &lt;a class=&quot;citation&quot; href=&quot;#cordts2016cityscapes&quot;&gt;(Cordts et al., 2016)&lt;/a&gt; as target domain data.&lt;/p&gt;

&lt;p&gt;In &lt;em&gt;Table 1&lt;/em&gt; we report our results on semantic segmentation from models trained on GTA5 $\rightarrow$ Cityscapes and from SYNTHIA $\rightarrow$ Cityscapes. We compare here only with the top performing method Adapt-SegMap &lt;a class=&quot;citation&quot; href=&quot;#tsai2018learning&quot;&gt;(Tsai et al., 2018)&lt;/a&gt;, while additional baselines and related methods are covered in the paper.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/advent/advent_table1.png&quot; alt=&quot;&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Table 1. Segmentation performance in mIoU with ResNet-101 based model and Deeplab-V2 as the segmentation network.&lt;/b&gt;We show results of our approaches using the direct entropy loss (MinEnt) and using adversarial training (AdvEnt).&lt;/div&gt;

&lt;p&gt;Our first approach of direct entropy minimization (&lt;em&gt;MinEnt&lt;/em&gt;) achieves comparable performance to state-of-the-art baselines. The light overhead of the entropy loss makes training time shorter for the MinEnt model, while being easier train compared to adversarial networks. Our second approach using adversarial training on the weighted self-information space, noted as &lt;em&gt;AdvEnt&lt;/em&gt;, shows consistent improvement to the baselines. In general, AdvEnt works better than MinEnt, confirming the importance of structural adaptation. The two approaches are complementary as their combination boosts performance further.&lt;/p&gt;

&lt;p&gt;In &lt;em&gt;Figure 3&lt;/em&gt;, we illustrate a few qualitative results of our models. Without domain adaptation, the model trained only on source supervision produces noisy segmentation predictions as well as high entropy activations, with a few exceptions on some classes like &lt;em&gt;“building”&lt;/em&gt; and &lt;em&gt;“car”&lt;/em&gt;. However, there are many confident predictions (low entropy) which are completely wrong. Our models, on the other hand, manage to produce correct predictions at high level of confidence.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/advent/advent_qualitative.png&quot; alt=&quot;&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 3. Segmentation and detection qualitative results.&lt;/b&gt; Segmentation on Cityscapes validation set with ResNet-101 + DeepLab-V2; Detection on Cityscapes-foggy with VGG-16 as the backbone and SSD.&lt;/div&gt;

&lt;h3 id=&quot;uda-for-object-detection&quot;&gt;UDA for object detection&lt;/h3&gt;

&lt;p&gt;The proposed entropy-based approaches are not limited to semantic segmentation and can be applied to UDA for other recognition tasks, e.g. object detection. We conducted experiments in the UDA object detection set-up Cityscapes $\rightarrow$ Cityscapes-Foggy, similar to the one in &lt;a class=&quot;citation&quot; href=&quot;#chen2018domain&quot;&gt;(Chen et al., 2018)&lt;/a&gt;. We report quantitative results in &lt;em&gt;Table 2&lt;/em&gt; and qualitative ones in &lt;em&gt;Figure 3&lt;/em&gt;. In spite of the unfavorable factors, our improvement over the baseline ($+11.5\%$ mAP using AdvEnt) is larger than the one reported in &lt;a class=&quot;citation&quot; href=&quot;#chen2018domain&quot;&gt;(Chen et al., 2018)&lt;/a&gt; ($+8.8\%$). Additional experiments and implementation details can be found in the paper. These encouraging preliminary results suggest the feasibility of applying entropy-based approached on UDA for detection.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/advent/advent_table2.png&quot; alt=&quot;&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Table 2. Object detection performance on Cityscapes Foggy.&lt;/b&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this work, we propose two approaches for unsupervised domain adaptation reaching state-of-the-art performances on standard synthetic-2-real benchmarks. Interestingly the method can be easily extended to UDA for object detection with promising preliminary results.
Check out our &lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/html/Vu_ADVENT_Adversarial_Entropy_Minimization_for_Domain_Adaptation_in_Semantic_Segmentation_CVPR_2019_paper.html&quot;&gt;paper&lt;/a&gt; to find out more about intuitions, experiments and implementation details for AdvEnt and try out our &lt;a href=&quot;https://github.com/valeoai/ADVENT&quot;&gt;code&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;sun2017revisiting&quot;&gt;Sun, C., Shrivastava, A., Singh, S., &amp;amp; Gupta, A. (2017). Revisiting unreasonable effectiveness of data in deep learning era. &lt;i&gt;Proceedings of the IEEE International Conference on Computer Vision&lt;/i&gt;, 843–852.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;cordts2016cityscapes&quot;&gt;Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., &amp;amp; Schiele, B. (2016). The cityscapes dataset for semantic urban scene understanding. &lt;i&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 3213–3223.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;csurka2017domain&quot;&gt;Csurka, G. (2017). &lt;i&gt;Domain adaptation in computer vision applications&lt;/i&gt;. &lt;i&gt;8&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ganin2015unsupervised&quot;&gt;Ganin, Y., &amp;amp; Lempitsky, V. (2015). Unsupervised domain adaptation by backpropagation. &lt;i&gt;International Conference on Machine Learning&lt;/i&gt;, 1180–1189.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;tzeng2017adversarial&quot;&gt;Tzeng, E., Hoffman, J., Saenko, K., &amp;amp; Darrell, T. (2017). Adversarial discriminative domain adaptation. &lt;i&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 7167–7176.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;zou2018unsupervised&quot;&gt;Zou, Y., Yu, Z., Vijaya Kumar, B. V. K., &amp;amp; Wang, J. (2018). Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. &lt;i&gt;Proceedings of the European Conference on Computer Vision (ECCV)&lt;/i&gt;, 289–305.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hoffman2018cycada&quot;&gt;Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y., Isola, P., Saenko, K., Efros, A., &amp;amp; Darrell, T. (2018). Cycada: Cycle-consistent adversarial domain adaptation. &lt;i&gt;International Conference on Machine Learning&lt;/i&gt;, 1989–1998.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;wu2018dcan&quot;&gt;Wu, Z., Han, X., Lin, Y.-L., Gokhan Uzunbas, M., Goldstein, T., Nam Lim, S., &amp;amp; Davis, L. S. (2018). Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation. &lt;i&gt;Proceedings of the European Conference on Computer Vision (ECCV)&lt;/i&gt;, 518–534.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;grandvalet2005semi&quot;&gt;Grandvalet, Y., &amp;amp; Bengio, Y. (2005). Semi-supervised learning by entropy minimization. &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;, 529–536.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;jain2018learning&quot;&gt;Jain, H., Zepeda, J., Pérez, P., &amp;amp; Gribonval, R. (2018). Learning a complete image indexing pipeline. &lt;i&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 4933–4941.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;long2016unsupervised&quot;&gt;Long, M., Zhu, H., Wang, J., &amp;amp; Jordan, M. I. (2016). Unsupervised domain adaptation with residual transfer networks. &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;, 136–144.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;richter2016playing&quot;&gt;Richter, S. R., Vineet, V., Roth, S., &amp;amp; Koltun, V. (2016). Playing for data: Ground truth from computer games. &lt;i&gt;European Conference on Computer Vision&lt;/i&gt;, 102–118.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ros2016synthia&quot;&gt;Ros, G., Sellart, L., Materzynska, J., Vazquez, D., &amp;amp; Lopez, A. M. (2016). The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. &lt;i&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 3234–3243.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;tsai2018learning&quot;&gt;Tsai, Y.-H., Hung, W.-C., Schulter, S., Sohn, K., Yang, M.-H., &amp;amp; Chandraker, M. (2018). Learning to adapt structured output space for semantic segmentation. &lt;i&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 7472–7481.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;chen2018domain&quot;&gt;Chen, Y., Li, W., Sakaridis, C., Dai, D., &amp;amp; Van Gool, L. (2018). Domain adaptive faster r-cnn for object detection in the wild. &lt;i&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 3339–3348.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name></name></author><summary type="html">This post describes our recent work on unsupervised domain adaptation for semantic segmentation presented at CVPR 2019. ADVENT is a flexible technique for bridging the gap between two different domains through entropy minimization. Our work builds upon a simple observation: models trained only on source domain tend to produce over-confident, i.e., low-entropy, predictions on source-like images and under-confident, i.e., high-entropy, predictions on target-like ones. Consequently by minimizing the entropy on the target domain, we make the feature distributions from the two domains more similar. We show that our approach achieves competitive performances on standard semantic segmentation benchmarks and that it can be successfully extended to other tasks such as object detection.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io/blog/images/posts/advent/qualitative_results.png" /><media:content medium="image" url="https://valeoai.github.io/blog/images/posts/advent/qualitative_results.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>