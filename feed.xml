<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://valeoai.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://valeoai.github.io/blog/" rel="alternate" type="text/html" /><updated>2023-12-08T13:57:49-06:00</updated><id>https://valeoai.github.io/blog/feed.xml</id><title type="html">valeo.ai blog</title><subtitle>valeo.ai research blog</subtitle><entry><title type="html">valeo.ai at NeurIPS 2023</title><link href="https://valeoai.github.io/blog/2023/12/08/valeoai-at-neurips-2023.html" rel="alternate" type="text/html" title="valeo.ai at NeurIPS 2023" /><published>2023-12-08T00:00:00-06:00</published><updated>2023-12-08T00:00:00-06:00</updated><id>https://valeoai.github.io/blog/2023/12/08/valeoai-at-neurips-2023</id><content type="html" xml:base="https://valeoai.github.io/blog/2023/12/08/valeoai-at-neurips-2023.html"><![CDATA[<p>The <a href="https://neurips.cc/">Neural Information Porcessing Systems Conference (NeurIPS)</a> is a major inter-disciplinary event that brings together researchers and practicioners in machine learning, computer vision, natural language processing, optimization, statistics, but also neuroscience, natural sciences, social sciences, etc. This year, at the thirty-seventh edition of NeurIPS, the <a href="https://ptrckprz.github.io/valeoai/">valeo.ai</a> team will present 4 papers in the main conference. We will be happy to discuss more about these projects and ideas, and share our exciting ongoing research. Take a quick view of our papers in the conference and come meet us at the posters, at our booth or in the hallway. Take a quick view of our papers and come meet us at the posters or catch us for a coffer in the hallways.</p>

<h2 id="resilient-multiple-choice-learning-a-learned-scoring-scheme-with-application-to-audio-scene-analysis">Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis</h2>
<h4 id="authors-victor-letzelter-mathieu-fontaine-mickaël-chen-patrick-pérez-slim-essid-gaël-richard">Authors: Victor Letzelter, Mathieu Fontaine, Mickaël Chen, Patrick Pérez, Slim Essid, Gaël Richard</h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2311.01052">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/Victorletzelter/code-rMCL">Code</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/blog/publications/rmcl/">Project page</a>]</h4>

<p>In this work, we tackle ambiguous machine learning tasks, where single predictions don’t suffice due to the task’s nature or inherent uncertainties.</p>

<p>We introduce a robust multi-hypotheses framework that is capable of deterministically offering a range of plausible predictions at inference time. 
Our experiments on both synthetic data and real-world audio data affirm the potential and versatility of our method. Check out the paper and the code for more details.</p>

<p><img src="/blog/images/posts/2023_neurips/training_dynamics.gif" alt="rmcl_overview" height="100%" width="100%" /></p>

<p>This problem involves estimating a conditional distribution that is dependent on the input. The accompanying animation illustrates the early stages in the evolution of our model’s learning process, highlighting how it progressively refines its predictions (represented by shaded blue points) to the actual data distribution (indicated by green points), which varies with the input ‘t’.</p>

<hr />

<h2 id="pop-3d-open-vocabulary-3d-occupancy-prediction-from-images">POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images</h2>

<h4 id="authors-antonin-vobecky-oriane-siméoni-david-hurych-spyros-gidaris-andrei-bursuc-patrick-pérez-josef-sivic">Authors: Antonin Vobecky, Oriane Siméoni, David Hurych, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Josef Sivic</h4>

<h4 align="center"> [<a href="https://openreview.net/forum?id=eBXM62SqKY">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/vobecant/POP3D">Code</a>] &nbsp;&nbsp; [<a href="https://vobecant.github.io/POP3D">Project page</a>]</h4>

<p>POP-3D is an approach to predict open-vocabulary 3D semantic voxel occupancy map from input 2D images to enable 3D grounding, segmentation, and retrieval of free-form language queries.</p>

<p><img src="/blog/images/posts/2023_neurips/pop3d-overview.png" alt="pop3d_overview" height="100%" width="100%" /></p>
<div class="caption">Given surround-view images on the input, our POP-3D outputs voxel occupancy with 3D-language features, which one can query using text, e.g., to obtain zero-shot semantic segmentation.
</div>

<p>We design a new model architecture for open-vocabulary 3D semantic occupancy prediction. The architecture consists of a 2D-3D encoder together with occupancy prediction and 3D-language heads. The output is a dense voxel map of 3D grounded language embeddings enabling a range of open-vocabulary tasks. Next, we develop a tri-modal self-supervised learning algorithm that leverages three modalities: (i) images, (ii) language, and (iii) LiDAR point clouds and enables training the proposed architecture using a strong pre-trained vision-language model without the need for any 3D manual language annotations.</p>

<p><img src="/blog/images/posts/2023_neurips/pop3d-model.png" alt="pop3d_model" height="100%" width="100%" /></p>
<div class="caption">Overview of POP-3D architecture and training approach.</div>

<p>Finally, we demonstrate the strengths of the proposed model quantitatively on several open-vocabulary tasks: Zero-shot 3D semantic segmentation using existing datasets; 3D grounding, and retrieval of free-form language queries, using a small dataset that we propose as an extension of nuScenes.</p>

<p><img src="/blog/images/posts/2023_neurips/pop3d-qualitative.png" alt="pop3d_example" height="100%" width="100%" /></p>

<hr />

<h2 id="rewarded-soups-towards-pareto-optimal-alignment-by-interpolating-weights-fine-tuned-on-diverse-rewards">Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards</h2>
<h4 id="authors-alexandre-ramé-guillaume-couairon-mustafa-shukor-corentin-dancette-jean-baptiste-gaya-laure-soulier-matthieu-cord">Authors: Alexandre Ramé, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste Gaya, Laure Soulier, Matthieu Cord</h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2306.04488">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/alexrame/rewardedsoups">Code</a>] &nbsp;&nbsp; [<a href="https://huggingface.co/spaces/alexrame/rewardedsoups">Project page</a>]</h4>

<p>Foundation models are first pre-trained on vast unsupervised datasets and then fine-tuned on labeled data. Reinforcement learning, notably from human feedback (RLHF), can further align the network with the intended usage. Yet the imperfections in the proxy reward may hinder the training and lead to suboptimal results; the diversity of objectives in real-world tasks and human opinions exacerbate the issue. This paper proposes embracing the heterogeneity of diverse rewards by following a multi-policy strategy. Rather than focusing on a single a priori reward, we aim for Pareto-optimal generalization across the entire space of preferences. To this end, we propose rewarded soup, first specializing multiple networks independently (one for each proxy reward) and then interpolating their weights linearly. This succeeds empirically because we show that the weights remain linearly connected when fine-tuned on diverse rewards from a shared pre-trained initialization. We demonstrate the effectiveness of our approach for text-to-text (summarization, Q&amp;A, helpful assistant, review), text-image (image captioning, text-to-image generation, visual grounding, VQA), and control (locomotion) tasks. We hope to enhance the alignment of deep models, and how they interact with the world in all its diversity.</p>

<p><img src="/blog/images/posts/2023_neurips/rewarded-soups.png" alt="rs_overview" height="100%" width="100%" /></p>
<div class="caption"><b>Illustration of the different steps of our proposed rewarded soup (RS).</b>  After unsupervised pre-training and supervised fine-tuning, we launch $N$ independent RL fine-tunings on the proxy rewards $\{R_i\}^{N}_{i=1}$. Then we combine the trained networks by interpolation in the weight space. The final weights are adapted at test time by selecting the coefficient $\lambda$.</div>

<hr />

<h2 id="unifying-gans-and-score-based-diffusion-as-generative-particle-models">Unifying GANs and Score-Based Diffusion as Generative Particle Models</h2>
<h4 id="authors-jean-yves-franceschi-mike-gartrell-ludovic-dos-santos-thibaut-issenhuth-emmanuel-de-bézenac-mickaël-chen-alain-rakotomamonjy">Authors: Jean-Yves Franceschi, Mike Gartrell, Ludovic Dos Santos, Thibaut Issenhuth, Emmanuel de Bézenac, Mickaël Chen, Alain Rakotomamonjy</h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2305.16150">Paper</a>] &nbsp;&nbsp; [Code (soon)]</h4>

<p>By describing the trajectories of GAN outputs during training with particle evolution equations, we propose an unifying framework for GAN and Diffusion Models. We provide a new insights on the role of the generator network, and as proof of concept validating our theories, we propose methods to train a generator with score-based gradient instead of a discriminator, or to use a discriminator’s gradient flow to generate instead of training a generator.</p>

<p><img src="/blog/images/posts/2023_neurips/unify-gan.png" alt="unigan_overview" height="70%" width="70%" /></p>]]></content><author><name></name></author><category term="multi-sensor" /><category term="limited supervision" /><category term="reliability" /><category term="deep-learning" /><summary type="html"><![CDATA[Victor Letzelter, Antonin Vobecky, Mickael Chen, Matthieu Cord, Andrei Bursuc]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io/blog/images/posts/2023_neurips/logo_neurips.svg" /><media:content medium="image" url="https://valeoai.github.io/blog/images/posts/2023_neurips/logo_neurips.svg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">valeo.ai at ICCV 2023</title><link href="https://valeoai.github.io/blog/2023/09/26/valeoai-at-iccv-2023.html" rel="alternate" type="text/html" title="valeo.ai at ICCV 2023" /><published>2023-09-26T00:00:00-05:00</published><updated>2023-09-26T00:00:00-05:00</updated><id>https://valeoai.github.io/blog/2023/09/26/valeoai-at-iccv-2023</id><content type="html" xml:base="https://valeoai.github.io/blog/2023/09/26/valeoai-at-iccv-2023.html"><![CDATA[<p>The <a href="https://iccv2023.thecvf.com/">IEEE / CVF International Conference on Computer Vision (ICCV)</a> is a landmark event for the increasingly large and diverse community of researchers in computer vision and machine learning. This year, ICCV takes place in Paris, home of the <a href="https://ptrckprz.github.io/valeoai/">valeo.ai</a> team. From interns to senior researchers, the valeo.ai team will participate in mass at ICCV and will be looking forward to welcoming you and talking about the exciting progress and ideas in the field.</p>

<p>At ICCV 2023 we will present 5 papers in the main conference and 3 in the workshops. We are also organizing 2 tutorials with 2 challenges (<a href="https://valeoai.github.io/bravo/">BRAVO</a> and <a href="https://uncv2023.github.io/">UNCV</a>) and a tutorial (<a href="https://abursuc.github.io/many-faces-reliability/">Many Faces of Reliability</a>).
Take a quick view of our papers in the conference and come meet us at the posters, at our booth or in the hallway.</p>

<h2 id="using-a-waffle-iron-for-automotive-point-cloud-semantic-segmentation">Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation</h2>
<h4 id="authors-gilles-puy-alexandre-boulch-renaud-marlet">Authors: Gilles Puy, Alexandre Boulch, Renaud Marlet</h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2301.10100">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/WaffleIron">Code</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/blog/publications/waffleiron/">Project page</a>]</h4>

<p>Semantic segmentation of point clouds delivered by lidars permits autonomous vehicles to make sense of their 3D surrounding environment. Sparse convolutions have become a de-facto tool to process these large outdoor point clouds. The top performing methods on public benchmarks, such SemanticKITTI or nuScenes, all leverage sparse convolutions. Nevertheless, despite their undeniable success and efficiency, these convolutions remain available in a limited number of deep learning frameworks and hardware platforms. In this work, we propose an alternative backbone built with tools broadly available (such as 2D and 1D convolutions) but that still reaches the level of performance of the top methods on automotive datasets.</p>

<p>We propose a point-based backbone, called WaffleIron, which is essentially built using standard MLPs and dense 2D convolutions, both readily available in all deep learning frameworks thanks to their wide use in the field of computer vision. The architecture of this backbone is illustrated in the figure below. It is inspired by the recent MLP-Mixer. It takes as input a point cloud with a token associated to each point. All these point tokens are then updated by a sequence of layers, each containing a token-mixing step (made of dense 2D convolutions) and a channel-mixing step (made of a MLP shared across points).</p>

<p><img src="/blog/images/posts/2023_iccv/waffleiron.png" alt="waffle_overview" height="70%" width="70%" /></p>
<div class="caption">The WaffleIron backbone takes as input point tokens, provided by an embedding layer (not represented), and updates these point representations L times via a point token-mixing layer (containing the WI block) followed by a channel-mixing layer. The WI block consists of a 2D projection along one of the main axes, a feed-forward network (FFN) with two dense channel-wise 2D convolutions with a ReLU activation in the hidden layer, and a simple copy of the 2D features to the 3D points. The channel-mixing layer contains a batch-norm, a MLP shared across each point, and a residual connection. The WaffleIron backbone is free of any point downsampling or upsampling layer, farthest point sampling, nearest neighbor search, or sparse convolution.
</div>

<p>WaffleIron has three main hyperparameters to tune: the depth L, the width F and the resolution of the 2D grid. We show that these parameters are easy to tune: the performance increases with the network width F and depth L, until an eventual saturation; we observe stable results over a wide range of values for the resolution of the 2D grid.</p>

<p>In our paper, we also provide many details on how to train WaffleIron to reach the performance of top-entries on two autonomous driving benchmarks: SemanticKITTI and nuScenes.</p>

<hr />

<h2 id="pøda-prompt-driven-zero-shot-domain-adaptation">PØDA: Prompt-driven Zero-shot Domain Adaptation</h2>
<h4 id="authors-mohammad-fahes-tuan-hung-vu-andrei-bursuc-patrick-pérez-raoul-de-charette">Authors: Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette</h4>

<h4 align="center"> [<a href=" https://arxiv.org/abs/2212.03241">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/astra-vision/PODA ">Code</a>] &nbsp;&nbsp; [<a href="https://www.youtube.com/watch?v=kataxQoPuSE">Video</a>]  &nbsp;&nbsp; [<a href="https://astra-vision.github.io/PODA/ ">Project page</a>]</h4>

<p>Domain adaptation has been vastly investigated in computer vision but still requires access to target images at train time, which might be intractable in some uncommon conditions. In this paper, we propose the task of ‘Prompt-driven Zero-shot Domain Adaptation’, where we adapt a model trained on a source domain using only a general description in natural language of the target domain, i.e., a prompt. First, we leverage a pre-trained contrastive vision-language model (CLIP) to optimize affine transformations of source features, steering them towards the target text embedding while preserving their content and semantics. To achieve this, we propose Prompt-driven Instance Normalization (PIN). Second, we show that these prompt-driven augmentations can be used to perform zero-shot domain adaptation for semantic segmentation. Experiments demonstrate that our method significantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand, even surpassing one-shot unsupervised domain adaptation. A similar boost is observed on object detection and image classification</p>

<p><img src="/blog/images/posts/2023_iccv/poda.png" alt="poda_overview" height="80%" width="80%" /></p>
<div class="caption">We perform zero-shot adaptation with natural language prompts. PØDA enables the adaptation of a segmenter model (here, DeepLabv3+ trained on the source dataset Cityscapes) to unseen conditions with only a prompt. Source-only predictions are shown as smaller segmentation masks to the left or right of the test images.
</div>

<hr />

<h2 id="you-never-get-a-second-chance-to-make-a-good-first-impression-seeding-active-learning-for-3d-semantic-segmentation">You Never Get a Second Chance To Make a Good First Impression: Seeding Active Learning for 3D Semantic Segmentation</h2>
<h4 id="authors-nermin-samet-oriane-siméoni-gilles-puy-georgy-ponimatkin-renaud-marlet-vincent-lepetit">Authors: Nermin Samet, Oriane Siméoni, Gilles Puy, Georgy Ponimatkin, Renaud Marlet, Vincent Lepetit</h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2304.11762">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/nerminsamet/seedal">Code</a>]</h4>

<p>We are interested in the efficient annotation of sparse 3D point clouds (as captured indoors by depth cameras or outdoors by automotive lidars) for semantic segmentation. Active Learning (AL) iteratively selects relevant data fractions to annotate within a given budget, but requires a first fraction of the dataset (a ’seed’) to be already annotated to estimate the benefit of annotating other data fractions. We show that the choice of the seed can significantly affect the performance of many AL methods and propose a method, named SeedAL, for automatically constructing a seed that will ensure good performance for AL. Assuming that images of the point clouds are available, which is common, our method relies on powerful unsupervised image features to measure the diversity of the point clouds. It selects the point clouds for the seed by optimizing the diversity under an annotation budget, which can be done by solving a linear optimization problem. Our experiments demonstrate the effectiveness of our approach compared to random seeding and existing methods on both the S3DIS and SemanticKitti datasets.</p>

<p><img src="/blog/images/posts/2023_iccv/seedal.png" alt="seedal_overview" height="70%" width="70%" /></p>
<div class="caption"><b>Impact of active learning seed on performance. </b>We show the variability of results obtained with 20 different random seeds (blue dashed lines), within an initial annotation budget of 3% of the dataset, when using various active learning methods for 3D semantic segmentation of S3DIS. We compare it to the result obtained with our seed selection strategy (solid red line), named SeedAL, which performs better or on par with the best (lucky) random seeds among 20, and “protects” from very bad (unlucky) random seeds.</div>

<hr />

<h2 id="ep-alm-efficient-perceptual-augmentation-of-language-models">eP-ALM: Efficient Perceptual Augmentation of Language Models</h2>
<h4 id="authors-mustafa-shukor-corentin-dancette-matthieu-cord">Authors: Mustafa Shukor, Corentin Dancette, Matthieu Cord</h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2303.11403">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/mshukor/eP-ALM">Code</a>]  &nbsp;&nbsp; [<a href="https://mshukor.github.io/eP-ALM.github.io/">Project page</a>]</h4>

<p>eP-ALM aims to augment large language models (LLMs) with perception. While most existing approaches train a large number of parameters and rely on extensive multimodal pre-training, we investigate the minimal computational effort required to adapt unimodal models to multimodal tasks. We show that by freezing more than 99% of total parameters, training only one linear projection layer and prepending only one trainable token, our approach (dubbed eP-ALM) significantly outperforms other baselines on VQA and captioning for image, video and audio modalities.</p>

<p><img src="/blog/images/posts/2023_iccv/ep-alm.png" alt="epalm_overview" height="70%" width="70%" /></p>
<div class="caption"><b>Illustration of the adaptation mechanism in eP-ALM.</b> The perceptual input (image/video/audio) is fed to the perceptual encoder E (e.g., ViT) and the corresponding text to the LM (e.g., OPT), which then generates a text conditioned on the perceptual input. The multimodal interaction is done via the [CLS] tokens acting as Perceptual Prompt, and are extracted from the last layers of the encoder, then injected in the last layers of LM, after passing by the Linear Connection C. The previous [CLS] token is replaced by the new one coming from a deeper layer, keeping the number of tokens fixed. The first layers (grayed) of each model are kept intact without any modality interaction. We ease the adaptation with a Soft Prompt that is prepended to the input of LM.
</div>

<hr />

<h2 id="zero-shot-spatial-layout-conditioning-for-text-to-image-diffusion-models">Zero-shot spatial layout conditioning for text-to-image diffusion models</h2>
<h4 id="authors-guillaume-couairon-marlène-careil-matthieu-cord-stéphane-lathuilière-jakob-verbeek">Authors: Guillaume Couairon, Marlène Careil, Matthieu Cord, Stéphane Lathuilière, Jakob Verbeek</h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2306.13754">Paper</a>]</h4>

<p>Large-scale text-to-image diffusion models have considerably improved the state of the art in generative image modeling, and provide an intuitive and powerful user interface to drive the image generation process. In this paper, we propose ZestGuide, a “zero-shot” segmentation guidance approach that can be integrated into pre-trained text-image diffusion models, and requires no additional training. It exploits the implicit segmentation maps that can be extracted from cross-attention layers, and uses them to align generation with input masks.</p>

<p><img src="/blog/images/posts/2023_iccv/zest-guide.png" alt="zest_overview" height="70%" width="70%" /></p>
<div class="caption">ZestGuide generates images conditioned on segmentation maps with corresponding free-form textual descriptions.
</div>

<hr />

<h2 id="diffhpe-robust-coherent-3d-human-pose-lifting-with-diffusion">DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion</h2>
<p class="page-description"><a href="https://web.northeastern.edu/smilelab/amfg2023/">ICCV Workshop on Analysis and Modeling of Faces and Gestures</a></p>

<h4 id="authors-cédric-rommel-eduardo-valle-mickaël-chen-souhaiel-khalfaoui-renaud-marlet-matthieu-cord-patrick-pérez">Authors: Cédric Rommel, Eduardo Valle, Mickaël Chen, Souhaiel Khalfaoui, Renaud Marlet, Matthieu Cord, Patrick Pérez</h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2309.01575">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/diffhpe">Code</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/blog/publications/diffhpe">Project page</a>]</h4>

<p>Diffusion models are making waves across various domains, including computer vision, natural language processing and time-series analysis. However, its application to purely predictive tasks, such as 3D human pose estimation (3D-HPE), remains largely unexplored. While a few pioneering works have shown promising performance metrics in 3D-HPE, the understanding of the benefits of diffusion models over classical supervision — as well as key design choices — is still in its infancy. In this work, we address those concerns, providing an in-depth analysis of the effects of diffusion models on 3D-HPE.</p>

<p><img src="/blog/images/posts/2023_iccv/diffhpe.gif" alt="diffhpe_overview" height="100%" width="100%" /></p>
<div class="caption">Poses across the learned reverse diffusion process converge to an accurate 3D reconstruction of the corresponding 2D pose in pixel space.</div>

<p>More precisely, we propose DiffHPE, a novel strategy to use diffusion models in 3D-HPE, and show that combining diffusion with pre-trained supervised models allows to outperform both pure diffusion and pure supervised models trained separately. Our analysis demonstrates not only that the diffusion framework can be used to enhance accuracy, as previously understood, but also that it can improve robustness and coherence. Namely, our experiments showcase how poses estimated with diffusion models’ display better bilateral and temporal coherence, and are more robust to occlusions, even when not perfectly trained for the latter.</p>

<hr />

<h2 id="challenges-of-using-real-world-sensory-inputs-for-motion-forecasting-in-autonomous-driving">Challenges of Using Real-World Sensory Inputs for Motion Forecasting in Autonomous Driving</h2>
<p class="page-description"><a href="https://sites.google.com/view/road-plus-plus">ROAD++: The Second Workshop and Challenge on Event Detection for Situation Awareness in Autonomous Driving</a></p>

<h4 id="authors-yihong-xu-loïck-chambon-éloi-zablocki-mickaël-chen-matthieu-cord-patrick-pérez">Authors: Yihong Xu, Loïck Chambon, Éloi Zablocki, Mickaël Chen, Matthieu Cord, Patrick Pérez</h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2306.09281">Paper</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/blog/publications/real-world-forecasting/">Project page</a>]</h4>

<p>Motion forecasting is crucial in enabling autonomous vehicles to anticipate the future trajectories of surrounding agents. To do so, it requires solving mapping, detection, tracking, and then forecasting problems, in a multi-step pipeline. In this complex system, advances in conventional forecasting methods have been made using curated data, i.e., with the assumption of perfect maps, detection, and tracking. This paradigm, however, ignores any errors from upstream modules. Meanwhile, an emerging end-to-end paradigm, that tightly integrates the perception and forecasting architectures into joint training, promises to solve this issue. So far, however, the evaluation protocols between the two methods were incompatible and their comparison was not possible. In fact, and perhaps surprisingly, conventional forecasting methods are usually not trained nor tested in real-world pipelines (e.g., with upstream detection, tracking, and mapping modules). In this work, we aim to bring forecasting models closer to real-world deployment. First, we propose a unified evaluation pipeline for forecasting methods with real-world perception inputs, allowing us to compare the performance of conventional and end-to-end methods for the first time. Second, our in-depth study uncovers a substantial performance gap when transitioning from curated to perception-based data. In particular, we show that this gap (1) stems not only from differences in precision but also from the nature of imperfect inputs provided by perception modules, and that (2) is not trivially reduced by simply finetuning on perception outputs. Based on extensive experiments, we provide recommendations for critical areas that require improvement and guidance towards more robust motion forecasting in the real world. We will release an evaluation library to benchmark models under standardized and practical conditions.</p>

<p><img src="/blog/images/posts/2023_iccv/e2e_forecasting.png" alt="forecast_overview" height="90%" width="90%" /></p>
<div class="caption"><b>Study overview.</b> We study the challenges of deploying motion forecasting models into the real world when only predicted perception inputs are available. We compare: (1) (top) "conventional methods" (i.e., methods trained on curated input data) where (middle) we directly replace the curated inputs with real-world data, and (2) (bottom) "end-to-end methods" that are trained and used with perception modules. In the real-world setting, evaluation is challenging as the past tracks are estimated with arbitrary identities, making it difficult to establish a direct correspondence to GT identities. Therefore, we propose a matching process (purple) to assign predictions to GT and thus evaluate forecasting performances. Moreover, we study in depth the impact changing from curated data (green) to real-world (orange) mapping, or detection and tracking errors to motion forecasting.
</div>

<hr />

<h2 id="pop-3d-open-vocabulary-3d-occupancy-prediction-from-images">POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images</h2>
<p class="page-description"><a href="https://opensun3d.github.io/">ICCV 2023 Workshop on Open-Vocabulary 3D Scene Understanding (OpenSUN 3D)</a></p>

<h4 id="authors-antonin-vobecky-oriane-siméoni-david-hurych-spyros-gidaris-andrei-bursuc-patrick-pérez-josef-sivic">Authors: Antonin Vobecky, Oriane Siméoni, David Hurych, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Josef Sivic</h4>

<h4 align="center"> [<a href="https://data.ciirc.cvut.cz/public/projects/2023POP3D/resources/pop3d_paper.pdf ">Paper</a>]</h4>

<p>We propose an approach to predict a 3D semantic voxel occupancy map from input 2D images with features allowing 3D grounding, segmentation and retrieval of free-form language queries. To this end: We design a new architecture that consists of a 2D-3D encoder together with occupancy prediction and 3D-language heads; We develop a tri-modal self-supervised training that leverages three modalities – images, language and LiDAR point clouds– and enables learning the proposed architecture using a strong pre-trained vision-language model without the need for any 3D manual annotations. We quantitatively evaluate the proposed model on the task of zero-shot 3D semantic segmentation using existing datasets and show results on the tasks of 3D grounding and retrieval of free-form language queries.</p>

<p><img src="/blog/images/posts/2023_iccv/pop3d.png" alt="forecast_overview" height="100%" width="100%" /></p>
<div class="caption"><b>Method overview.</b>Given surround-view images, POP-3D produces a voxel grid of text-aligned features that support open-vocabulary downstream tasks such as zero-shot occupancy segmentation or text-based grounding and retrieval.


</div>]]></content><author><name></name></author><category term="3d perception" /><category term="multi-sensor" /><category term="limited supervision" /><category term="reliability" /><category term="domain-adaptation" /><summary type="html"><![CDATA[Gilles Puy, Tuan-Hung Vu, Oriane Siméoni, Matthieu Cord, Cédric Rommel, Andrei Bursuc]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io/blog/images/posts/2023_iccv/iccv_logo.svg" /><media:content medium="image" url="https://valeoai.github.io/blog/images/posts/2023_iccv/iccv_logo.svg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">valeo.ai at CVPR 2023</title><link href="https://valeoai.github.io/blog/2023/06/14/valeoai-at-cvpr-2023.html" rel="alternate" type="text/html" title="valeo.ai at CVPR 2023" /><published>2023-06-14T00:00:00-05:00</published><updated>2023-06-14T00:00:00-05:00</updated><id>https://valeoai.github.io/blog/2023/06/14/valeoai-at-cvpr-2023</id><content type="html" xml:base="https://valeoai.github.io/blog/2023/06/14/valeoai-at-cvpr-2023.html"><![CDATA[<p>The <a href="https://cvpr2023.thecvf.com/">IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)</a> is a key event for researchers and engineers working on computer vision and machine learning. At the 2023 edition the <a href="https://ptrckprz.github.io/valeoai/">valeo.ai</a> team will present six <a href="https://ptrckprz.github.io/vaipub/">papers</a> in the main conference, one workshop <a href="https://vision4allseason.net/">keynote</a> and organize a <a href="https://osimeoni.github.io/object-localization-for-free/">tutorial</a>. The team will be at CVPR to present these works and will be happy to discuss more about these projects and ideas, and share our exciting ongoing research.
We outline four of our team papers below.</p>

<h2 id="octet-object-aware-counterfactual-explanations">OCTET: Object-aware Counterfactual Explanations</h2>
<h4 id="authors-mehdi-zemni-mickaël-chen-éloi-zablocki-hédi-ben-younes-patrick-pérez-matthieu-cord">Authors: Mehdi Zemni, <a href="https://scholar.google.com/citations?user=QnRpMJAAAAAJ&amp;hl=fr&amp;oi=sra">Mickaël Chen</a>, <a href="https://scholar.google.com/citations?user=dOkbUmEAAAAJ&amp;hl=fr">Éloi Zablocki</a>, <a href="https://scholar.google.com/citations?hl=fr&amp;user=IFLcfvUAAAAJ">Hédi Ben-Younes</a>, <a href="https://ptrckprz.github.io/">Patrick Pérez</a>, <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a></h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2211.12380 ">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/octet">Code</a>] &nbsp;&nbsp; [<a href="https://www.youtube.com/watch?v=Xfq0uRcw9jQ">Video</a>]  &nbsp;&nbsp; [<a href="https://valeoai.github.io/blog/publications/octet/">Project page</a>]</h4>

<p>Nowadays, deep vision models are being widely deployed in safety-critical applications, e.g., autonomous driving, and explainability of such models is becoming a pressing concern. Among explanation methods, counterfactual explanations aim to find minimal and interpretable changes to the input image that would also change the output of the model to be explained. Such explanations point end-users at the main factors that impact the decision of the model. However, previous methods struggle to explain decision models trained on images with many objects, e.g., urban scenes, which are more difficult to work with but also arguably more critical to explain. In this work, we propose to tackle this issue with an object-centric framework for counterfactual explanation generation. Our method, inspired by recent generative modeling works, encodes the query image into a latent space that is structured in a way to ease object-level manipulations. Doing so, it provides the end-user with control over which search directions (e.g., spatial displacement of objects, style modification, etc.) are to be explored during the counterfactual generation. We conduct a set of experiments on counterfactual explanation benchmarks for driving scenes, and we show that our method can be adapted beyond classification, e.g., to explain semantic segmentation models. To complete our analysis, we design and run a user study that measures the usefulness of counterfactual explanations in understanding a decision model.</p>

<p><img src="/blog/images/posts/2023_cvpr/octet.png" alt="octet_overview" height="80%" width="80%" /></p>
<div class="caption"><b>Counterfactual explanations generated by OCTET.</b>  Given a classifier that predicts whether or not it is possible to go left, and a query image (top left), OCTET produces a counterfactual explanation where the most influential features that led to the decision are changed (top right). On the bottom row, we show that OCTET can also operate under different settings that result in different focused explanations. We report the prediction made by the decision model at the top left of each image. 
</div>

<hr />

<h2 id="also-automotive-lidar-self-supervision-by-occupancy-estimation">ALSO: Automotive Lidar Self-supervision by Occupancy estimation</h2>
<h4 id="authors-alexandre-boulch-corentin-sautier-björn-michele-gilles-puy-renaud-marlet">Authors: <a href="https://www.boulch.eu/">Alexandre Boulch</a>, <a href="https://scholar.google.com/citations?user=xYDkHEsAAAAJ">Corentin Sautier</a>, <a href="https://scholar.google.com/citations?user=xQcKnXkAAAAJ&amp;hl=en">Björn Michele</a>, <a href="https://sites.google.com/site/puygilles/home">Gilles Puy</a>, <a href="http://imagine.enpc.fr/~marletr/">Renaud Marlet</a></h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2212.05867">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/ALSO">Code</a>] &nbsp;&nbsp; [<a href="https://www.youtube.com/watch?v=GGIBKlMvphw">Video</a>]  &nbsp;&nbsp; [<a href="https://valeoai.github.io/blog/publications/also/">Project page</a>]</h4>

<p>We propose a new self-supervised method for pre-training the backbone of deep perception models operating on point clouds. The core idea is to train the model on a pretext task which is the reconstruction of the surface on which the 3D points are sampled, and to use the underlying latent vectors as input to the perception head. The intuition is that if the network is able to reconstruct the scene surface, given only sparse input points, then it probably also captures some fragments of semantic information that can be used to boost an actual perception task. This principle has a very simple formulation, which makes it both easy to implement and widely applicable to a large range of 3D sensors and deep networks performing semantic segmentation or object detection. In fact, it supports a single-stream pipeline, as opposed to most contrastive learning approaches, allowing training on limited resources. We conducted extensive experiments on various autonomous driving datasets, involving very different kinds of lidars, for both semantic segmentation and object detection. The results show the effectiveness of our method to learn useful representations without any annotation, compared to existing approaches.</p>

<p><img src="/blog/images/posts/2023_cvpr/also.png" alt="also_overview" height="100%" width="100%" /></p>
<div class="caption"><b>ALSO overview.</b> The backbone to pre-train produces latent vectors for each input point. At pre-training time, the latent vector are fed into an volumetric occupancy head that classifies query points as full or empty. At semantic training or test time, the same latent vectors are fed into a semantic head, e.g., for semantic segmentation or object detection. 
</div>

<hr />

<h2 id="unsupervised-object-localization-observing-the-background-to-discover-objects">Unsupervised Object Localization: Observing the Background to Discover Objects</h2>
<h4 id="authors-oriane-siméoni-chloé-sekkat-gilles-puy-antonin-vobecky-éloi-zablocki-patrick-pérez">Authors: <a href="https://osimeoni.github.io/">Oriane Siméoni</a>, <a href="https://github.com/chloeskt">Chloé Sekkat</a>, <a href="https://sites.google.com/site/puygilles/home">Gilles Puy</a>, <a href="https://vobecant.github.io/">Antonin Vobecky</a>, <a href="https://scholar.google.com/citations?user=dOkbUmEAAAAJ&amp;hl=fr">Éloi Zablocki</a>, <a href="https://ptrckprz.github.io/">Patrick Pérez</a></h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2212.07834">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/FOUND">Code</a>] &nbsp;&nbsp; [<a href="https://youtu.be/jfYQfFcrJBE">Video</a>]  &nbsp;&nbsp; [<a href="https://valeoai.github.io/blog/publications/found">Project page</a>]</h4>

<p>Recent advances in self-supervised visual representation learning have paved the way for unsupervised methods tackling tasks such as object discovery and instance segmentation. However, discovering objects in an image with no supervision is a very hard task; what are the desired objects, when to separate them into parts, how many are there, and of what classes? The answers to these questions depend on the tasks and datasets of evaluation. In this work, we take a different approach and propose to look for the background instead. This way, the salient objects emerge as a by-product without any strong assumption on what an object should be. We propose FOUND, a simple model made of a single conv1 × 1 initialized with coarse background masks extracted from self-supervised patch-based representations. After fast training and refining these seed masks, the model reaches state-of-the-art results on unsupervised saliency detection and object discovery benchmarks. Moreover, we show that our approach yields good results in the unsupervised semantic segmentation retrieval task.</p>

<p><img src="/blog/images/posts/2023_cvpr/found.png" alt="found_overview" height="65%" width="65%" /></p>
<div class="caption"><b>Overview of FOUND. </b>In the first stage (green upperpart), a background mask is discovered by mining a seed patch through a reweighting of the self-attention maps of a frozen DINO self-supervised features. This seed is then used to find similar patches likely belonging to the background. In the second stage (blue lower part), we train a lightweight 1 × 1 convolutional layer that produces refined masks from the self-supervised features. It is trained in a self-supervised fashion to predict both smoothed inverse coarse masks of the first step, and smoothed binarized version of its own output. Blue arrows denote where the gradients flow (in the reverse direction).</div>

<hr />

<h2 id="rangevit-towards-vision-transformers-for-3d-semantic-segmentation-in-autonomous-driving">RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving</h2>
<h4 id="authors-angelika-ando-spyros-gidaris-andrei-bursuc-gilles-puy-alexandre-boulch-renaud-marlet">Authors: Angelika Ando, <a href="https://scholar.google.fr/citations?user=7atfg7EAAAAJ&amp;hl=en">Spyros Gidaris</a>, <a href="https://abursuc.github.io/">Andrei Bursuc</a>, <a href="https://sites.google.com/site/puygilles/home">Gilles Puy</a>, <a href="https://www.boulch.eu/">Alexandre Boulch</a>, <a href="http://imagine.enpc.fr/~marletr/">Renaud Marlet</a></h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2301.10222">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/rangevit ">Code</a>] &nbsp;&nbsp; [<a href="https://www.youtube.com/watch?v=urd2ZIJ70WY">Video</a>]  &nbsp;&nbsp; [<a href="https://valeoai.github.io/blog/publications/rangevit/">Project page</a>]</h4>

<p>Semantic segmentation of LiDAR point clouds permits vehicles to perceive their surrounding 3D environment independently of the lighting condition, providing useful information to build safe and reliable vehicles. A common approach to segment large scale LiDAR point clouds is to project the points on a 2D surface and then to use regular CNNs, originally designed for images, to process the projected point clouds. These projection-based methods usually benefit from fast computations and, when combined with techniques which use other point cloud representations, achieve state-of-the-art results.</p>

<p>Today, projection-based methods leverage 2D CNNs but recent advances in computer vision show that vision transformers (ViTs) have achieved state-of-the-art results in many image-based benchmarks. Despite the absence of almost any domain-specific inductive bias apart from the image tokenization process, ViTs have a strong representation learning capacity and achieve excellent results on various image perception tasks, such as image classification, object detection or semantic segmentation. Inspired by this success of ViTs for image understanding, in this work, we show that projection-based methods for 3D semantic segmentation can benefit from these latest improvements on ViTs when combined with three key ingredients, all described in our <a href="https://arxiv.org/abs/2301.10222">paper</a>.</p>

<p><img src="/blog/images/posts/2023_cvpr/rangevit-teaser.png" alt="rangevit_teaser" height="75%" width="75%" /></p>
<div class="caption"><b>Exploiting vision transformer (ViT) architectures and weights for LiDAR point cloud semantic segmentation.</b> We leverage the flexibility of transformer-based architectures to re-purpose them with minimal changes for processing sparse point clouds in autonomous driving tasks. The common ViT backbone across modalities allows to effectively transfer weights pre-trained on large image repositories towards improving point cloud segmentation performance with fine-tuning.
</div>

<p>We answer positively but only after combining them with three key ingredients: (a) ViTs are notoriously hard to train and require a lot of training data to learn powerful representations. By preserving the same backbone architecture as for RGB images, we can exploit the knowledge from long training on large image collections that are much cheaper to acquire and annotate than point clouds. We reach our best results with pre-trained ViTs on large image datasets. (b) We compensate for ViTs’ lack of inductive bias by substituting a tailored non-linear convolutional stem for the classical linear embedding layer. (c) We refine pixel-wise predictions with a convolutional decoder and a skip connection from the convolutional stem to combine low-level but fine-grained features of the convolutional stem with the high-level but coarse predictions of the ViT encoder. With these ingredients, we show that our method, called RangeViT, outperforms prior projection-based methods on nuScenes and SemanticKITTI.</p>

<p><img src="/blog/images/posts/2023_cvpr/rangevit-overview.png" alt="rangevit_overview" height="100%" width="100%" /></p>
<div class="caption"><b> Overview of RangeViT architecture.</b> First, the point cloud is projected in a 2D space with range projection. Then, the produced range image is processed by the convolutional stem, the ViT encoder and the decoder to obtain a 2D feature map. It is then processed by a 3D refiner layer for 3D point-wise predictions. Note that there is a single skip connection between the convolutional stem and the decoder.
</div>

<p>In summary, our work offers the following contributions:</p>
<ul>
  <li>Exploiting the powerful representation learning capacity of vision transformers for LiDAR semantic segmentation.</li>
  <li>Unifying the network architectures for processing LiDAR point clouds and images, enabling advancements in one domain to benefit both.</li>
  <li>Demonstrating the utilization of pre-trained ViTs on large-scale natural image datasets for LiDAR point cloud segmentation.</li>
</ul>

<p>We believe that this finding is highly intriguing. The RangeViT approach can leverage off-the-shelf pre-trained ViT models, enabling direct benefits from ongoing and future advances in training ViT models with natural RGB images - a rapidly growing research field.</p>]]></content><author><name></name></author><category term="3d perception" /><category term="multi-sensor" /><category term="limited supervision" /><category term="reliability" /><summary type="html"><![CDATA[Alexandre Boulch, Oriane Siméoni, Gilles Puy, Éloi Zablocki, Spyros Gidaris, Andrei Bursuc]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io/blog/images/posts/2023_cvpr/cvpr_banner.svg" /><media:content medium="image" url="https://valeoai.github.io/blog/images/posts/2023_cvpr/cvpr_banner.svg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">valeo.ai at CVPR 2022</title><link href="https://valeoai.github.io/blog/2022/06/14/valeoai-at-cvpr-2022.html" rel="alternate" type="text/html" title="valeo.ai at CVPR 2022" /><published>2022-06-14T00:00:00-05:00</published><updated>2022-06-14T00:00:00-05:00</updated><id>https://valeoai.github.io/blog/2022/06/14/valeoai-at-cvpr-2022</id><content type="html" xml:base="https://valeoai.github.io/blog/2022/06/14/valeoai-at-cvpr-2022.html"><![CDATA[<p>The <a href="https://cvpr2022.thecvf.com/">IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)</a> is a major event for researchers and engineers working on computer vision and machine learning. At the 2022 edition the <a href="https://ptrckprz.github.io/valeoai/">valeo.ai</a> team will present four <a href="https://ptrckprz.github.io/vaipub/">papers</a> in the main conference, three <a href="https://ptrckprz.github.io/vaipub/">papers</a> in workshops and one workshop <a href="https://vision4allseason.net/">keynote</a>. The team will be at CVPR to present these works and will be happy to discuss more about these projects and ideas, and share our exciting ongoing research.</p>

<h2 id="image-to-lidar-self-supervised-distillation-for-autonomous-driving-data">Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data</h2>
<h4 id="authors-corentin-sautier-gilles-puy--spyros-gidaris--alexandre-boulch-andrei-bursuc-renaud-marlet">Authors: Corentin Sautier, <a href="https://sites.google.com/site/puygilles/home">Gilles Puy</a>,  <a href="https://scholar.google.fr/citations?user=7atfg7EAAAAJ&amp;hl=en">Spyros Gidaris</a>,  <a href="https://www.boulch.eu/">Alexandre Boulch</a>, <a href="https://abursuc.github.io/">Andrei Bursuc</a>, <a href="http://imagine.enpc.fr/~marletr/">Renaud Marlet</a></h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2203.16258">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/SLidR">Code</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/blog/publications/slidr/">Project page</a>]</h4>

<p>Self-driving vehicles require object detection or segmentation to safely maneuver in their environment. Such safety-critical tasks are usually performed by neural networks demanding huge Lidar datasets with high quality annotations, and no domain shift between training and testing conditions. However, annotating 3D Lidar data for these tasks is tedious and costly. In <a href="https://arxiv.org/abs/2203.16258">Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data</a>, we propose a self-supervised pre-training method for 3D perception models that is tailored to autonomous driving data and that does not require any annotation. Specifically, we leverage the availability of synchronized and calibrated image and Lidar data in autonomous driving setups for distilling self-supervised pre-trained image representations into 3D models, using neither point cloud nor image annotations.</p>

<p><img src="/blog/images/posts/2022_cvpr/SLidR_overview_2.png" alt="slidr_overview" height="100%" width="100%" /></p>
<div class="caption"><b>Synchronized Lidar and camera frames are encoded through two modality-specific features extractors.</b> The camera backbone has pre-trained weights obtained with no annotations (e.g., with MoCo v2 <a class="citation" href="#chen2020improved">(Chen et al., 2020)</a>). Features are pooled at a pseudo-object level using image superpixels, and contrasted between both modalities</div>

<p>A key ingredient of our method is the use of superpixels which are used to pool 3D point features and 2D pixel features in visually similar regions. We then train a 3D network on the self-supervised task of matching these pooled 3D-point features with the corresponding pooled image pixel features. Extensive experiments on autonomous driving datasets demonstrate the ability of our image-to-Lidar distillation strategy to produce 3D representations that transfer well to semantic segmentation and object detection tasks.</p>

<p><img src="/blog/images/posts/2022_cvpr/SLidR_results.jpg" alt="slidr_results" height="100%" width="100%" /></p>
<div class="caption">The similarity between a query point's features (in red) and all other Lidar points is shown, to assert the quality of the learned representation. Color scale goes from purple (low similarity) to yellow (high similarity). </div>

<p>With our pre-training, a Lidar network can learn features that are mostly consistent within an object class. This pre-training greatly improves data annotation efficiency, both in semantic segmentation and object detection, and is even applicable in cross-dataset setups.</p>

<hr />

<h2 id="raw-high-definition-radar-for-multi-task-learning">Raw High-Definition Radar for Multi-Task Learning</h2>
<h4 id="authors--julien-rebut-arthur-ouaknine-waqas-walik-patrick-pérez">Authors:  <a href="https://scholar.google.com/citations?user=BJcQNcoAAAAJ">Julien Rebut</a>, <a href="https://arthurouaknine.github.io/">Arthur Ouaknine</a>, Waqas Walik, <a href="https://ptrckprz.github.io/">Patrick Pérez</a></h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2112.10646 ">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/RADIal">Code</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/blog/publications/radial/">Project page</a>]</h4>

<p>With their robustness to adverse weather conditions and their ability to measure speeds, radar sensors have been part of the automotive landscape for more than two decades. Recent progress toward High Definition (HD) Imaging radars has driven the angular resolution below the degree, thus approaching laser scanning performance. However, the amount of data a HD radar delivers and the computational cost to estimate the angular positions remain a challenge. In this paper, we propose a novel HD radar sensing model, FFT-RadNet, that eliminates the overhead of computing the range-azimuth-Doppler 3D tensor, learning instead to recover angles from a range-Doppler spectrum. This architecture can be leveraged for various perception tasks with raw HD radar signals. In particular we show how to train FFT-RadNet both to detect vehicles and to segment free driving space. On both tasks, it competes with the most recent radar-based models while requiring less compute and memory.</p>

<p><img src="/blog/images/posts/2022_cvpr/radial_overview.png" alt="" height="100%" width="100%" /></p>
<div class="caption">Overview of FFT-RadNet for vehicle detection and drivable space segmentation in raw HD radar signal.</div>

<p>Also, and importantly, we collected and annotated 2-hour worth of raw data from synchronized automotive-grade sensors (camera, laser, HD radar) in various environments (city street, highway, countryside road). This unique dataset, nick-named RADIal for “Radar, Lidar et al.”, is <a href="https://valeoai.github.io/blog/publications/radial/">publicly available</a>.</p>

<p><img src="/blog/images/posts/2022_cvpr/radial_teaser.jpg" alt="" height="100%" width="100%" /></p>
<div class="caption"><b>Scene sample form RADIal dataset</b> with (a) camera image, (b) radar power spectrum, (c) free-space in bird-eye view, (d) Range-azimuth map in Cartesian coordinates, and (e) GPS trace (red) and odometry trajectory (green); laser (resp. radar) points are in red (resp. indigo), annotated vehicle bounding boxes in orange and annotated drivable space in green.</div>

<hr />

<h2 id="poco-point-convolution-for-surface-reconstruction">POCO: Point convolution for surface reconstruction</h2>
<h4 id="authors-alexandre-boulch-renaud-marlet">Authors: <a href="https://boulch.eu/">Alexandre Boulch</a>, <a href="http://imagine.enpc.fr/~marletr/">Renaud Marlet</a></h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2201.01831">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/POCO">Code</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/blog/publications/poco/">Project page</a>]</h4>

<p>Implicit neural networks have been successfully used for surface reconstruction from point clouds. However, many of them face scalability issues as they encode the isosurface function of a whole object or scene into a single latent vector. To overcome this limitation, a few approaches infer latent vectors on a coarse regular 3D grid or on 3D patches, and interpolate them to answer occupancy queries. In doing so, they lose the direct connection with the input points sampled on the surface of objects, and they attach information uniformly in space rather than where it matters the most, i.e., near the surface. Besides, relying on fixed patch sizes may require discretization tuning.</p>

<p>In POCO, we propose to use point cloud convolution and compute a latent vector at each input point. We then perform a learning-based interpolation on nearest neighbors using inferred weights. On the one hand, using a convolutional backbone allows the aggregation of global information about the shape needed to correctly orientate the surface (decide which side of the surface is inside or outside). On the other hand, surface location is inferred via a local attention-based approach which enables accurate surface positioning.</p>

<p><img src="/blog/images/posts/2022_cvpr/poco_teaser.png" alt="" height="100%" width="100%" /></p>
<div class="caption"><b>POCO overview.</b> Top row: the decoding mechanism takes as input local latent vectors and local coordinates which are lifted with a point-wise MLP. The resulting representations are weighted with an attention mechanism in order to take the occupancy decision.
Bottom row: reconstruction examples with POCO, scene reconstruction with a model trained on objects (left), object reconstruction with noisy point cloud (middle) and out of domain object reconstruction (right).
</div>

<p>We show that our approach, while being very simple to set up, reaches the state of the art on several reconstruction-from-point-cloud benchmarks. It underlines the importance of reasoning about the surface location at a local scale, close to the input points.
POCO also shows good generalization properties including the possibility of learning on object datasets while being able to reconstruct complex scenes.</p>

<hr />

<h2 id="dytox-transformers-for-continual-learning-with-dynamic-token-expansion">DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion</h2>
<h4 id="authors-arthur-douillard--alexandre-ramé--guillaume-couairon-matthieu-cord">Authors: <a href="https://arthurdouillard.com/">Arthur Douillard</a>,  <a href="https://alexrame.github.io/">Alexandre Ramé</a>,  Guillaume Couairon, <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a></h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2111.11326">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/arthurdouillard/dytox">Code</a>] </h4>

<p>Deep network architectures struggle to continually learn new tasks without forgetting the previous tasks. In this paper, we propose a transformer architecture based on a dedicated encoder/decoder framework. Critically, the encoder and decoder are shared among all tasks. Through a dynamic expansion of special tokens, we specialize each forward of our decoder network on a task distribution. Our strategy scales to a large number of tasks while having negligible memory and time overheads due to strict control of the parameters expansion. Moreover, this efficient strategy does not need any hyperparameter tuning to control the network’s expansion. Our model reaches excellent results on CIFAR100 and state-of-the-art performances on the large-scale ImageNet100 and ImageNet1000 while having fewer parameters than concurrent dynamic frameworks.</p>

<p><img src="/blog/images/posts/2022_cvpr/dytox.png" alt="" height="85%" width="85%" /></p>
<div class="caption"> DyTox transformer model.</div>

<hr />

<h2 id="flexit-towards-flexible-semantic-image-translation">FlexIT: Towards Flexible Semantic Image Translation</h2>
<h4 id="authors-guillaume-couairon-asya-grechka-jakob-verbeek-holger-schwenk-matthieu-cord">Authors: Guillaume Couairon, Asya Grechka, <a href="https://lear.inrialpes.fr/people/verbeek/">Jakob Verbeek</a>, <a href="https://scholar.google.fr/citations?user=Ysjk8kkAAAAJ&amp;hl=en">Holger Schwenk</a>, <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a></h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2203.04705">Paper</a>] </h4>

<p>Deep generative models, like GANs, have considerably improved the state of the art in image synthesis, and are able to generate near photo-realistic images in structured domains such as human faces. We propose FlexIT, a novel method which can take any input image and a user-defined text instruction for editing. Our method achieves flexible and natural editing, pushing the limits of semantic image translation. First, FlexIT combines the input image and text into a single target point in the CLIP multimodal embedding space. Via the latent space of an auto-encoder, we iteratively transform the input image toward the target point, ensuring coherence and quality with a variety of novel regularization terms. We propose an evaluation protocol for semantic image translation, and thoroughly evaluate our method on ImageNet.</p>

<p><img src="/blog/images/posts/2022_cvpr/flexit.png" alt="" height="95%" width="95%" /></p>
<div class="caption"> <b>FlexIT transformation examples.</b> From top to bottom: input image, transformed image, and text query.</div>

<hr />

<h2 id="raising-context-awareness-in-motion-forecasting">Raising context awareness in motion forecasting</h2>
<p class="page-description"><a href="https://cvpr2022.wad.vision/">CVPR 2022 Workshop on Autonomous Driving</a></p>

<h4 id="authors-hédi-ben-younes-éloi-zablocki-mickaël-chen-patrick-pérez-matthieu-cord">Authors: <a href="https://scholar.google.com/citations?hl=fr&amp;user=IFLcfvUAAAAJ">Hédi Ben-Younes</a>, <a href="https://scholar.google.com/citations?user=dOkbUmEAAAAJ&amp;hl=fr">Éloi Zablocki</a>, <a href="https://scholar.google.com/citations?user=QnRpMJAAAAAJ&amp;hl=fr&amp;oi=sra">Mickaël Chen</a>, <a href="https://ptrckprz.github.io/">Patrick Pérez</a>, <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a></h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2109.08048">Paper</a>]</h4>

<p><img src="/blog/images/posts/2022_cvpr/cab.png" alt="cab_overview" height="50%" width="50%" /></p>
<div class="caption"><b>Overview of CAB.</b> CAB employs a CVAE backbone which produces distributions over the latent variable and the future trajectory. During training, a blind input is forwarded into the CVAE and the resulting distribution over the latent variable is used to encourage the prediction of the model to be different from the context-agnostic distribution, thanks to the CAB-KL loss.</div>

<p>Learning-based trajectory prediction models have encountered great success, with the promise of leveraging contextual information in addition to motion history. Yet, we find that state-of-the-art forecasting methods tend to overly rely on the agent’s current dynamics, failing to exploit the semantic contextual cues provided at its input. To alleviate this issue, we introduce CAB, a motion forecasting model equipped with a training procedure designed to promote the use of semantic contextual information. We also introduce two novel metrics, dispersion and convergence-to-range, to measure the temporal consistency of successive forecasts, which we found missing in standard metrics. Our method is evaluated on the widely adopted nuScenes Prediction benchmark as well as on a subset of the most difficult examples from this benchmark.</p>

<hr />

<h2 id="csg0-continual-urban-scene-generation-with-zero-forgetting">CSG0: Continual Urban Scene Generation with Zero Forgetting</h2>
<p class="page-description"><a href="https://sites.google.com/view/clvision2022">CVPR 2022 Workshop on Continual Learning (CLVision)</a></p>

<h4 id="authors--himalaya-jain-tuan-hung-vu-patrick-pérez-matthieu-cord">Authors:  <a href="https://himalayajain.github.io/">Himalaya Jain</a>, <a href="https://tuanhungvu.github.io/">Tuan-Hung Vu</a>, <a href="https://ptrckprz.github.io/">Patrick Pérez</a>, <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a></h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2112.03252">Paper</a>] &nbsp;&nbsp; [<a href=" https://valeoai.github.io/blog/publications/csg0/ 
">Project page</a>]</h4>

<p>With the rapid advances in generative adversarial networks (GANs), the visual quality of synthesized scenes keeps improving, including for complex urban scenes with applications to automated driving. We address in this work a continual scene generation setup in which GANs are trained on a stream of distinct domains; ideally, the learned models should eventually be able to generate new scenes in all seen domains. This setup reflects the real-life scenario where data are continuously acquired in different places at different times. In such a continual setup, we aim for learning with zero forgetting, i.e., with no degradation in synthesis quality over earlier domains due to catastrophic forgetting. To this end, we introduce a novel framework, named CSG0, that not only (i) enables seamless knowledge transfer in continual training but also (ii) guarantees zero forgetting with a small overhead cost.</p>

<p><img src="/blog/images/posts/2022_cvpr/csg0_teaser.png" alt="" height="85%" width="85%" /></p>
<div class="caption"><b>Overview of CSG0.</b> Our continual setup for urban-scene generation involves a stream of datasets, with GANs trained from one dataset to another. Our framework makes use of the knowledge learned from previous domains and adapts to new ones with a small overhead.</div>

<p>To showcase the merit of our framework, we conduct intensive experiments on various continual urban scene setups, covering both synthetic-to-real and real-to-real scenarios. Quantitative evaluations and qualitative visualizations demonstrate the interest of our CSG0 framework, which operates with minimal overhead cost (in terms of architecture size and training). Benefiting from continual learning, CSG0 outperforms the state-of-the-art OASIS model trained on single domains. We also provide experiments with three datasets to emphasize how well our strategy generalizes despite its cost constraints. Under extreme low-data regimes, our approach outperforms the baseline by a large margin.</p>

<hr />

<h2 id="multi-head-distillation-for-continual-unsupervised-domain-adaptation-in-semantic-segmentation">Multi-Head Distillation for Continual Unsupervised Domain Adaptation in Semantic Segmentation</h2>
<p class="page-description"><a href="https://sites.google.com/view/clvision2022">CVPR 2022 Workshop on Continual Learning (CLVision)</a></p>

<h4 id="authors--antoine-saporta-arthur-douillard-tuan-hung-vu-patrick-pérez-matthieu-cord">Authors:  <a href="https://scholar.google.com/citations?user=jSwfIU4AAAAJ">Antoine Saporta</a>, <a href="https://arthurdouillard.com/">Arthur Douillard</a>, <a href="https://tuanhungvu.github.io/">Tuan-Hung Vu</a>, <a href="https://ptrckprz.github.io/">Patrick Pérez</a>, <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a></h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2204.11667">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/MuHDi">Code</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/blog/publications/muhdi/">Project page</a>]</h4>

<p>This work focuses on a novel framework for learning UDA, continuous UDA, in which models operate on multiple target domains discovered sequentially, without access to previous target domains. We propose MuHDi, for Multi-Head Distillation, a method that solves the catastrophic forgetting problem, inherent in continual learning tasks. MuHDi performs distillation at multiple levels from the previous model as well as an auxiliary target-specialist segmentation head. We report both extensive ablation and experiments on challenging multi-target UDA semantic segmentation benchmarks to validate the proposed learning scheme and architecture.</p>

<p><img src="/blog/images/posts/2022_cvpr/muhdi_teaser.png" alt="" height="90%" width="90%" /></p>
<div class="caption"><b>Predictions of continual baseline and MuHDi in a Cityscapes scene.</b> The baseline model suffers from catastrophic forgetting when adapting from one domain to another. The proposed MuHDi is more resilient to continual adaptation and preserve predictive accuracy.
</div>

<h2 id="references">References</h2>

<ol class="bibliography"><li><span id="chen2020improved">Chen, X., Fan, H., Girshick, R., &amp; He, K. (2020). Improved baselines with momentum contrastive learning. <i>ArXiv Preprint ArXiv:2003.04297</i>.</span></li></ol>]]></content><author><name></name></author><category term="domain adaptation" /><category term="3d perception" /><category term="multi-sensor" /><category term="limited supervision" /><summary type="html"><![CDATA[Corentin Sautier, Alexandre Boulch, Patrick Pérez, Éloi Zablocki, Tuan-Hung Vu, Matthieu Cord, Andrei Bursuc]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io/blog/images/posts/2022_cvpr/cvpr_logo.png" /><media:content medium="image" url="https://valeoai.github.io/blog/images/posts/2022_cvpr/cvpr_logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">valeo.ai at ICCV 2021</title><link href="https://valeoai.github.io/blog/2021/10/08/valeoai-at-iccv-2021.html" rel="alternate" type="text/html" title="valeo.ai at ICCV 2021" /><published>2021-10-08T00:00:00-05:00</published><updated>2021-10-08T00:00:00-05:00</updated><id>https://valeoai.github.io/blog/2021/10/08/valeoai-at-iccv-2021</id><content type="html" xml:base="https://valeoai.github.io/blog/2021/10/08/valeoai-at-iccv-2021.html"><![CDATA[<p>The <a href="https://iccv2021.thecvf.com/home">International Conference on Computer Vision (ICCV)</a> is a top event for researchers and engineers working on computer vision and machine learning. The <a href="https://ptrckprz.github.io/valeoai/">valeo.ai</a> team will present six <a href="https://ptrckprz.github.io/vaipub/">papers</a> in the main conference, four of which are presented below. Join us to find out more about these projects and ideas, meet our team and learn about our exciting ongoing research. See you at ICCV!</p>

<h2 id="multi-view-radar-semantic-segmentation">Multi-View Radar Semantic Segmentation</h2>
<h4 id="authors-arthur-ouaknine-alasdair-newson-patrick-pérez-florence-tupin-julien-rebut">Authors: <a href="https://arthurouaknine.github.io/">Arthur Ouaknine</a>, <a href="https://sites.google.com/site/alasdairnewson/">Alasdair Newson</a>, <a href="https://ptrckprz.github.io/">Patrick Pérez</a>, <a href="https://perso.telecom-paristech.fr/tupin/">Florence Tupin</a>, <a href="https://scholar.google.com/citations?user=BJcQNcoAAAAJ&amp;hl=fr">Julien Rebut</a></h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2103.16214">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/MVRSS">Code</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/blog/publications/mvrss/">Project page</a>]</h4>

<p><img src="/blog/images/posts/2021_iccv/radar.gif" alt="radar_overview" height="80%" width="80%" /></p>
<div class="caption"><b>Example of a scene from the CARRADA dataset <a class="citation" href="#ouaknine2021carrada">(Ouaknine et al., 2021)</a>.</b> From left to right: camera image, range-angle view, range-Doppler view, angle, Doppler view.</div>

<p>Understanding the scene around the ego-vehicle is key to assisted and autonomous driving. Nowadays, this is mostly conducted using cameras and laser scanners, despite their reduced performance in adverse weather conditions. Automotive radars are low-cost active sensors that measure properties of surrounding objects, including their relative speed, and have the key advantage of not being impacted by rain, snow or fog and could effectively complement the other perception sensors mounted on the car, e.g., cameras, LIDAR. However, they are seldom used for scene understanding due to the size and complexity of radar raw data and the lack of annotated datasets. Fortunately, recent open-sourced datasets have opened up research on classification, object detection and semantic segmentation with raw radar signals using end-to-end trainable models.</p>

<p><img src="/blog/images/posts/2021_iccv/radar_semseg.png" alt="mvrss_overview" height="80%" width="80%" /></p>
<div class="caption">Sequences of raw radar tensors are aggregated and used as input for our multi-view architecture to segment semantically range-angle and range-Doppler views simultaneously.
</div>

<p>In our paper, <a href="https://arxiv.org/abs/2103.16214">Multi-View Radar Semantic Segmentation</a>, we propose a set of deep neural network architectures to segment simultaneously range-angle and range-Doppler radar representations, providing the location and the radial velocity of the detected objects. Our best model takes a sequence of radar views as input, extracts features using individual branches including ASPP blocks, and recovers the range-angle and range-Doppler view dimensions with two decoding branches. We also propose a combination of loss functions composed of a weighted cross entropy, a soft dice and an additional coherence term. We introduce a coherence loss to impose a spatial consistency between the segmented radar views. Our experiments on the CARRADA dataset <a class="citation" href="#ouaknine2021carrada">(Ouaknine et al., 2021)</a> demonstrate that our best model outperforms competing methods with a large margin while requiring significantly fewer parameters.</p>

<hr />

<h2 id="triggering-failures-out-of-distribution-detection-by-learning-from-local-adversarial-attacks-in-semantic-segmentation">Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation</h2>
<h4 id="authors-victor-besnier-andrei-bursuc-alexandre-briot-david-picard">Authors: <a href="https://scholar.google.com/citations?user=n_C2h-QAAAAJ">Victor Besnier</a>, <a href="https://abursuc.github.io/">Andrei Bursuc</a>, Alexandre Briot, <a href="https://davidpicard.github.io/">David Picard</a></h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2108.01634">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/obsnet">Code</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/blog/publications/obsnet/">Project page</a>]</h4>

<p><img src="/blog/images/posts/2021_iccv/obsnet_qualitative.png" alt="" height="100%" width="100%" /></p>
<div class="caption"><b>Uncertainty map visualization on the BDD-Anomaly dataset.</b> 1st col.: We highlight the ground truth locations of the OOD objects to help visualize them (red bounding box). 2nd col.: Segmentation map of the SegNet. 3rd to 5th col.: Uncertainty Map highlighted in yellow. Our method produces stronger responses on OOD regions compared to other methods, while being as strong on regular error regions, e.g., boundaries. 
</div>

<p>For real-world decision systems such as autonomous vehicles, accuracy is not the only performance requirement and it often comes second to <em>reliability</em>, <em>robustness</em>, and <em>safety concerns</em>, as any failure carries serious consequences. Component modules of such systems frequently rely on powerful Deep Neural Networks (DNNs), that however do not always generalize to objects unseen in the training data.  Simple uncertainty estimation techniques, e.g., entropy of softmax predictions, are less effective since modern DNNs are consistently overconfident on both in-domain and out-of-distribution (OOD) data samples. This hinders further the performance of downstream components relying on their predictions. Dealing successfully with the <em>“unknown unknown”</em>, e.g., by launching an alert or failing gracefully, is crucial.</p>

<p><img src="/blog/images/posts/2021_iccv/robot_hit.gif" alt="" height="50%" width="50%" /></p>
<div class="caption">By making our target model to fail we can learn its behavior when failing and more reliably detect it at test time.</div>

<p>In this work we take inspiration from practices in industrial validation, where the performance of a target model is tested in various extreme cases. Instead of simply verifying the performance of the model we learn how this model behaves in face of failures. To this end we propose a new OOD detection architecture called ObsNet and an associated training scheme based on Local Adversarial Attacks (LAA). Finding failure modes in a trained DNN is quite challenging as such models typically achieve high accuracy, i.e., are rarely wrong, and corner-case samples are rather inserted in the training set than used for validation. LAA triggers failure modes in the target model that are a good proxy for failures in face of unknown OOD data. 
ObsNet achieves reliable detection of failure and OOD objects without compromising on predictive accuracy and computational time.</p>

<hr />

<h2 id="multi-target-adversarial-frameworks-for-domain-adaptation-in-semantic-segmentation">Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation</h2>
<h4 id="authors--antoine-saporta-tuan-hung-vu-matthieu-cord-patrick-pérez">Authors:  <a href="https://scholar.google.com/citations?user=jSwfIU4AAAAJ">Antoine Saporta</a>, <a href="https://tuanhungvu.github.io/">Tuan-Hung Vu</a>, <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a>, <a href="https://ptrckprz.github.io/">Patrick Pérez</a></h4>

<h4 align="center"> [<a href="https://arxiv.org/abs/2108.06962">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/MTAF">Code</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/blog/publications/mtaf/">Project page</a>]</h4>

<p><img src="/blog/images/posts/2021_iccv/mtaf_teaser.png" alt="" height="100%" width="100%" /></p>
<div class="caption"><b>Single-target unsupervised domain adaptation fails to produce models that perform on multiple target domains.</b> The aim of multi-target unsupervised domain adaptation is to train a model that excels on these multiple target domains.</div>

<p>Autonomous vehicles rely on perception models that require a tremendous amount of annotated data to be trained in a supervised fashion. To reduce the reliance on manual annotation which can get extremely expensive when we consider semantic segmentation of urban scenes for instance, domain adaptation is a popular topic that leverages annotated data from a source domain to train a model on a target domain. More specifically, the unsupervised domain adaptation (UDA) setting only relies on unlabeled data from the target domain and aims at bridging the gap between target and source domains. Most UDA approaches tackle the alignment between a single source domain and a single target domain but don’t generalize well to more domains. Yet, real-world perception systems need to be confronted to a variety of scenarios, such as multiple cities or multiple weather conditions, motivating to extend UDA to multi-target settings.</p>

<p>In our work, <a href="&quot;https://arxiv.org/abs/2108.06962">Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation</a>, we introduce two UDA frameworks to tackle multi-target adaptation: <strong>(i)</strong> multi-discriminator, which extends single target UDA approaches to multiple target domains by explicitly aligning each target domain to its counterparts; <strong>(ii)</strong> multi-target knowledge transfer, which learns a target-agnostic model thanks to a multiple teachers/single student distillation mechanism. We also propose multiple new challenging evaluation benchmarks for multi-target UDA in semantic segmentation based on existing urban scenes datasets.</p>

<hr />

<h2 id="pcam-product-of-cross-attention-matrices-for-rigid-registration-of-point-clouds">PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds</h2>
<h4 id="authors-anh-quan-cao-gilles-puy-alexandre-boulch-renaud-marlet">Authors: <a href="https://anhquancao.github.io">Anh-Quan Cao</a>, <a href="https://sites.google.com/site/puygilles/home">Gilles Puy</a>, <a href="https://www.boulch.eu/">Alexandre Boulch</a>, <a href="http://imagine.enpc.fr/~marletr/">Renaud Marlet</a></h4>

<h4 align="center"> [<a href="http://arxiv.org/abs/2110.01269">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/PCAM">Code</a>] &nbsp;&nbsp; [<a href="https://valeoai.github.io/blog/publications/pcam/">Project page</a>]</h4>

<p><img src="/blog/images/posts/2021_iccv/pcam_overview.png" alt="" height="100%" width="100%" /></p>

<p>Point cloud registration has many applications in various domains such as autonomous driving, motion and pose estimation, 3D reconstruction, simultaneous localisation and mapping (SLAM), and augmented reality. The most famous method to solve this task is ICP, but is mostly suited for small transformations. Several improvements have been made and the most recent techniques leverage deep learning.</p>

<p>The typical pipeline for point cloud registration is <strong>(a)</strong> point matching followed by <strong>(b)</strong> point-pairs filtering to remove incorrect matches in, e.g., non-overlapping regions. One natural way to improve this pipeline is to use deep learning in step <strong>(a)</strong> to obtain point features of high quality and get pairs of matching points with a nearest neighbors search in this learned feature space. Then, one can typically rely on a classical RANSAC-based method in step <strong>(b)</strong>. Another category of methods exploits deep learning in step <strong>(a)</strong> and step <strong>(b)</strong>, as proposed by, e.g., DCP, PRNet, DGR. PCAM belongs to this second category where a first network outputs pairs of matching points and a second network filters incorrect pairs.</p>

<p>We construct PCAM by observing that one needs two types of information to correctly match points between two point clouds. First, one needs local fine geometric information to precisely select the best corresponding point. Second, one also needs high-level contextual information to differentiate between points with similar local geometry but from different parts of the scene. Therefore, we compute point correspondences at every layer of our deep network via cross-attention matrices, and combine these matrices via a pointwise multiplication. This simple yet very effective solution naturally ensures that both low-level geometric and high-level context information are exploited when matching points. It also permits to remove spurious matches found only at one scale. Furthermore, these cross-attention matrices are also exploited to exchange information between the point clouds at each layer, allowing the network to use context information to find the best matching point within the overlapping regions.</p>

<h2 id="references">References</h2>

<ol class="bibliography"><li><span id="ouaknine2021carrada">Ouaknine, A., Newson, A., Rebut, J., Tupin, F., &amp; Pérez, P. (2021). CARRADA Dataset: Camera and Automotive Radar with Range- Angle- Doppler Annotations. <i>2020 25th International Conference on Pattern Recognition (ICPR)</i>, 5068–5075.</span></li></ol>]]></content><author><name></name></author><category term="domain adaptation" /><category term="3d perception" /><category term="reliability" /><category term="multi-sensor" /><category term="limited supervision" /><summary type="html"><![CDATA[Andrei Bursuc, Gilles Puy, Arthur Ouaknine, Antoine Saporta]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io/blog/images/posts/2021_iccv/iccv_logo.png" /><media:content medium="image" url="https://valeoai.github.io/blog/images/posts/2021_iccv/iccv_logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">How can we make driving systems explainable?</title><link href="https://valeoai.github.io/blog/2021/02/18/explainable-driving.html" rel="alternate" type="text/html" title="How can we make driving systems explainable?" /><published>2021-02-18T00:00:00-06:00</published><updated>2021-02-18T00:00:00-06:00</updated><id>https://valeoai.github.io/blog/2021/02/18/explainable-driving</id><content type="html" xml:base="https://valeoai.github.io/blog/2021/02/18/explainable-driving.html"><![CDATA[<p><em>This post is an introduction to our survey on the explainability of vision-based driving systems, which can be found on arXiv <a href="https://arxiv.org/abs/2101.05307">here</a>.</em></p>

<p>Research on autonomous vehicles is blooming thanks to recent advances in deep learning and computer vision, as well as the development of autonomous driving datasets and simulators.
The number of academic publications on this subject is rising in most machine learning, computer vision, robotics and transportation conferences, and journals.
On the industry side, several suppliers are already producing cars equipped with advanced computer vision technologies for automatic lane following, assisted parking, or collision detection among other things. Meanwhile, constructors are working on and designing prototypes with level 4 and 5 autonomy.</p>

<p>In the 2010s, we observe an interest in approaches aiming to <em>train</em> driving systems, usually in the form of neural networks, either by leveraging large quantities of expert recordings or through simulation.
In both cases, these systems learn a highly complex transformation that operates over input sensor data and produces end-commands (steering angle, throttle). 
While these neural driving models overcome some of the limitations of the traditional modular pipeline stack, they are sometimes described as <em>black-boxes</em> for their critical lack of transparency and interpretability. 
Thus, being able to explain the behavior of neural driving models is of paramount importance for their deployment and social acceptance.</p>

<h3 id="explainability">Explainability?</h3>

<p>Many terms are related to the concept of explainability and several definitions have been proposed for each of these terms. The boundaries between concepts are fuzzy and constantly evolving. 
In human-machine interactions, explainability is defined as the ability for the human user to understand the agent’s logic <a class="citation" href="#RosenfeldR19">(Rosenfeld &amp; Richardson, 2019)</a>. 
The explanation is based on how the human user understands the connections between inputs and outputs of the model. 
According to <a class="citation" href="#doshi2017accountability">(Doshi-Velez &amp; Kortz, 2017)</a>, an explanation is a human-interpretable description of the process by which a decision-maker took a particular set of inputs and reached a particular conclusion. They state that in practice, an explanation should answer at least one of the three following questions:</p>
<ul>
  <li><em>What were the main factors in the decision?</em></li>
  <li><em>Would changing a certain factor have changed the decision?</em></li>
  <li><em>Why did two similar-looking cases get different decisions, or vice versa?</em></li>
</ul>

<p>The term <em>explainability</em> often co-occurs with the concept of <em>interpretability</em>.
Some recent work of <a class="citation" href="#beaudouin2020identifying">(Beaudouin et al., 2020)</a> simply advocate that explainability and interpretability are synonyms.
However, <a class="citation" href="#GilpinBYBSK18">(Gilpin et al., 2018)</a> provide a nuance between these terms that we find interesting. According to them, interpretability designates to which extent an explanation is understandable by a human. 
They state that an explanation should be designed and assessed in a trade-off between its interpretability and its completeness, which measures how accurate the explanation is as it describes the inner workings of the system. 
<!-- For example, an exhaustive and completely faithful explanation is a description of the system itself and all its processing: this is a complete explanation although the exhaustive description of the processing may be incomprehensible. -->
The whole challenge in explaining neural networks is to provide explanations that are both interpretable and complete.</p>

<p>Interestingly, depending on who is the explanation geared towards, it is expected to have varying nature, form and should convey different types of information.</p>
<ul>
  <li><strong>End-users</strong> and citizens need to trust the autonomous system and to be reassured. They put their life in the hands of the driving system and thus need to gain trust in it. 
<!-- There is a long and dense line of research trying to define, characterize, evaluate, and increase the trust between an individual and a machine.  -->
It appears that user trust is heavily impacted by the system transparency <a class="citation" href="#trusthci20">(Zhang et al., 2020)</a>: providing information that helps the user understand how the system functions foster his or her trust in the system. 
<!-- Interestingly, research on human-computer interactions argues that the timing of explanations is important for trust: they should be provided before the vehicle takes an action, in a formulation which is concise and direct. <a class="citation" href="#RosenfeldR19">(Rosenfeld &amp; Richardson, 2019)</a>,<a class="citation" href="#haspiel2018explanations">(Haspiel et al., 2018)</a>,<a class="citation" href="#du2019look">(Du et al., 2019)</a> -->
Interestingly, research on human-computer interactions argues that an explanation should be provided before the vehicle takes an action, in a formulation which is concise and direct.</li>
  <li><strong>Designers</strong> of self-driving models need to understand their limitations to validate them and improve future versions.
The concept of Operational Design Domain (ODD) is often used by carmakers to designate the conditions under which the car is expected to behave safely.
Thus, whenever a machine learning model is built to address the task of driving, it is crucial to know and understand its failure modes, and to verify that these situations do not overlap with the ODD. 
A common practice is to stratify the evaluation into situations, as is done by the European New Car Assessment Program (Euro NCAP) to test and assess assisted driving functionalities in new vehicles.
But even if these in-depth performance analyses are helpful to improve the model’s performance, it is not possible to exhaustively list and evaluate every situation the model may possibly encounter. 
As a fallback solution, explainability can help delving deeper into the inner workings of the model and to understand why it makes these errors and correct the model/training data accordingly.</li>
  <li><strong>Legal and regulatory bodies</strong> are interested in explanations for <em>liability</em> and <em>accountability</em> purposes, especially when a self-driving system is involved in a car accident. 
 Notably, explanations generated for legal or regulatory institutions are likely to be different from those addressed to the end-user, as all aspects of the decision process could be required to identify the reasons for a malfunction.
<!-- These explanations are directed towards experts who will likely spend large amounts of time studying the system, and who are thus inclined to receive rich explanations with great amounts of detail.  --></li>
</ul>

<h3 id="driving-system">Driving system?</h3>

<p>The history of autonomous driving systems started in the late ’80s and early ’90s with the European Eureka project called Prometheus.
This has later been followed by <a href="https://www.youtube.com/watch?v=7a6GrKqOxeU">driving challenges</a> proposed by the Defense Advanced Research Projects Agency (DARPA). 
The vast majority of autonomous systems competing in these challenges is characterized by their modularity: several sub-modules are assembled, each completing a very specific task. 
Broadly speaking, these subtasks deal with sensing the environment, forecasting future events, planning, taking high-level decisions, and controlling the vehicle.</p>

<p>As pipeline architectures split the driving task into easier-to-solve problems, they offer somewhat interpretable processing of sensor data through specialized modules (perception, planning, decision, control).
However, these approaches have several drawbacks:</p>
<ul>
  <li>First, they rely on human heuristics and manually-chosen intermediate representations, which are not proven to be optimal for the driving task.</li>
  <li>Second, they lack flexibility to account for real-world uncertainties and to generalize to unplanned scenarios.
<!-- Moreover, from an engineering point of view, these systems are hard to scale and to maintain as the various modules are entangled together. --></li>
  <li>Finally, they are prone to error propagation between the multiple sub-modules.</li>
</ul>

<p>To circumvent these issues, and nurtured by the deep learning revolution, researchers put more and more efforts on machine learning-based driving systems, and in particular on deep neural networks which can leverage large quantities of data.</p>

<p>We can distinguish four key elements involved in the design of a neural driving system: input sensors, input representations, output type, and learning paradigm</p>

<p><img src="/blog/images/posts/explainable_driving/driving_architecture.png" alt="driving_architecture" width="80%" /></p>
<div class="caption"><b>Figure 1. Overview of neural network-based autonomous driving systems.</b></div>

<ul>
  <li><strong>Sensors</strong>. They are the hardware interface through which the neural network perceives its environment.
<!-- Typical neural driving systems rely on sensors from two families: *proprioceptive* sensors and *exteroceptive* sensors. *Proprioceptive* sensors provide information about the internal vehicle state such as speed, acceleration, yaw, change of position, and velocity. They are measured through tachometers, inertial measurement units (IMU), and odometers.  All these sensors communicate through the controller area network (CAN) bus, which allows signals to be easily accessible. In contrast, *exteroceptive* sensors acquire information about the surrounding environment.  -->
They include cameras, radars, LiDARs, GPS, but also sensors about internal vehicle state such as speed or yaw. For a thorough review of driving sensors, we refer the reader to <a class="citation" href="#survey_sensors">(Yurtsever et al., 2020)</a>.</li>
  <li><strong>Input representation</strong>. Once sensory inputs are acquired by the system, they are processed by computer vision models to build a structured representation, before being passed to the neural driving system. In the <em>mediated perception</em> approach, several perception systems provide their understanding of the world, and their outputs are aggregated to build an input for the driving model.
An example of such vision tasks is object detection and semantic segmentation, tracking objects across time, extracting depth information (<em>i.e.</em> knowing the distance that separates the vehicle from each point in the space), recognizing pedestrian intent…
Mediated perception contrasts with the <em>direct perception</em> approach, which instead extracts visual affordances from an image.
Affordances are scalar indicators that describe the road situation such as curvature, deviation to neighboring lanes, or distances between ego and other vehicles.
<!-- These human-interpretable features are usually recognized using neural networks as in <a class="citation" href="#deepdrivingaffordance">(Chen et al., 2015)</a>. -->
<!-- Then, they are passed at the input of a driving controller which is usually hard-coded, even if some recent approaches use affordance recognition to provide compact inputs to learning-based driving systems <a class="citation" href="#marinaffordance">(Toromanoff et al., 2020)</a>.  --></li>
  <li><strong>Outputs</strong>. Ultimately, the goal is to generate vehicle controls. Some approaches, called end-to-<em>end</em>, tackle this problem by training the deep network to directly output the commands.
However, in practice most methods instead predict the future trajectory of the autonomous vehicle; they are called end-to-<em>mid</em> methods. The trajectory is then expected to be followed by a low-level controller, such as the proportional–integral–derivative (PID) controller.</li>
  <li><strong>Learning</strong>.
Two families of methods coexist for training self-driving neural models: <em>behavior cloning</em> approaches, which leverage datasets of human driving sessions, and <em>reinforcement learning</em> approaches, which train models through trial-and-error simulation.
    <ul>
      <li>Behavior cloning (BC) approaches leverage huge quantities of recorded human driving sessions to learn the input-output driving mapping by imitation. 
  In this setting, the network is trained to mimic the commands applied by the expert driver (end-to-end models), or the future trajectory (end-to-mid models), in a supervised fashion. 
  An initial attempt to behavior cloning of vehicle controls was made by <a class="citation" href="#Pomerleau88">(Pomerleau, 1988)</a>, and continued later in <a class="citation" href="#pilotnet">(Bojarski et al., 2016)</a>.</li>
      <li>Reinforcement learning (RL) was alternatively explored by researchers to train neural driving systems. This paradigm learns a policy by balancing self-exploration and reinforcement.
  This training paradigm relies on a simulator (such as CARLA <a class="citation" href="#carla">(Dosovitskiy et al., 2017)</a>).</li>
    </ul>
  </li>
</ul>

<h3 id="the-challenges-of-explainability-of-neural-driving-systems">The challenges of explainability of neural driving systems</h3>

<p>Introducing explainability in the design of learning-based self-driving systems is a challenging task.
These concerns arise from two aspects:</p>
<ul>
  <li>From a <strong>Deep Learning perspective</strong>, explainability hurdles of self-driving models are shared with most deep learning models, across many application domains. Indeed, decisions of deep systems are intrinsically hard to explain as the functions these systems represent, mapping from inputs to outputs, are not transparent. 
In particular, although it may be possible for an expert to broadly understand the structure of the model, the parameter values, which have been learned, are yet to be explained.
Several factors cause interpretability issues for self-driving machine learning models.
First, a finite training dataset cannot exhaustively cover all possible driving situations. It will likely under- and over-represent some specific cases, and questions such as <em>Has the model encountered situations like X?</em> are legitimate. 
Moreover, datasets contain numerous biases of various nature (omitted variable bias, cause-effect bias, sampling bias), which also gives rise to explainability issues related to fairness.
Second, the mapping function represented by the trained model is poorly understood and is considered as a <em>black-box</em>. The model is highly non-linear and does not provide any robustness guarantee as small input changes may dramatically change the output behavior. 
Explainability issues thus occur regarding the generalizability and robustness aspects: <em>How will the model behave under these new scenarios?</em> 
Third, the learning phase is not perfectly understood. Among other things, there are no guarantees that the model will settle at a minimum point that generalizes well to new situations. Thus, the model may learn to ground its decisions on spurious correlations during training instead of on the true causes. We aim at finding answers to questions like <em>Which factors caused this decision to be taken?</em></li>
</ul>

<p><img src="/blog/images/posts/explainable_driving/ml_challenges.png" alt="ml_challenges" width="80%" /></p>
<div class="caption"><b>Figure 2. Explainability hurdles and questions for autonomous driving models, as seen from a machine learning point of view.</b></div>

<ul>
  <li>From a <strong>driving perspective</strong>, it has been shown that humans tackle this task by solving many intermediate sub-problems, at different levels of hierarchy <a class="citation" href="#michon1984critical">(Michon, 1984)</a>.
In the effort towards building an autonomous driving system, researchers aim at providing the machine with these intermediate capabilities. Thus, explaining the general behavior of an autonomous vehicle inevitably requires understanding how each of these intermediate steps is carried and how it interacts with others. We can categorize these capabilities into three types:
    <ul>
      <li><em>Perception</em>: information about the system’s understanding of its local environment. This includes the objects that have been recognized and assigned to a semantic label (persons, cars, urban furniture, driveable area, crosswalks, traffic lights), their localization, properties of their motion (velocity, acceleration), intentions of other agents, <em>etc</em>.;</li>
      <li><em>Reasoning</em>: information about how the different components of the perceived environment are organized and assembled by the system. This includes global explanations about the rules that are learned by the model, instance-wise explanation showing which objects are relevant in a given scene, traffic pattern recognition, object occlusion reasoning, <em>etc.</em>;</li>
      <li><em>Decision</em>: information about how the system processes the perceived environment and its associated reasoning to produce a decision. This decision can be a high-level goal stating that the car should turn right, a prediction of the ego vehicle’s trajectory, its low-level relative motion or even the raw controls, <em>etc</em>.</li>
    </ul>
  </li>
</ul>

<p><img src="/blog/images/posts/explainable_driving/driving_challenges.png" alt="driving_challenges" width="80%" /></p>
<div class="caption"><b>Figure 3. Explainability hurdles and questions for autonomous driving models, as seen from an autonomous driving point of view.</b></div>

<p>While the separation between perception, reasoning, and decision is clear in modular driving systems, some recent end-to-end neural networks such as PilotNet <a class="citation" href="#pilotnet">(Bojarski et al., 2016)</a> blur the lines and perform these simultaneously. Indeed, when an explanation method is developed for a neural driving system, it is often not clear whether it attempts to explain the perception, the reasoning, or the decision step.
Considering the nature of neural networks architecture and training, disentangling perception, reasoning, and decision in neural driving systems constitutes a non-trivial challenge.</p>

<h3 id="conclusion">Conclusion</h3>

<p>As an answer to such problems, many explanation methods have been proposed and are usually organized into two categories: applying <em>post-hoc methods</em> on an already-trained driving model, and directly building driving models which are inherently interpretable <em>by design</em>. 
In our <a href="https://arxiv.org/abs/2101.05307">survey</a>, we provide details on existing explainability techniques, show how they tackle to the problem of explaining driving models and highlight their limitations. 
In addition, we detail remaining challenges and open research avenues to increase explainability of self-driving models.
We hope our survey will enable increased awareness in this area from researchers and practitioners in the field, as well as from other potentially related fields.</p>

<h3 id="references">References</h3>

<ol class="bibliography"><li><span id="RosenfeldR19">Rosenfeld, A., &amp; Richardson, A. (2019). Explainability in human-agent systems. <i>Auton. Agents Multi Agent Syst.</i></span></li>
<li><span id="doshi2017accountability">Doshi-Velez, F., &amp; Kortz, M. A. (2017). Accountability of AI Under the Law: The Role of Explanation. <i>CoRR</i>.</span></li>
<li><span id="beaudouin2020identifying">Beaudouin, V., Bloch, I., Bounie, D., Clémençon, S., d’Alché-Buc, F., Eagan, J., Maxwell, W., Mozharovskyi, P., &amp; Parekh, J. (2020). Flexible and Context-Specific AI Explainability: A Multidisciplinary
               Approach. <i>CoRR</i>.</span></li>
<li><span id="GilpinBYBSK18">Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., &amp; Kagal, L. (2018). Explaining Explanations: An Overview of Interpretability of Machine
               Learning. <i>DSSA</i>.</span></li>
<li><span id="trusthci20">Zhang, Q., Yang, X. J., &amp; Robert, L. P. (2020). Expectations and Trust in Automated Vehicles. <i>CHI</i>.</span></li>
<li><span id="haspiel2018explanations">Haspiel, J., Du, N., Meyerson, J., Jr., L. P. R., Tilbury, D. M., Yang, X. J., &amp; Pradhan, A. K. (2018). Explanations and Expectations: Trust Building in Automated Vehicles. <i>HRI</i>.</span></li>
<li><span id="du2019look">Du, N., Haspiel, J., Zhang, Q., Tilbury, D., Pradhan, A. K., Yang, X. J., &amp; Robert Jr, L. P. (2019). Look who’s talking now: Implications of AV’s explanations on driver’s trust, AV preference, anxiety and mental workload. <i>Transportation Research Part C: Emerging Technologies</i>.</span></li>
<li><span id="survey_sensors">Yurtsever, E., Lambert, J., Carballo, A., &amp; Takeda, K. (2020). A Survey of Autonomous Driving: Common Practices and Emerging Technologies. <i>IEEE Access</i>.</span></li>
<li><span id="deepdrivingaffordance">Chen, C., Seff, A., Kornhauser, A. L., &amp; Xiao, J. (2015). DeepDriving: Learning Affordance for Direct Perception in Autonomous
               Driving. <i>ICCV</i>.</span></li>
<li><span id="marinaffordance">Toromanoff, M., Émilie Wirbel, &amp; Moutarde, F. (2020). End-to-End Model-Free Reinforcement Learning for Urban Driving Using
               Implicit Affordances. <i>CVPR</i>.</span></li>
<li><span id="Pomerleau88">Pomerleau, D. (1988). ALVINN: An Autonomous Land Vehicle in a Neural Network. <i>NIPS</i>.</span></li>
<li><span id="pilotnet">Bojarski, M., Testa, D. D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., Jackel, L. D., Monfort, M., Muller, U., Zhang, J., Zhang, X., Zhao, J., &amp; Zieba, K. (2016). End to End Learning for Self-Driving Cars. <i>CoRR</i>.</span></li>
<li><span id="carla">Dosovitskiy, A., Ros, G., Codevilla, F., López, A., &amp; Koltun, V. (2017). CARLA: An Open Urban Driving Simulator. <i>CoRL</i>.</span></li>
<li><span id="michon1984critical">Michon, J. A. (1984). <i>A Critical View of Driver Behavior Models: What Do We Know, what Should We Do?</i> Human behavior and traffic safety.</span></li></ol>]]></content><author><name></name></author><category term="explainability" /><category term="interpretability" /><category term="survey" /><category term="self-driving" /><summary type="html"><![CDATA[Hedi Ben younes*, Eloi Zablocki*, Matthieu Cord and Patrick Pérez]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io/blog/images/posts/explainable_driving/logo_explainable.png" /><media:content medium="image" url="https://valeoai.github.io/blog/images/posts/explainable_driving/logo_explainable.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving</title><link href="https://valeoai.github.io/blog/2020/11/26/plop-trajectory-prediction.html" rel="alternate" type="text/html" title="PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving" /><published>2020-11-26T00:00:00-06:00</published><updated>2020-11-26T00:00:00-06:00</updated><id>https://valeoai.github.io/blog/2020/11/26/plop-trajectory-prediction</id><content type="html" xml:base="https://valeoai.github.io/blog/2020/11/26/plop-trajectory-prediction.html"><![CDATA[<p><em>This post describes our <a href="https://drive.google.com/file/d/1--QAL2sR7KMk9R4DwxyfJAT5iGCheFrn/view">recent work</a>
 on probabilistic trajectory prediction for autonomous driving presented
 at <a href="https://www.robot-learning.org/home">CORL 2020</a>. PLOP is a trajectory prediction method that
intent to control an autonomous vehicle (ego vehicle) in urban environment while considering 
and predicting the intents of other road users (neighbors). We focus here on predicting
 multiple feasible future trajectories for both ego vehicle and neighbors through a 
probabilistic framework and rely on a conditional imitation learning algorithm, conditioned by a
 navigation command for the ego vehicle (e.g., ``turn right’’). Our model processes only onboard
sensor data (camera and lidars) along with detections of past and presents objects relaxing the
necessity of an HDMap and is computationally efficient as it can run in real time (25 fps) on 
an embedded board in the real vehicle. We evaluate our method offline on the
 publicly available dataset nuScenes <a class="citation" href="#holger2020nuscenes">(Caesar et al., 2020)</a>,
 achieving state-of-the-art performance, investigate 
the impact of our architecture choices on online simulated experiments and show preliminary
 insights for real vehicle control.</em></p>

<p><img src="/blog/images/posts/plop/plop.png" alt="plop_teaser" height="60%" width="60%" /></p>
<div class="caption"><b>Figure 1. Qualitative example of trajectory predictions on a test
 sample from nuScenes dataset.</b> The top image show a bird's eye view of PLOP's predictions
 for the ego and neighbor vehicles (to be compared with the ground truth in green). The bottom
 row present the input image (left) in which we added object correspondance with the bird's
 eye view and the auxiliary semantic segmentation of this image (right)</div>

<p>Predicting the future positions of other agents of the road, or of the
 autonomous vehicle itself, is critical for autonomous driving.
  This trajectory prediction must not only respect the rules of
   the road, but capture the interactions of the agents over time.
    It is also important to allow multiple possible predictions, as
     there is usually not a single valid trajectory.</p>

<p>Some approaches such as ChauffeurNet <a class="citation" href="#bansal2018chauffeutnet">(Bansal et al., 2018)</a> use a high-levelscene representation 
(road map, traffic lights, speed limit, route, dynamic bounding boxes, etc.).
 More recently, MultiPath <a class="citation" href="#chai2019multipath">(Chai et al., 2019)</a> uses trajectory anchors, used in one-step object detection,
 extracted from the training data for ego vehicle prediction. <a class="citation" href="#hong2019rules">(Hong et al., 2019)</a> use a high level
 representation which includes some dynamic context.
In contrast, we choose to leverage also low level sensor data, here Lidar
 point clouds and camera image. In that domain, recent approaches address
 the variation in agent behaviors by predicting multiple trajectories, often
 in a stochastic way. Many works, e.g., PRECOG <a class="citation" href="#rhinehart2019precog">(Rhinehart et al., 2019)</a>,
 MFP <a class="citation" href="#tang2019mfp">(Tang &amp; Salakhutdinov, 2019)</a>, SocialGAN <a class="citation" href="#gupta2018socialgan">(Gupta et al., 2018)</a> 
and others <a class="citation" href="#rhinehart2018deepim">(Rhinehart et al., 2018)</a>,
 focus on this aspect through a probabilistic framework on the network output
 or latent representations, producing multiple trajectories for ego vehicle, nearby vehicles 
or both. <a class="citation" href="#phan2020covernet">(Phan-Minh et al., 2020)</a> generate a trajectory set, then classify 
correct trajectories.
 <a class="citation" href="#marchetti2020mantra">(Marchetti et al., 2020)</a> generate multiple futures from encodings of similar 
trajectories stored
 in a memory. <a class="citation" href="#ohn2020learning">(Ohn-Bar et al., 2020)</a> learn a weighted mixture of expert policies trained to mimic
 agents with specific behaviors. In PRECOG, <a class="citation" href="#rhinehart2019precog">(Rhinehart et al., 2019)</a> advance a 
probabilistic 
formulation that explicitly models interactions between agents, using latent variables
 to model their plausible reactions, with the possibility to precondition the trajectory 
of the ego vehicle by a goal.</p>

<h2 id="plop-method">PLOP method</h2>

<h3 id="contributions">Contributions</h3>

<p>Our main goal is to produce a trajectory prediction which can be used to drive the 
ego vehicle relying on a conditional imitation learning algorithm, conditioned by 
a navigation command for the ego vehicle (e.g., “turn right”).
To do so, we propose  a single-shot, anchor-less trajectory prediction method,
 based on Mixture Desity Networks (MDNs) and polynomial trajectory constraints, 
relying only on on-board sensors which relaxes the HD map requirement and allow 
more flexibility for driving in the real world.
The polynomial formulation ensures that the predicted
 trajectories are coherent and smooth, while providing more
 learning flexibility through the extra parameters. We find
 that this mitigates training instability and mode collapse
 that are common to MDNs <a class="citation" href="#cui2019multimodal">(Cui et al., 2019)</a>.
PLOP is trainable end-to-end from imitation learning,
 where data is relatively easier to obtain and it is 
computationally efficient during both training and inference as it predicts
 trajectory coefficients in a single step, without requiring a RNN-based decoder. 
The polynomial function trajectory coefficients eschew the need for anchors <a class="citation" href="#chai2019multipath">(Chai et al., 2019)</a>,
 whose quality can vary across datasets.</p>

<p>We propose an extensive evaluation of PLOP and show its effectiveness across datasets 
and settings. We conduct a comparison showing the improvement over state-of-the-art 
PRECOG <a class="citation" href="#rhinehart2019precog">(Rhinehart et al., 2019)</a> on the public dataset nuScenes <a class="citation" href="#holger2020nuscenes">(Caesar et al., 2020)</a>;</p>

<p>Then for a better evaluation of the driving capacities of PLOP, 
we study closed loop performance for the ego vehicle, on simulation
 and with preliminary insights for real vehicle control.</p>

<h3 id="network-architecture">Network architecture</h3>

<p>PLOP takes as inputs: the ego and neighbor vehicles past positions
 represented as time sequences of x and y over the last 2 seconds,
 the frontal camera image of the ego vehicle, and 2 second history 
of bird’s eye views with a cell resolution of 1m square containing 
the lidar point cloud and the object detections information represented 
in Figure 2. The objects detections being the output of a state of the art
 perception algorithm.</p>

<p><img src="/blog/images/posts/plop/inputs_plop_post.PNG" alt="plop_inputs" height="100%" width="100%" /></p>
<div class="caption"><b>Figure 2. Image and Bird's eye view.</b> The left image is an example of a front camera input image of PLOP and the diagram on the right is a representation of the bird'eye view input.</div>

<p>We pass these inputs through a multibranch neural network represented 
in Figure 3 to predict the ego vehicle future trajectory and two auxiliary 
tasks that are the future trajectory prediction for the neighbors vehicles 
and the semantic segmentation of the camera image.</p>

<p><img src="/blog/images/posts/plop/archi_outputs.png" alt="plop_archi_outputs" height="100%" width="100%" /></p>
<div class="caption"><b>Figure 3. PLOP's Architecture.</b> PLOP's architecture is reprented on the left while the polynomial multimodal gaussian trajectory representation is on the right</div>

<p>The front camera image features, the bird’s eye view features and the ego vehicle 
past positions features are passed down to conditional fully connected architecture
 to output multiple future trajectories for the ego vehicle regarding the 
 current navigation order. The trajectories are predicted using MDNs where
 gaussian means are generated using polynomial functions of degree 4 over x and y</p>

<p>To improve the learning stability of our training and inject awareness about
the scene layouts into the camera features 
we pass them through a U-Net decoder to output semantic segmentation and then use 
an auxiliary cross entropy loss.
To improve the encoding of interactions between the differents agents of the scene 
in the bird’s eye features,
 we predict the future possible trajectories for each neighbor feeding the bird’s
 eye views encoding and its past positions encoded through a LSTM layer to a small
  fully connected network. The weights of LSTMs and fully connected layers are shared between
   all neighbors. This output allows us to get useful information about the ego
    vehicle environment that can be used online to improve the ego vehicle driving
     with safety collision checks for example.</p>

<h2 id="offline-evaluation">Offline evaluation</h2>

<p>To evaluate PLOP, we use the nuScenes dataset to train the trajectory loss 
along with the Audi <a class="citation" href="#geyer2019a2d2">(Geyer et al., 2019)</a> dataset to train the semantic segmentation loss. 
We choose to compare our method with the DESIRE <a class="citation" href="#lee2017desire">(Lee et al., 2017)</a> baseline and against 
two state of the art methods that are PRECOG and ESP <a class="citation" href="#rhinehart2019precog">(Rhinehart et al., 2019)</a>
 using the minimum
 Mean Squared Deviation metric to avoid penalizing valid trajectories that
 are not matching the ground truth. For one agent, meaning ego vehicle only, 
 PRECOG and ESP have access to the future desired target position and PRECOG 
 return significantly better results than PLOP but PLOP still reaches similar 
 results as ESP. For multiple agents PLOP outperforms other presented methods
 . We note that the comparison if fairer for neighbor trajectories and the
  performance is relevant since they are by definition open loop.</p>

<p><img src="/blog/images/posts/plop/offline_results.PNG" alt="plop_offline_results" height="60%" width="60%" /></p>
<div class="caption"><b>Figure 4. Comparison with state-of-the-art:</b> Against the DESIRE, ESP and PRECOG
 for predicting a trajectory of 4 seconds into the future</div>

<p>But we argue that such evaluation is not totally relevant for controling the ego 
vehicle in real conditions. 
Such metrics does not value the situations in which the errors are made, failing to brake
at a traffic light is a critical error for example but it is 
quick and represent a very small part of the test set so it will impact very poorly the 
overall metrics. However, making a small constant error such as driving 2kph too slow over 
the whole test set set might be an acceptable and non impacting error but will lead to a 
considerable overall error. Also, using only offline metrics where the method can’t
control the vehicle does not allow us to evaluate its capacities to react to its own
mistakes.</p>

<h2 id="online-evaluation-through-simulation">Online Evaluation through simulation</h2>

<p>To simulate driving, we developped a data driven simulator that 
allows us to use real driving data to simulate applying
the prediction to the ego vehicle. We can generate the input data that corresponds
to the new vehicle position after following the trajectory using reprojections (for 
the image and the pointcloud), then use it to predict 
a new trajectory, and so on. This allows us the measure the performance in closed
 loop, and in particular to count failures which would have resulted in a takeover. 
 We rely on 3 metrics: lateral (&gt;1m from expert), high speed (catching up to a vehicle
 15% faster than the real vehicle up to 0.6s in the future) and low speed (&gt; 20kph
 under the expert speed) errors count.</p>

<p><img src="/blog/images/posts/plop/simu_plop.png" alt="plop_online_results" height="100%" width="100%" /></p>
<div class="caption"><b>Figure 5. Evaluation using the simulator.</b> Comparison with PLOP 
without semantic segmentation loss, Constant velocity baseline and Multi-Layer
 Perceptron baseline in the table on the left. Additionnal qualitative
 results about the errors positioning on the differents test tracks are on
 the right.</div>

<p>We trained PLOP on an internal dataset combining both open road and urban test
track and compared PLOP, PLOP without auxiliary semantic loss, the constant velocity 
baseline and a MLP baseline in our simulator using test data. We note that semantic
segmentation improve the driving performance and that MLP has better offline metrics
than constant velocity approach but still perform worse due to the simulated driving
conditions.
As expected, offline metrics are not discriminating enough for the online behavior
since the best model checkpoints in simulation are not necessarily the ones with 
the better offline metrics. An additionnal ablation study where we remove mandatory 
information (such as the camera image input) shows that it may even be dangerous 
to trust them blindly.</p>

<div class="publication-teaser">
<iframe width="560" height="315" src="https://www.youtube.com/embed/94FwahFmc5A?start=94" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div>

<h2 id="conclusion">Conclusion</h2>

<p>In this work, we demonstrate the interest of our multi-input multimodal 
approach PLOP for vehicle trajectory prediction in an urban environment. 
Our architecture leverages frontal camera and Lidar inputs, to produce multiple 
trajectories using reparameterized Mixture Density Networks, with an auxiliary 
semantic segmentation task. We show that we can improve open loop state-of-the-art 
performance in a multi-agent system, by evaluating the vehicle trajectories from the 
nuScenes dataset. We also provide a simulated closed loop evaluation, to go towards
 real vehicle online application. Please check out our paper along with supplementary materials 
for greater details about our approach and experiments and feel free to contact us for any
question.</p>

<h2 id="references">References</h2>

<ol class="bibliography"><li><span id="holger2020nuscenes">Caesar, H., Bankiti, V., Lang, A. H., Vora, S., Liong, V. E., Xu, Q., Krishnan, A., Pan, Y., Baldan, G., &amp; Beijbom, O. (2020). nuScenes: A Multimodal Dataset for Autonomous Driving. <i>Cvpr</i>.</span></li>
<li><span id="bansal2018chauffeutnet">Bansal, M., Krizhevsky, A., &amp; Ogale, A. S. (2018). ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst. <i>CoRR</i>.</span></li>
<li><span id="chai2019multipath">Chai, Y., Sapp, B., Bansal, M., &amp; Anguelov, D. (2019). <i>MultiPath: Multiple Probabilistic Anchor Trajectory Hypotheses for Behavior Prediction</i>.</span></li>
<li><span id="hong2019rules">Hong, J., Sapp, B., &amp; Philbin, J. (2019). Rules of the Road: Predicting Driving Behavior with a Convolutional Model of Semantic Interactions. <i>CoRR</i>.</span></li>
<li><span id="rhinehart2019precog">Rhinehart, N., McAllister, R., Kitani, K., &amp; Levine, S. (2019). Precog: Prediction conditioned on goals in visual multi-agent settings. <i>Iccv</i>.</span></li>
<li><span id="tang2019mfp">Tang, C., &amp; Salakhutdinov, R. R. (2019). Multiple futures prediction. <i>Advances in Neural Information Processing Systems</i>, 15424–15434.</span></li>
<li><span id="gupta2018socialgan">Gupta, A., Johnson, J., Fei-Fei, L., Savarese, S., &amp; Alahi, A. (2018). Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks. <i>CoRR</i>.</span></li>
<li><span id="rhinehart2018deepim">Rhinehart, N., McAllister, R., &amp; Levine, S. (2018). Deep Imitative Models for Flexible Inference, Planning, and Control. <i>CoRR</i>.</span></li>
<li><span id="phan2020covernet">Phan-Minh, T., Grigore, E. C., Boulton, F. A., Beijbom, O., &amp; Wolff, E. M. (2020). Covernet: Multimodal behavior prediction using trajectory sets. <i>Cvpr</i>.</span></li>
<li><span id="marchetti2020mantra">Marchetti, F., Becattini, F., Seidenari, L., &amp; Bimbo, A. D. (2020). Mantra: Memory augmented networks for multiple trajectory prediction. <i>Cvpr</i>.</span></li>
<li><span id="ohn2020learning">Ohn-Bar, E., Prakash, A., Behl, A., Chitta, K., &amp; Geiger, A. (2020). Learning Situational Driving. <i>Cvpr</i>.</span></li>
<li><span id="cui2019multimodal">Cui, H., Radosavljevic, V., Chou, F.-C., Lin, T.-H., Nguyen, T., Huang, T.-K., Schneider, J., &amp; Djuric, N. (2019). Multimodal trajectory predictions for autonomous driving using deep convolutional networks. <i>Icra</i>.</span></li>
<li><span id="geyer2019a2d2">Geyer, J., Kassahun, Y., Mahmudi, M., Ricou, X., Durgesh, R., Chung, A. S., Hauswald, L., Pham, V. H., Mühlegg, M., Dorn, S., Fernandez, T., Jänicke, M., Mirashi, S., Savani, C., Sturm, M., Vorobiov, O., &amp; Schuberth, P. (2019). <i>A2D2: AEV Autonomous Driving Dataset</i>. http://www.a2d2.audi</span></li>
<li><span id="lee2017desire">Lee, N., Choi, W., Vernaza, P., Choy, C., Torr, P., &amp; Chandraker, M. (2017). <i>DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents</i>. 2165–2174. https://doi.org/10.1109/CVPR.2017.233</span></li></ol>]]></content><author><name></name></author><category term="driving" /><category term="multi-sensor" /><summary type="html"><![CDATA[Thibault Buhet, Emilie Wirbel, Andrei Bursuc and Xavier Perrotton]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io/blog/images/publications/plop/plop.png" /><media:content medium="image" url="https://valeoai.github.io/blog/images/publications/plop/plop.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Is Deep Reinforcement Learning Really Superhuman on Atari?</title><link href="https://valeoai.github.io/blog/2020/10/19/saber.html" rel="alternate" type="text/html" title="Is Deep Reinforcement Learning Really Superhuman on Atari?" /><published>2020-10-19T00:00:00-05:00</published><updated>2020-10-19T00:00:00-05:00</updated><id>https://valeoai.github.io/blog/2020/10/19/saber</id><content type="html" xml:base="https://valeoai.github.io/blog/2020/10/19/saber.html"><![CDATA[<p><em>This post describes our recent <a href="https://arxiv.org/pdf/1908.04683.pdf">work</a> on Deep Reinforcement Learning (DRL) on the Atari benchmark. DRL appears today as one of the closest paradigm to Artificial General Intelligence and large progress in this field has been enabled by the popular Atari benchmark. However, training and evaluation protocols on Atari vary across papers leading to biased comparisons, difficulty in reproducing results and in estimating the true contributions of the methods. Here we attempt to mitigate this problem with SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms. SABER allows us to compare multiple methods under the same conditions against a human baseline and to note that previous claims of superhuman performance on DRL do not hold. Finally, we propose a new state-of-the-art algorithm R-IQN combining Rainbow with Implicit Quantile Networks (IQN). Our code is <a href="https://github.com/valeoai/rainbow-iqn-apex">available</a>.</em></p>

<p>Deep Reinforcement Learning is a learning scheme based on trial-and-error in which an agent learns an optimal policy from its own experiments and a reward signal. The goal of the agent is to maximize the sum of future accumulated rewards and thus the agent needs to think about sequences of actions rather than instantaneous ones. The Atari benchmark is valuable for evaluating general AI algorithms as it includes more than 50 games displaying high variability in the task to solve, ranging from simple paddle control in the ball game Pong to complex labyrinth exploration in Montezuma’s Revenge which remains unsolved by general algorithms up to today.</p>

<p>We notice however that training and evaluation procedures on Atari can be different from paper to paper and thus leading to bias in comparison. Moreover this leads to difficulties to reproduce results of published works as some training or evaluation parameters are barely explained or sometimes not mentioned. In order to facilitate reproducible and comparable DRL, we introduce SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms. Furthermore, we introduce a human world record baseline and argue that previous claims of superhuman performance of DRL might not hold. Finally, we propose a new state-of-the-art algorithm R-IQN by combining the current state-of-the-art <strong>Rainbow</strong> <a class="citation" href="#hessel2017rainbow">(Hessel et al., 2018)</a> along with Implicit Quantile Networks (<strong>IQN</strong> <a class="citation" href="#dabney2018implicit">(Dabney et al., 2018)</a>). We release an open-source <a href="https://github.com/valeoai/rainbow-iqn-apex">implementation</a> of distributed R-IQN following the ideas from Ape-X!</p>

<h2 id="dqns-human-baseline-vs-human-world-record-on-atari-games">DQN’s human baseline vs human world record on Atari Games</h2>

<p>A common way to evaluate AI for games is to let agents compete against the best humans. Recent examples for DRL include the victory of <a href="https://deepmind.com/alphago-korea">AlphaGo versus Lee Sedol</a> for Go, <a href="https://openai.com/blog/openai-five/">OpenAI Five</a> on Dota 2 or <a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii">AlphaStar versus Mana</a> for StarCraft 2. For this reason one of the most used metrics for evaluating RL agents on Atari is to compare them to the human baseline introduced in DQN.</p>

<p>Previous works use the normalized human score, <em>i.e</em>., 0% is the score of a random player and 100% is the score of the human baseline, which allows one to summarize the performance on the whole Atari set in one number, instead of individually comparing raw scores for each of the 61 games. However we argue that this human baseline is far from being representative of the best human player, which means that using it to claim superhuman performance is misleading.</p>

<p>The current world records are available online for 58 of the 61 evaluated Atari games. For example, on VideoPinball, the world record is 50,000 times higher than the human baseline of DQN. Evaluating these world records scores using the usual human normalized score has a median of 4,400% and a mean of 99,300% (see Figure below for details on each game), to be compared to 200% and 800% of the current state-of-the-art Rainbow!</p>

<p><img src="/blog/images/posts/saber/pro_vs_beginner.png" alt="pro_vs_beginner" height="60%" width="60%" /></p>
<div class="caption"><b>Figure 1: World record scores vs. the usual beginner human baseline</b> (log scale) <a class="citation" href="#dabney2018implicit">(Dabney et al., 2018)</a> baseline can be up to 50k times lower than registered world records. Beating that baseline does not necessarily make the agent superhuman.</div>

<p>We estimate that evaluating the algorithms under the world record baseline instead of the DQN human baseline will give a better view of the gap remaining between best human players and DRL agents.
Results are confirming this, as Rainbow reaches only a median human-normalized score of 3% (see Figure 2 below) meaning that for half of Atari games, the agent doesn’t even reach 3% of the way from random to best human run.</p>

<p>In the video below we analyze agents previously claimed as above human-level but far from the world record. By taking a closer look at the AI playing, we discovered that on most of Atari games DRL agents fail to understand the goal of the game. Sometimes they just don’t explore other levels than the initial one, sometimes they are stuck in a loop giving small amount of reward, etc. A compelling example of this is the game Riverraid. In this game, the agent must shoot everything and take fuel to survive: the agent dies if there is a collision with an enemy or if out of fuel. But as shooting fuel actually gives points, the agent doesn’t understand that he could play way longer and win even more points by actually taking this fuel bonus and not shooting them!</p>

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/oH6P3ksYLek" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" align="center"></iframe>
</p>

<h2 id="saber-a-standardized-atari-benchmark-for-general-reinforcement-learning-algorithms">SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms</h2>

<p>In our work we extend the recommendations proposed by <a class="citation" href="#machado2018revisiting">(Machado et al., 2018)</a> They also point out divergences in training and evaluating agents on Atari. Consequently they compile and propose a set of recommendations for more reproducible and comparable RL including sticky actions, ignoring life signal and using full action set.</p>

<p>There is however one major parameter that is left out in <a class="citation" href="#machado2018revisiting">(Machado et al., 2018)</a>: the maximum number of frames allowed per episode. This parameter ends the episode after a fixed number of time steps even if the game is not over. In most of the recent works, this is set to 30 min of game play (i.e., 108k frames) and only to 5 min in some others (i.e.,18k frames). This means that the reported scores can not be compared fairly. For example, in easy games (e.g., Atlantis), the agent never dies and the score is more or less linear with the allowed time: the reported score will be 6 times higher if capped at 30 minutes instead of 5 minutes.</p>

<p>Another issue with this time cap comes from the fact that some games are designed for much longer gameplay than 5 or 30 minutes. On those games (e.g., Atlantis, Video Pinball, Enduro) the scores reported of Ape-X, Rainbow and IQN are almost the same. This is due to all agents reaching the time limit and getting the maximum possible score in 30 minutes: the difference in scores is due to minor variations, not algorithmic difference and thus the comparison is not significant. As a consequence, the more successful agents are, the more games are incomparable because they reach the maximum possible score in the time cap while still being far behind human world record.</p>

<p>This parameter can also be a source of ambiguity and error. The best score on Atlantis (2,311,815) is reported by Proximal Policy Optimization by <a class="citation" href="#schulman2017proximal">(Schulman et al., 2017)</a>, however this score is likely wrong: it seems impossible to reach in 30 minutes only! The first distributional paper by (<a href="http://proceedings.mlr.press/v70/bellemare17a.html">Bellemare et al.</a>) also did this mistake and reported wrong results before adding an erratum in a later version on <em>ArXiv</em> <a class="citation" href="#bellemare2017distributional">(Bellemare et al., 2017)</a>.</p>

<p><img src="/blog/images/posts/saber/saber_params.png" alt="saber_params" height="60%" width="60%" /></p>
<div class="caption"><b>Table 1:</b> Game parameters of SABER.</div>

<p>We first re-implemented the current state-of-the-art Rainbow and evaluated it on SABER. We noticed that the same algorithm under different evaluation settings can lead to significantly different results. This showed again the necessity of a common and standardized benchmark, more details can be found in the paper.</p>

<p>Then we implemented a new algorithm, R-IQN, by replacing the C51 algorithm (which is one of the 6 components of Rainbow) by Implicit Quantile Network (IQN). Both C51 and IQN belong to the field of Distributional RL which aims to predict the full distribution of the Q-value function instead of just predicting the mean of it. The fundamental difference between these two algorithms is how they parametrize the Q-value distribution. C51, which is the first algorithm of Distributional RL, approximates the Q-value as a categorical distribution with fixed support and just learns the mass to attribute to each category. On the other hand, IQN approximates the Q-value with quantile regression and both the support and the mass arelearned resulting in a major improvement in performance and data-efficiency over C51. As IQN arises much better performance than C51 while still designed for the same goal (predict the full distribution of the Q-function), combining Rainbow with IQN is relevant and natural.</p>

<p>As shown in the graph below, R-IQN outperforms Rainbow and thus becomes the new state-of-the-art on Atari. However, we acknowledge that in order to make a more confident state-of-the-art claim we should run multiple times with different seeds. Testing an increased number of random seeds across the 60 Atari games is a computationally costly endeavor and is beyond the scope of this study. We test the stability of the performances of R-IQN across 5 random seeds on a subset of 14 games. We compare against Rainbow and report similar results in Figure 3.</p>

<p><img src="/blog/images/posts/saber/median_human_normalised_Rainbow_vs_Rainbow_IQN.png" alt="median_rainbow_vs_us" height="90%" width="90%" /></p>
<div class="caption"><b>Figure 2: Comparison of Rainbow and Rainbow-IQN on SABER.</b> We report median normalized scores w.r.t training steps.</div>

<p><img src="/blog/images/posts/saber/median_human_normalised_5_seeds_both_rainbow_riqn.png" alt="median_rainbow_vs_us_seeds" height="90%" width="90%" /></p>
<div class="caption"><b>Figure 3: Comparison of Rainbow and Rainbow-IQN on a subset of 14 games using 5 seeds.</b> We report median normalized scores w.r.t training steps.</div>

<h2 id="open-source-implementation-of-distributed-rainbow-iqn-r-iqn-ape-x">Open-source implementation of distributed Rainbow-IQN: R-IQN Ape-X</h2>

<p>We release our code of a distributed version of Rainbow-IQN following ideas from Ape-X <a class="citation" href="#horgan2018distributed">(Horgan et al., 2018)</a>. The distributed part is our main practical advantage over some existing DRL repositories (particularly <a href="https://github.com/google/dopamine">Dopamine</a> a popular open-source implementation of DQN, C51, IQN and a <em>small-Rainbow</em> but in which all algorithms are single worker).
Indeed, when using DRL algorithms for other tasks (other than Atari and MuJoCO) a major bottleneck is the <em>speed</em> of the environment. DRL algorithms often need a huge amount of data before reaching reasonable performance. This amount may be practically impossible to reach if the environment is real-time and if collecting data from multiple environments at the same time is not possible.</p>

<p>Building upon ideas from Ape-X, we use REDIS as a side server to store our replay memory. Multiple actors will act in their own instances of the environment to fill as fast as they can the replay memory. Finally, the learner will sample from this replay memory (the learner is actually completely independent of the environment) for backprop. The learner will also periodically update the weight of each actor as shown in the schema below.</p>

<p><img src="/blog/images/posts/saber/appex_arch.png" alt="apex_arch" height="60%" width="60%" /></p>
<div class="caption"><b>Figure 4: Ape-X architecture.</b> image taken from <a class="citation" href="#horgan2018distributed">(Horgan et al., 2018)</a></div>

<p>This scheme allowed us to use R-IQN Ape-X for the task of autonomous driving using <a href="http://carla.org/">CARLA</a> as environment. This enabled us to win the <a href="https://carlachallenge.org/results-challenge-2019/">CARLA Challenge</a> on Track 2 <em>Cameras Only</em> showing the strength of R-IQN Ape-X as a general algorithm.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this work, we confirm the impact of standardized guidelines for DRL evaluation, and build a consolidated benchmark, SABER. In order to provide a more significant comparison, we build a new baseline based on human world records and show that the state-of-the-art Rainbow agent is in fact far from human world record performance. In the paper we share possible reasons for this failure. We hope that SABER will facilitate better comparisons and enable new exciting methods to prove their effectiveness without ambiguity.</p>

<p>Check out our paper to find out more about intuitions, experiments and interpretations.</p>

<h2 id="references">References</h2>

<ol class="bibliography"><li><span id="hessel2017rainbow">Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., &amp; Silver, D. (2018). Rainbow: Combining improvements in deep reinforcement learning. <i>AAAI</i>.</span></li>
<li><span id="dabney2018implicit">Dabney, W., Ostrovski, G., Silver, D., &amp; Munos, R. (2018). Implicit quantile networks for distributional reinforcement learning. <i>ICML</i>.</span></li>
<li><span id="machado2018revisiting">Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., &amp; Bowling, M. (2018). Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. <i>Journal of Artificial Intelligence Research</i>, <i>61</i>, 523–562.</span></li>
<li><span id="schulman2017proximal">Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017). Proximal policy optimization algorithms. <i>ArXiv Preprint ArXiv:1707.06347</i>.</span></li>
<li><span id="bellemare2017distributional">Bellemare, M. G., Dabney, W., &amp; Munos, R. (2017). A distributional perspective on reinforcement learning. <i>ArXiv Preprint ArXiv:1707.06887</i>.</span></li>
<li><span id="horgan2018distributed">Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., Van Hasselt, H., &amp; Silver, D. (2018). Distributed prioritized experience replay. <i>ArXiv Preprint ArXiv:1803.00933</i>.</span></li></ol>]]></content><author><name></name></author><category term="deep reinforcement learning" /><summary type="html"><![CDATA[Marin Toromanoff, Emilie Wirbel]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io/blog/images/posts/saber/space_invaders.jpg" /><media:content medium="image" url="https://valeoai.github.io/blog/images/posts/saber/space_invaders.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">ADVENT - Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation</title><link href="https://valeoai.github.io/blog/2020/07/07/advent-domain-adaptation.html" rel="alternate" type="text/html" title="ADVENT - Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation" /><published>2020-07-07T00:00:00-05:00</published><updated>2020-07-07T00:00:00-05:00</updated><id>https://valeoai.github.io/blog/2020/07/07/advent-domain-adaptation</id><content type="html" xml:base="https://valeoai.github.io/blog/2020/07/07/advent-domain-adaptation.html"><![CDATA[<p><em>This post describes our <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Vu_ADVENT_Adversarial_Entropy_Minimization_for_Domain_Adaptation_in_Semantic_Segmentation_CVPR_2019_paper.html">recent work</a> on unsupervised domain adaptation for semantic segmentation presented at <a href="http://cvpr2019.thecvf.com/">CVPR 2019</a>. ADVENT is a flexible technique for bridging the gap between two different domains through entropy minimization. Our work builds upon a simple observation: models trained only on source domain tend to produce over-confident, i.e., low-entropy, predictions on source-like images and under-confident, i.e., high-entropy, predictions on target-like ones. Consequently by minimizing the entropy on the target domain, we make the feature distributions from the two domains more similar. We show that our approach achieves competitive performances on standard semantic segmentation benchmarks and that it can be successfully extended to other tasks such as object detection.</em></p>

<p>Visual perception is a remarkable ability that human drivers leverage for understanding their surroundings and for supporting the multiple micro-decisions needed in traffic. Since many years, researchers have been working on mimicking this human capability by means of computer algorithms. This research field is known as computer vision and it has seen impressive progress and wide adoption. Most of the modern <em>computer vision</em> systems rely on Deep Neural Networks (DNNs) which are powerful and widely employed tools able to learn from large amounts of data and make accurate predictions. In autonomous driving, DNN-based visual perception is also at the heart of the complex architectures under intelligent cars, and supports downstream decisions of the vehicle, <em>e.g.,</em> steering, braking, signaling, etc.</p>

<p>The diversity and complexity of the situations encountered in real-world driving is tremendous. Unlike humans who can extrapolate effortlessly from previous experience in order to adapt to new environments and conditions, the scope of DNNs beyond the types of conditions and scenes seen during training is limited. For instance a model trained on data from a sunny country, would have a hard time delivering the same performance on streets with mixed weather conditions in a different country (with different urban architecture, furniture, vegetation, types of cars and pedestrian appearance and clothing). Similarly a model trained on a particular type of camera, is expected to see a drop in performance with images coming from a camera with different specifications. This difference between environments that leads to performance drops is referred to as <em>domain gap</em>.</p>

<h2 id="bridging-domains">Bridging domains</h2>

<p>We can resort to two options for narrowing the domain gap: (i) annotate more data; (ii) leverage the experience acquired on an initial environment and transfer it to the new environment. More annotated data has been shown to always improve performance of DNNs <a class="citation" href="#sun2017revisiting">(Sun et al., 2017)</a>. However the labeling process brings a significant financial and temporal burden. The time required for a high-quality annotation, such as the ones from the popular Cityscapes dataset is ∼90 minutes per image <a class="citation" href="#cordts2016cityscapes">(Cordts et al., 2016)</a>. The amount of images required to train high performance DNNs typically counts in hundreds of thousands. The acquisition of diverse data across seasons and weather conditions adds up even more time. It makes then sense to look for a solution elsewhere and the second option seems now more appealing, though achieving it remains technically challenging. This is actually the area of research of domain adaptation (DA) which addresses the domain-gap problem by transferring knowledge from a source domain (with full annotations) to a target domain (with fewer annotations if any), aiming to reach good performances on target samples. DA has consistently attracted interest from different communities across years <a class="citation" href="#csurka2017domain">(Csurka, 2017)</a>.</p>

<p>Here we are working on <em>Unsupervised DA</em> (UDA), which is a more challenging task where we have access to labeled source samples and only unlabeled target samples. We use as source, data generated by a simulator or video game engine, while for target we consider real-data from car-mounted cameras. In <em>Figure 1</em> we illustrate the difficulty of this task and the impact of our UDA technique, ADVENT.</p>

<p><img src="/blog/images/posts/advent/advent_teaser.png" alt="advent_teaser" height="60%" width="60%" /></p>
<div class="caption"><b>Figure 1. Proposed entropy-based unsupervised domain adaptation for semantic segmentation.</b> The top two rows show results on source and target domain scenes of the model trained without adaptation. The bottom row shows the result on the same target domain scene of the model trained with entropy-based adaptation. The left and right columns visualize respectively the semantic segmentation outputs and the corresponding prediction entropy maps.</div>

<p>The main approaches for UDA include discrepancy minimization between source and target feature distributions usually achieved via adversarial training  <a class="citation" href="#ganin2015unsupervised">(Ganin &amp; Lempitsky, 2015)</a>, <a class="citation" href="#tzeng2017adversarial">(Tzeng et al., 2017)</a>, self-training with pseudo-labels <a class="citation" href="#zou2018unsupervised">(Zou et al., 2018)</a> and generative approaches <a class="citation" href="#hoffman2018cycada">(Hoffman et al., 2018)</a>, <a class="citation" href="#wu2018dcan">(Wu et al., 2018)</a>.</p>

<p><em>Entropy minimization</em> has been shown to be useful for semi-supervised learning <a class="citation" href="#grandvalet2005semi">(Grandvalet &amp; Bengio, 2005)</a>, clustering <a class="citation" href="#jain2018learning">(Jain et al., 2018)</a> and more recently to domain adaptation for classification <a class="citation" href="#long2016unsupervised">(Long et al., 2016)</a>. We chose to explore entropy based UDA training to obtain competitive performance on semantic segmentation.</p>

<h2 id="approach">Approach</h2>

<p>We present our two proposed approaches for entropy minimization using (i) an unsupervised entropy loss and (ii) adversarial training. To build our models, we start from existing semantic segmentation frameworks and add an additional network branch used for domain adaptation. <em>Figure 2</em> illustrates our architectures.</p>

<p><img src="/blog/images/posts/advent/advent_approach.jpg" alt="" height="100%" width="100%" /></p>

<div class="caption"><b>Figure 2. Approach overview.</b> First, direct entropy minimization decreases the entropy of the target $P_{x_t}$, which is equivalent to minimizing the sum of weighted self-information maps $I_{x_t}$. In the second approach, we use adversarial training to enforce the consistency in $P_x$ across domains. Red arrows are used for target domain, blue arrows for source.
</div>

<h3 id="direct-entropy-minimization">Direct entropy minimization</h3>

<p>On the source domain we train our model, denoted as $F$, as usual using a supervised loss. For the target domain, we do not have annotations and we can no longer use the segmentation loss to train $F$. We notice that models trained only on source domain tend to produce over-confident predictions on source-like images and under-confident predictions on target-like ones. Motivated by this observation, we propose a supervision signal that could leverage visual information from the target samples, in spite of the lack of annotations. The objective is to constrain $F$ to produce high-confident predictions on target samples similarly to source samples. To this effect, we introduce the entropy loss $\mathcal{L}_{ent}$​ to maximize directly the prediction confidence in the target domain. Here we consider the Shannon Entropy (<a href="http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">Shannon</a>). During training, we jointly optimize the supervised segmentation loss $\mathcal{L}_{seg}$ on source samples and the unsupervised entropy loss $\mathcal{L}_{ent}$​​ on target samples.</p>

<h3 id="entropy-minimization-by-adverarial-learning">Entropy minimization by adverarial learning</h3>

<p>A limitation of the entropy loss is related to the absence of structural dependencies between local semantics. This is caused by the aggregation of the pixel-wise prediction entropies by summation. We address this through a unified adversarial training framework which minimizes indirectly the entropy of target data, by encouraging it to become similar to the source one. This allows the exploitation of the structural consistency between the domains. To this end, we formulate the UDA task as minimizing distribution distance between source and target on the <em>weighted self-information</em> space. Since the trained model produces naturally low-entropy predictions on source-like images, by aligning weighted self-information distributions of target and source domains, we reach the same behavior on target-like data.</p>

<p>We perform the adversarial adaptation on weighted self-information maps using a fully-convolutional discriminator network $D$. The discriminator produces domain classification outputs, <em>i.e.,</em> class label $1$ (resp. $0$) for the source (resp. target) domain. We train $D$ to discriminate outputs coming from source and target images, and at the same time, train the segmentation network to fool the discriminator.</p>

<h2 id="experiments">Experiments</h2>

<p>We evaluate our approaches on the challenging <em>synthetic-2-real</em> unsupervised domain adaptation set-ups. Models are trained on fully-annotated synthetic data and are validated on real-world data. In such set-ups, the models have access to some unlabeled real images during training.</p>

<h3 id="semantic-segmentation">Semantic Segmentation</h3>

<p>To train our models, we use either GTA5 <a class="citation" href="#richter2016playing">(Richter et al., 2016)</a> or SYNTHIA <a class="citation" href="#ros2016synthia">(Ros et al., 2016)</a> as source synthetic data, along with the training split of Cityscapes dataset <a class="citation" href="#cordts2016cityscapes">(Cordts et al., 2016)</a> as target domain data.</p>

<p>In <em>Table 1</em> we report our results on semantic segmentation from models trained on GTA5 $\rightarrow$ Cityscapes and from SYNTHIA $\rightarrow$ Cityscapes. We compare here only with the top performing method Adapt-SegMap <a class="citation" href="#tsai2018learning">(Tsai et al., 2018)</a>, while additional baselines and related methods are covered in the paper.</p>

<p><img src="/blog/images/posts/advent/advent_table1.png" alt="" height="60%" width="60%" /></p>
<div class="caption"><b>Table 1. Segmentation performance in mIoU with ResNet-101 based model and Deeplab-V2 as the segmentation network.</b>We show results of our approaches using the direct entropy loss (MinEnt) and using adversarial training (AdvEnt).</div>

<p>Our first approach of direct entropy minimization (<em>MinEnt</em>) achieves comparable performance to state-of-the-art baselines. The light overhead of the entropy loss makes training time shorter for the MinEnt model, while being easier train compared to adversarial networks. Our second approach using adversarial training on the weighted self-information space, noted as <em>AdvEnt</em>, shows consistent improvement to the baselines. In general, AdvEnt works better than MinEnt, confirming the importance of structural adaptation. The two approaches are complementary as their combination boosts performance further.</p>

<p>In <em>Figure 3</em>, we illustrate a few qualitative results of our models. Without domain adaptation, the model trained only on source supervision produces noisy segmentation predictions as well as high entropy activations, with a few exceptions on some classes like <em>“building”</em> and <em>“car”</em>. However, there are many confident predictions (low entropy) which are completely wrong. Our models, on the other hand, manage to produce correct predictions at high level of confidence.</p>

<p><img src="/blog/images/posts/advent/advent_qualitative.png" alt="" height="100%" width="100%" /></p>
<div class="caption"><b>Figure 3. Segmentation and detection qualitative results.</b> Segmentation on Cityscapes validation set with ResNet-101 + DeepLab-V2; Detection on Cityscapes-foggy with VGG-16 as the backbone and SSD.</div>

<h3 id="uda-for-object-detection">UDA for object detection</h3>

<p>The proposed entropy-based approaches are not limited to semantic segmentation and can be applied to UDA for other recognition tasks, e.g. object detection. We conducted experiments in the UDA object detection set-up Cityscapes $\rightarrow$ Cityscapes-Foggy, similar to the one in <a class="citation" href="#chen2018domain">(Chen et al., 2018)</a>. We report quantitative results in <em>Table 2</em> and qualitative ones in <em>Figure 3</em>. In spite of the unfavorable factors, our improvement over the baseline ($+11.5\%$ mAP using AdvEnt) is larger than the one reported in <a class="citation" href="#chen2018domain">(Chen et al., 2018)</a> ($+8.8\%$). Additional experiments and implementation details can be found in the paper. These encouraging preliminary results suggest the feasibility of applying entropy-based approached on UDA for detection.</p>

<p><img src="/blog/images/posts/advent/advent_table2.png" alt="" height="60%" width="60%" /></p>
<div class="caption"><b>Table 2. Object detection performance on Cityscapes Foggy.</b></div>

<h2 id="conclusion">Conclusion</h2>

<p>In this work, we propose two approaches for unsupervised domain adaptation reaching state-of-the-art performances on standard synthetic-2-real benchmarks. Interestingly the method can be easily extended to UDA for object detection with promising preliminary results.
Check out our <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Vu_ADVENT_Adversarial_Entropy_Minimization_for_Domain_Adaptation_in_Semantic_Segmentation_CVPR_2019_paper.html">paper</a> to find out more about intuitions, experiments and implementation details for AdvEnt and try out our <a href="https://github.com/valeoai/ADVENT">code</a>.</p>

<h2 id="references">References</h2>

<ol class="bibliography"><li><span id="sun2017revisiting">Sun, C., Shrivastava, A., Singh, S., &amp; Gupta, A. (2017). Revisiting unreasonable effectiveness of data in deep learning era. <i>Proceedings of the IEEE International Conference on Computer Vision</i>, 843–852.</span></li>
<li><span id="cordts2016cityscapes">Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., &amp; Schiele, B. (2016). The cityscapes dataset for semantic urban scene understanding. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 3213–3223.</span></li>
<li><span id="csurka2017domain">Csurka, G. (2017). <i>Domain adaptation in computer vision applications</i>. <i>8</i>.</span></li>
<li><span id="ganin2015unsupervised">Ganin, Y., &amp; Lempitsky, V. (2015). Unsupervised domain adaptation by backpropagation. <i>International Conference on Machine Learning</i>, 1180–1189.</span></li>
<li><span id="tzeng2017adversarial">Tzeng, E., Hoffman, J., Saenko, K., &amp; Darrell, T. (2017). Adversarial discriminative domain adaptation. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 7167–7176.</span></li>
<li><span id="zou2018unsupervised">Zou, Y., Yu, Z., Vijaya Kumar, B. V. K., &amp; Wang, J. (2018). Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. <i>Proceedings of the European Conference on Computer Vision (ECCV)</i>, 289–305.</span></li>
<li><span id="hoffman2018cycada">Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y., Isola, P., Saenko, K., Efros, A., &amp; Darrell, T. (2018). Cycada: Cycle-consistent adversarial domain adaptation. <i>International Conference on Machine Learning</i>, 1989–1998.</span></li>
<li><span id="wu2018dcan">Wu, Z., Han, X., Lin, Y.-L., Gokhan Uzunbas, M., Goldstein, T., Nam Lim, S., &amp; Davis, L. S. (2018). Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation. <i>Proceedings of the European Conference on Computer Vision (ECCV)</i>, 518–534.</span></li>
<li><span id="grandvalet2005semi">Grandvalet, Y., &amp; Bengio, Y. (2005). Semi-supervised learning by entropy minimization. <i>Advances in Neural Information Processing Systems</i>, 529–536.</span></li>
<li><span id="jain2018learning">Jain, H., Zepeda, J., Pérez, P., &amp; Gribonval, R. (2018). Learning a complete image indexing pipeline. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 4933–4941.</span></li>
<li><span id="long2016unsupervised">Long, M., Zhu, H., Wang, J., &amp; Jordan, M. I. (2016). Unsupervised domain adaptation with residual transfer networks. <i>Advances in Neural Information Processing Systems</i>, 136–144.</span></li>
<li><span id="richter2016playing">Richter, S. R., Vineet, V., Roth, S., &amp; Koltun, V. (2016). Playing for data: Ground truth from computer games. <i>European Conference on Computer Vision</i>, 102–118.</span></li>
<li><span id="ros2016synthia">Ros, G., Sellart, L., Materzynska, J., Vazquez, D., &amp; Lopez, A. M. (2016). The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 3234–3243.</span></li>
<li><span id="tsai2018learning">Tsai, Y.-H., Hung, W.-C., Schulter, S., Sohn, K., Yang, M.-H., &amp; Chandraker, M. (2018). Learning to adapt structured output space for semantic segmentation. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 7472–7481.</span></li>
<li><span id="chen2018domain">Chen, Y., Li, W., Sakaridis, C., Dai, D., &amp; Van Gool, L. (2018). Domain adaptive faster r-cnn for object detection in the wild. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, 3339–3348.</span></li></ol>]]></content><author><name></name></author><category term="domain adaptation" /><category term="semantic segmentation" /><category term="object detection" /><summary type="html"><![CDATA[Tuan-Hung Vu, Andrei Bursuc]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io/blog/images/posts/advent/qualitative_results.png" /><media:content medium="image" url="https://valeoai.github.io/blog/images/posts/advent/qualitative_results.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>