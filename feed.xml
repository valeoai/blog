<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://valeoai.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://valeoai.github.io/blog/" rel="alternate" type="text/html" /><updated>2022-10-20T09:15:28-05:00</updated><id>https://valeoai.github.io/blog/feed.xml</id><title type="html">valeo.ai blog</title><subtitle>valeo.ai research blog</subtitle><entry><title type="html">valeo.ai at CVPR 2022</title><link href="https://valeoai.github.io/blog/2022/06/14/valeoai-at-cvpr-2022.html" rel="alternate" type="text/html" title="valeo.ai at CVPR 2022" /><published>2022-06-14T00:00:00-05:00</published><updated>2022-06-14T00:00:00-05:00</updated><id>https://valeoai.github.io/blog/2022/06/14/valeoai-at-cvpr-2022</id><content type="html" xml:base="https://valeoai.github.io/blog/2022/06/14/valeoai-at-cvpr-2022.html">&lt;p&gt;The &lt;a href=&quot;https://cvpr2022.thecvf.com/&quot;&gt;IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)&lt;/a&gt; is a major event for researchers and engineers working on computer vision and machine learning. At the 2022 edition the &lt;a href=&quot;https://ptrckprz.github.io/valeoai/&quot;&gt;valeo.ai&lt;/a&gt; team will present four &lt;a href=&quot;https://ptrckprz.github.io/vaipub/&quot;&gt;papers&lt;/a&gt; in the main conference, three &lt;a href=&quot;https://ptrckprz.github.io/vaipub/&quot;&gt;papers&lt;/a&gt; in workshops and one workshop &lt;a href=&quot;https://vision4allseason.net/&quot;&gt;keynote&lt;/a&gt;. The team will be at CVPR to present these works and will be happy to discuss more about these projects and ideas, and share our exciting ongoing research.&lt;/p&gt;

&lt;h2 id=&quot;image-to-lidar-self-supervised-distillation-for-autonomous-driving-data&quot;&gt;Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data&lt;/h2&gt;
&lt;h4 id=&quot;authors-corentin-sautier-gilles-puy--spyros-gidaris--alexandre-boulch-andrei-bursuc-renaud-marlet&quot;&gt;Authors: Corentin Sautier, &lt;a href=&quot;https://sites.google.com/site/puygilles/home&quot;&gt;Gilles Puy&lt;/a&gt;,  &lt;a href=&quot;https://scholar.google.fr/citations?user=7atfg7EAAAAJ&amp;amp;hl=en&quot;&gt;Spyros Gidaris&lt;/a&gt;,  &lt;a href=&quot;https://www.boulch.eu/&quot;&gt;Alexandre Boulch&lt;/a&gt;, &lt;a href=&quot;https://abursuc.github.io/&quot;&gt;Andrei Bursuc&lt;/a&gt;, &lt;a href=&quot;http://imagine.enpc.fr/~marletr/&quot;&gt;Renaud Marlet&lt;/a&gt;&lt;/h4&gt;

&lt;h4 align=&quot;center&quot;&gt; [&lt;a href=&quot;https://arxiv.org/abs/2203.16258&quot;&gt;Paper&lt;/a&gt;] &amp;nbsp;&amp;nbsp; [&lt;a href=&quot;https://github.com/valeoai/SLidR&quot;&gt;Code&lt;/a&gt;] &amp;nbsp;&amp;nbsp; [&lt;a href=&quot;https://valeoai.github.io/blog/publications/slidr/&quot;&gt;Project page&lt;/a&gt;]&lt;/h4&gt;

&lt;p&gt;Self-driving vehicles require object detection or segmentation to safely maneuver in their environment. Such safety-critical tasks are usually performed by neural networks demanding huge Lidar datasets with high quality annotations, and no domain shift between training and testing conditions. However, annotating 3D Lidar data for these tasks is tedious and costly. In &lt;a href=&quot;https://arxiv.org/abs/2203.16258&quot;&gt;Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data&lt;/a&gt;, we propose a self-supervised pre-training method for 3D perception models that is tailored to autonomous driving data and that does not require any annotation. Specifically, we leverage the availability of synchronized and calibrated image and Lidar data in autonomous driving setups for distilling self-supervised pre-trained image representations into 3D models, using neither point cloud nor image annotations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/2022_cvpr/SLidR_overview_2.png&quot; alt=&quot;slidr_overview&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Synchronized Lidar and camera frames are encoded through two modality-specific features extractors.&lt;/b&gt; The camera backbone has pre-trained weights obtained with no annotations (e.g., with MoCo v2 &lt;a class=&quot;citation&quot; href=&quot;#chen2020improved&quot;&gt;(Chen et al., 2020)&lt;/a&gt;). Features are pooled at a pseudo-object level using image superpixels, and contrasted between both modalities&lt;/div&gt;

&lt;p&gt;A key ingredient of our method is the use of superpixels which are used to pool 3D point features and 2D pixel features in visually similar regions. We then train a 3D network on the self-supervised task of matching these pooled 3D-point features with the corresponding pooled image pixel features. Extensive experiments on autonomous driving datasets demonstrate the ability of our image-to-Lidar distillation strategy to produce 3D representations that transfer well to semantic segmentation and object detection tasks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/2022_cvpr/SLidR_results.jpg&quot; alt=&quot;slidr_results&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;The similarity between a query point's features (in red) and all other Lidar points is shown, to assert the quality of the learned representation. Color scale goes from purple (low similarity) to yellow (high similarity). &lt;/div&gt;

&lt;p&gt;With our pre-training, a Lidar network can learn features that are mostly consistent within an object class. This pre-training greatly improves data annotation efficiency, both in semantic segmentation and object detection, and is even applicable in cross-dataset setups.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;raw-high-definition-radar-for-multi-task-learning&quot;&gt;Raw High-Definition Radar for Multi-Task Learning&lt;/h2&gt;
&lt;h4 id=&quot;authors--julien-rebut-arthur-ouaknine-waqas-walik-patrick-pérez&quot;&gt;Authors:  &lt;a href=&quot;https://scholar.google.com/citations?user=BJcQNcoAAAAJ&quot;&gt;Julien Rebut&lt;/a&gt;, &lt;a href=&quot;https://arthurouaknine.github.io/&quot;&gt;Arthur Ouaknine&lt;/a&gt;, Waqas Walik, &lt;a href=&quot;https://ptrckprz.github.io/&quot;&gt;Patrick Pérez&lt;/a&gt;&lt;/h4&gt;

&lt;h4 align=&quot;center&quot;&gt; [&lt;a href=&quot;https://arxiv.org/abs/2112.10646 &quot;&gt;Paper&lt;/a&gt;] &amp;nbsp;&amp;nbsp; [&lt;a href=&quot;https://github.com/valeoai/RADIal&quot;&gt;Code&lt;/a&gt;] &amp;nbsp;&amp;nbsp; [&lt;a href=&quot;https://valeoai.github.io/blog/publications/radial/&quot;&gt;Project page&lt;/a&gt;]&lt;/h4&gt;

&lt;p&gt;With their robustness to adverse weather conditions and their ability to measure speeds, radar sensors have been part of the automotive landscape for more than two decades. Recent progress toward High Definition (HD) Imaging radars has driven the angular resolution below the degree, thus approaching laser scanning performance. However, the amount of data a HD radar delivers and the computational cost to estimate the angular positions remain a challenge. In this paper, we propose a novel HD radar sensing model, FFT-RadNet, that eliminates the overhead of computing the range-azimuth-Doppler 3D tensor, learning instead to recover angles from a range-Doppler spectrum. This architecture can be leveraged for various perception tasks with raw HD radar signals. In particular we show how to train FFT-RadNet both to detect vehicles and to segment free driving space. On both tasks, it competes with the most recent radar-based models while requiring less compute and memory.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/2022_cvpr/radial_overview.png&quot; alt=&quot;&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;Overview of FFT-RadNet for vehicle detection and drivable space segmentation in raw HD radar signal.&lt;/div&gt;

&lt;p&gt;Also, and importantly, we collected and annotated 2-hour worth of raw data from synchronized automotive-grade sensors (camera, laser, HD radar) in various environments (city street, highway, countryside road). This unique dataset, nick-named RADIal for “Radar, Lidar et al.”, is &lt;a href=&quot;https://valeoai.github.io/blog/publications/radial/&quot;&gt;publicly available&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/2022_cvpr/radial_teaser.jpg&quot; alt=&quot;&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Scene sample form RADIal dataset&lt;/b&gt; with (a) camera image, (b) radar power spectrum, (c) free-space in bird-eye view, (d) Range-azimuth map in Cartesian coordinates, and (e) GPS trace (red) and odometry trajectory (green); laser (resp. radar) points are in red (resp. indigo), annotated vehicle bounding boxes in orange and annotated drivable space in green.&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;poco-point-convolution-for-surface-reconstruction&quot;&gt;POCO: Point convolution for surface reconstruction&lt;/h2&gt;
&lt;h4 id=&quot;authors-alexandre-boulch-renaud-marlet&quot;&gt;Authors: &lt;a href=&quot;https://boulch.eu/&quot;&gt;Alexandre Boulch&lt;/a&gt;, &lt;a href=&quot;http://imagine.enpc.fr/~marletr/&quot;&gt;Renaud Marlet&lt;/a&gt;&lt;/h4&gt;

&lt;h4 align=&quot;center&quot;&gt; [&lt;a href=&quot;https://arxiv.org/abs/2201.01831&quot;&gt;Paper&lt;/a&gt;] &amp;nbsp;&amp;nbsp; [&lt;a href=&quot;https://github.com/valeoai/POCO&quot;&gt;Code&lt;/a&gt;] &amp;nbsp;&amp;nbsp; [&lt;a href=&quot;https://valeoai.github.io/blog/publications/poco/&quot;&gt;Project page&lt;/a&gt;]&lt;/h4&gt;

&lt;p&gt;Implicit neural networks have been successfully used for surface reconstruction from point clouds. However, many of them face scalability issues as they encode the isosurface function of a whole object or scene into a single latent vector. To overcome this limitation, a few approaches infer latent vectors on a coarse regular 3D grid or on 3D patches, and interpolate them to answer occupancy queries. In doing so, they lose the direct connection with the input points sampled on the surface of objects, and they attach information uniformly in space rather than where it matters the most, i.e., near the surface. Besides, relying on fixed patch sizes may require discretization tuning.&lt;/p&gt;

&lt;p&gt;In POCO, we propose to use point cloud convolution and compute a latent vector at each input point. We then perform a learning-based interpolation on nearest neighbors using inferred weights. On the one hand, using a convolutional backbone allows the aggregation of global information about the shape needed to correctly orientate the surface (decide which side of the surface is inside or outside). On the other hand, surface location is inferred via a local attention-based approach which enables accurate surface positioning.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/2022_cvpr/poco_teaser.png&quot; alt=&quot;&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;POCO overview.&lt;/b&gt; Top row: the decoding mechanism takes as input local latent vectors and local coordinates which are lifted with a point-wise MLP. The resulting representations are weighted with an attention mechanism in order to take the occupancy decision.
Bottom row: reconstruction examples with POCO, scene reconstruction with a model trained on objects (left), object reconstruction with noisy point cloud (middle) and out of domain object reconstruction (right).
&lt;/div&gt;

&lt;p&gt;We show that our approach, while being very simple to set up, reaches the state of the art on several reconstruction-from-point-cloud benchmarks. It underlines the importance of reasoning about the surface location at a local scale, close to the input points.
POCO also shows good generalization properties including the possibility of learning on object datasets while being able to reconstruct complex scenes.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;dytox-transformers-for-continual-learning-with-dynamic-token-expansion&quot;&gt;DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion&lt;/h2&gt;
&lt;h4 id=&quot;authors-arthur-douillard--alexandre-ramé--guillaume-couairon-matthieu-cord&quot;&gt;Authors: &lt;a href=&quot;https://arthurdouillard.com/&quot;&gt;Arthur Douillard&lt;/a&gt;,  &lt;a href=&quot;https://alexrame.github.io/&quot;&gt;Alexandre Ramé&lt;/a&gt;,  Guillaume Couairon, &lt;a href=&quot;http://webia.lip6.fr/~cord/&quot;&gt;Matthieu Cord&lt;/a&gt;&lt;/h4&gt;

&lt;h4 align=&quot;center&quot;&gt; [&lt;a href=&quot;https://arxiv.org/abs/2111.11326&quot;&gt;Paper&lt;/a&gt;] &amp;nbsp;&amp;nbsp; [&lt;a href=&quot;https://github.com/arthurdouillard/dytox&quot;&gt;Code&lt;/a&gt;] &lt;/h4&gt;

&lt;p&gt;Deep network architectures struggle to continually learn new tasks without forgetting the previous tasks. In this paper, we propose a transformer architecture based on a dedicated encoder/decoder framework. Critically, the encoder and decoder are shared among all tasks. Through a dynamic expansion of special tokens, we specialize each forward of our decoder network on a task distribution. Our strategy scales to a large number of tasks while having negligible memory and time overheads due to strict control of the parameters expansion. Moreover, this efficient strategy does not need any hyperparameter tuning to control the network’s expansion. Our model reaches excellent results on CIFAR100 and state-of-the-art performances on the large-scale ImageNet100 and ImageNet1000 while having fewer parameters than concurrent dynamic frameworks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/2022_cvpr/dytox.png&quot; alt=&quot;&quot; height=&quot;85%&quot; width=&quot;85%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt; DyTox transformer model.&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;flexit-towards-flexible-semantic-image-translation&quot;&gt;FlexIT: Towards Flexible Semantic Image Translation&lt;/h2&gt;
&lt;h4 id=&quot;authors-guillaume-couairon-asya-grechka-jakob-verbeek-holger-schwenk-matthieu-cord&quot;&gt;Authors: Guillaume Couairon, Asya Grechka, &lt;a href=&quot;https://lear.inrialpes.fr/people/verbeek/&quot;&gt;Jakob Verbeek&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.fr/citations?user=Ysjk8kkAAAAJ&amp;amp;hl=en&quot;&gt;Holger Schwenk&lt;/a&gt;, &lt;a href=&quot;http://webia.lip6.fr/~cord/&quot;&gt;Matthieu Cord&lt;/a&gt;&lt;/h4&gt;

&lt;h4 align=&quot;center&quot;&gt; [&lt;a href=&quot;https://arxiv.org/abs/2203.04705&quot;&gt;Paper&lt;/a&gt;] &lt;/h4&gt;

&lt;p&gt;Deep generative models, like GANs, have considerably improved the state of the art in image synthesis, and are able to generate near photo-realistic images in structured domains such as human faces. We propose FlexIT, a novel method which can take any input image and a user-defined text instruction for editing. Our method achieves flexible and natural editing, pushing the limits of semantic image translation. First, FlexIT combines the input image and text into a single target point in the CLIP multimodal embedding space. Via the latent space of an auto-encoder, we iteratively transform the input image toward the target point, ensuring coherence and quality with a variety of novel regularization terms. We propose an evaluation protocol for semantic image translation, and thoroughly evaluate our method on ImageNet.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/2022_cvpr/flexit.png&quot; alt=&quot;&quot; height=&quot;95%&quot; width=&quot;95%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt; &lt;b&gt;FlexIT transformation examples.&lt;/b&gt; From top to bottom: input image, transformed image, and text query.&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;raising-context-awareness-in-motion-forecasting&quot;&gt;Raising context awareness in motion forecasting&lt;/h2&gt;
&lt;p class=&quot;page-description&quot;&gt;&lt;a href=&quot;https://cvpr2022.wad.vision/&quot;&gt;CVPR 2022 Workshop on Autonomous Driving&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;authors-hédi-ben-younes-éloi-zablocki-mickaël-chen-patrick-pérez-matthieu-cord&quot;&gt;Authors: &lt;a href=&quot;https://scholar.google.com/citations?hl=fr&amp;amp;user=IFLcfvUAAAAJ&quot;&gt;Hédi Ben-Younes&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/citations?user=dOkbUmEAAAAJ&amp;amp;hl=fr&quot;&gt;Éloi Zablocki&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/citations?user=QnRpMJAAAAAJ&amp;amp;hl=fr&amp;amp;oi=sra&quot;&gt;Mickaël Chen&lt;/a&gt;, &lt;a href=&quot;https://ptrckprz.github.io/&quot;&gt;Patrick Pérez&lt;/a&gt;, &lt;a href=&quot;http://webia.lip6.fr/~cord/&quot;&gt;Matthieu Cord&lt;/a&gt;&lt;/h4&gt;

&lt;h4 align=&quot;center&quot;&gt; [&lt;a href=&quot;https://arxiv.org/abs/2109.08048&quot;&gt;Paper&lt;/a&gt;]&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/2022_cvpr/cab.png&quot; alt=&quot;cab_overview&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Overview of CAB.&lt;/b&gt; CAB employs a CVAE backbone which produces distributions over the latent variable and the future trajectory. During training, a blind input is forwarded into the CVAE and the resulting distribution over the latent variable is used to encourage the prediction of the model to be different from the context-agnostic distribution, thanks to the CAB-KL loss.&lt;/div&gt;

&lt;p&gt;Learning-based trajectory prediction models have encountered great success, with the promise of leveraging contextual information in addition to motion history. Yet, we find that state-of-the-art forecasting methods tend to overly rely on the agent’s current dynamics, failing to exploit the semantic contextual cues provided at its input. To alleviate this issue, we introduce CAB, a motion forecasting model equipped with a training procedure designed to promote the use of semantic contextual information. We also introduce two novel metrics, dispersion and convergence-to-range, to measure the temporal consistency of successive forecasts, which we found missing in standard metrics. Our method is evaluated on the widely adopted nuScenes Prediction benchmark as well as on a subset of the most difficult examples from this benchmark.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;csg0-continual-urban-scene-generation-with-zero-forgetting&quot;&gt;CSG0: Continual Urban Scene Generation with Zero Forgetting&lt;/h2&gt;
&lt;p class=&quot;page-description&quot;&gt;&lt;a href=&quot;https://sites.google.com/view/clvision2022&quot;&gt;CVPR 2022 Workshop on Continual Learning (CLVision)&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;authors--himalaya-jain-tuan-hung-vu-patrick-pérez-matthieu-cord&quot;&gt;Authors:  &lt;a href=&quot;https://himalayajain.github.io/&quot;&gt;Himalaya Jain&lt;/a&gt;, &lt;a href=&quot;https://tuanhungvu.github.io/&quot;&gt;Tuan-Hung Vu&lt;/a&gt;, &lt;a href=&quot;https://ptrckprz.github.io/&quot;&gt;Patrick Pérez&lt;/a&gt;, &lt;a href=&quot;http://webia.lip6.fr/~cord/&quot;&gt;Matthieu Cord&lt;/a&gt;&lt;/h4&gt;

&lt;h4 align=&quot;center&quot;&gt; [&lt;a href=&quot;https://arxiv.org/abs/2112.03252&quot;&gt;Paper&lt;/a&gt;] &amp;nbsp;&amp;nbsp; [&lt;a href=&quot; https://valeoai.github.io/blog/publications/csg0/ 
&quot;&gt;Project page&lt;/a&gt;]&lt;/h4&gt;

&lt;p&gt;With the rapid advances in generative adversarial networks (GANs), the visual quality of synthesized scenes keeps improving, including for complex urban scenes with applications to automated driving. We address in this work a continual scene generation setup in which GANs are trained on a stream of distinct domains; ideally, the learned models should eventually be able to generate new scenes in all seen domains. This setup reflects the real-life scenario where data are continuously acquired in different places at different times. In such a continual setup, we aim for learning with zero forgetting, i.e., with no degradation in synthesis quality over earlier domains due to catastrophic forgetting. To this end, we introduce a novel framework, named CSG0, that not only (i) enables seamless knowledge transfer in continual training but also (ii) guarantees zero forgetting with a small overhead cost.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/2022_cvpr/csg0_teaser.png&quot; alt=&quot;&quot; height=&quot;85%&quot; width=&quot;85%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Overview of CSG0.&lt;/b&gt; Our continual setup for urban-scene generation involves a stream of datasets, with GANs trained from one dataset to another. Our framework makes use of the knowledge learned from previous domains and adapts to new ones with a small overhead.&lt;/div&gt;

&lt;p&gt;To showcase the merit of our framework, we conduct intensive experiments on various continual urban scene setups, covering both synthetic-to-real and real-to-real scenarios. Quantitative evaluations and qualitative visualizations demonstrate the interest of our CSG0 framework, which operates with minimal overhead cost (in terms of architecture size and training). Benefiting from continual learning, CSG0 outperforms the state-of-the-art OASIS model trained on single domains. We also provide experiments with three datasets to emphasize how well our strategy generalizes despite its cost constraints. Under extreme low-data regimes, our approach outperforms the baseline by a large margin.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;multi-head-distillation-for-continual-unsupervised-domain-adaptation-in-semantic-segmentation&quot;&gt;Multi-Head Distillation for Continual Unsupervised Domain Adaptation in Semantic Segmentation&lt;/h2&gt;
&lt;p class=&quot;page-description&quot;&gt;&lt;a href=&quot;https://sites.google.com/view/clvision2022&quot;&gt;CVPR 2022 Workshop on Continual Learning (CLVision)&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;authors--antoine-saporta-arthur-douillard-tuan-hung-vu-patrick-pérez-matthieu-cord&quot;&gt;Authors:  &lt;a href=&quot;https://scholar.google.com/citations?user=jSwfIU4AAAAJ&quot;&gt;Antoine Saporta&lt;/a&gt;, &lt;a href=&quot;https://arthurdouillard.com/&quot;&gt;Arthur Douillard&lt;/a&gt;, &lt;a href=&quot;https://tuanhungvu.github.io/&quot;&gt;Tuan-Hung Vu&lt;/a&gt;, &lt;a href=&quot;https://ptrckprz.github.io/&quot;&gt;Patrick Pérez&lt;/a&gt;, &lt;a href=&quot;http://webia.lip6.fr/~cord/&quot;&gt;Matthieu Cord&lt;/a&gt;&lt;/h4&gt;

&lt;h4 align=&quot;center&quot;&gt; [&lt;a href=&quot;https://arxiv.org/abs/2204.11667&quot;&gt;Paper&lt;/a&gt;] &amp;nbsp;&amp;nbsp; [&lt;a href=&quot;https://github.com/valeoai/MuHDi&quot;&gt;Code&lt;/a&gt;] &amp;nbsp;&amp;nbsp; [&lt;a href=&quot;https://valeoai.github.io/blog/publications/muhdi/&quot;&gt;Project page&lt;/a&gt;]&lt;/h4&gt;

&lt;p&gt;This work focuses on a novel framework for learning UDA, continuous UDA, in which models operate on multiple target domains discovered sequentially, without access to previous target domains. We propose MuHDi, for Multi-Head Distillation, a method that solves the catastrophic forgetting problem, inherent in continual learning tasks. MuHDi performs distillation at multiple levels from the previous model as well as an auxiliary target-specialist segmentation head. We report both extensive ablation and experiments on challenging multi-target UDA semantic segmentation benchmarks to validate the proposed learning scheme and architecture.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/2022_cvpr/muhdi_teaser.png&quot; alt=&quot;&quot; height=&quot;90%&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Predictions of continual baseline and MuHDi in a Cityscapes scene.&lt;/b&gt; The baseline model suffers from catastrophic forgetting when adapting from one domain to another. The proposed MuHDi is more resilient to continual adaptation and preserve predictive accuracy.
&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;chen2020improved&quot;&gt;Chen, X., Fan, H., Girshick, R., &amp;amp; He, K. (2020). Improved baselines with momentum contrastive learning. &lt;i&gt;ArXiv Preprint ArXiv:2003.04297&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name></name></author><summary type="html">The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR) is a major event for researchers and engineers working on computer vision and machine learning. At the 2022 edition the valeo.ai team will present four papers in the main conference, three papers in workshops and one workshop keynote. The team will be at CVPR to present these works and will be happy to discuss more about these projects and ideas, and share our exciting ongoing research.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io/blog/images/posts/2022_cvpr/cvpr_logo.png" /><media:content medium="image" url="https://valeoai.github.io/blog/images/posts/2022_cvpr/cvpr_logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">valeo.ai at ICCV 2021</title><link href="https://valeoai.github.io/blog/2021/10/08/valeoai-at-iccv-2021.html" rel="alternate" type="text/html" title="valeo.ai at ICCV 2021" /><published>2021-10-08T00:00:00-05:00</published><updated>2021-10-08T00:00:00-05:00</updated><id>https://valeoai.github.io/blog/2021/10/08/valeoai-at-iccv-2021</id><content type="html" xml:base="https://valeoai.github.io/blog/2021/10/08/valeoai-at-iccv-2021.html">&lt;p&gt;The &lt;a href=&quot;https://iccv2021.thecvf.com/home&quot;&gt;International Conference on Computer Vision (ICCV)&lt;/a&gt; is a top event for researchers and engineers working on computer vision and machine learning. The &lt;a href=&quot;https://ptrckprz.github.io/valeoai/&quot;&gt;valeo.ai&lt;/a&gt; team will present six &lt;a href=&quot;https://ptrckprz.github.io/vaipub/&quot;&gt;papers&lt;/a&gt; in the main conference, four of which are presented below. Join us to find out more about these projects and ideas, meet our team and learn about our exciting ongoing research. See you at ICCV!&lt;/p&gt;

&lt;h2 id=&quot;multi-view-radar-semantic-segmentation&quot;&gt;Multi-View Radar Semantic Segmentation&lt;/h2&gt;
&lt;h4 id=&quot;authors-arthur-ouaknine-alasdair-newson-patrick-pérez-florence-tupin-julien-rebut&quot;&gt;Authors: &lt;a href=&quot;https://arthurouaknine.github.io/&quot;&gt;Arthur Ouaknine&lt;/a&gt;, &lt;a href=&quot;https://sites.google.com/site/alasdairnewson/&quot;&gt;Alasdair Newson&lt;/a&gt;, &lt;a href=&quot;https://ptrckprz.github.io/&quot;&gt;Patrick Pérez&lt;/a&gt;, &lt;a href=&quot;https://perso.telecom-paristech.fr/tupin/&quot;&gt;Florence Tupin&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/citations?user=BJcQNcoAAAAJ&amp;amp;hl=fr&quot;&gt;Julien Rebut&lt;/a&gt;&lt;/h4&gt;

&lt;h4 align=&quot;center&quot;&gt; [&lt;a href=&quot;https://arxiv.org/abs/2103.16214&quot;&gt;Paper&lt;/a&gt;] &amp;nbsp;&amp;nbsp; [&lt;a href=&quot;https://github.com/valeoai/MVRSS&quot;&gt;Code&lt;/a&gt;] &amp;nbsp;&amp;nbsp; [&lt;a href=&quot;https://valeoai.github.io/blog/publications/mvrss/&quot;&gt;Project page&lt;/a&gt;]&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/2021_iccv/radar.gif&quot; alt=&quot;radar_overview&quot; height=&quot;80%&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Example of a scene from the CARRADA dataset &lt;a class=&quot;citation&quot; href=&quot;#ouaknine2021carrada&quot;&gt;(Ouaknine et al., 2021)&lt;/a&gt;.&lt;/b&gt; From left to right: camera image, range-angle view, range-Doppler view, angle, Doppler view.&lt;/div&gt;

&lt;p&gt;Understanding the scene around the ego-vehicle is key to assisted and autonomous driving. Nowadays, this is mostly conducted using cameras and laser scanners, despite their reduced performance in adverse weather conditions. Automotive radars are low-cost active sensors that measure properties of surrounding objects, including their relative speed, and have the key advantage of not being impacted by rain, snow or fog and could effectively complement the other perception sensors mounted on the car, e.g., cameras, LIDAR. However, they are seldom used for scene understanding due to the size and complexity of radar raw data and the lack of annotated datasets. Fortunately, recent open-sourced datasets have opened up research on classification, object detection and semantic segmentation with raw radar signals using end-to-end trainable models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/2021_iccv/radar_semseg.png&quot; alt=&quot;mvrss_overview&quot; height=&quot;80%&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;Sequences of raw radar tensors are aggregated and used as input for our multi-view architecture to segment semantically range-angle and range-Doppler views simultaneously.
&lt;/div&gt;

&lt;p&gt;In our paper, &lt;a href=&quot;https://arxiv.org/abs/2103.16214&quot;&gt;Multi-View Radar Semantic Segmentation&lt;/a&gt;, we propose a set of deep neural network architectures to segment simultaneously range-angle and range-Doppler radar representations, providing the location and the radial velocity of the detected objects. Our best model takes a sequence of radar views as input, extracts features using individual branches including ASPP blocks, and recovers the range-angle and range-Doppler view dimensions with two decoding branches. We also propose a combination of loss functions composed of a weighted cross entropy, a soft dice and an additional coherence term. We introduce a coherence loss to impose a spatial consistency between the segmented radar views. Our experiments on the CARRADA dataset &lt;a class=&quot;citation&quot; href=&quot;#ouaknine2021carrada&quot;&gt;(Ouaknine et al., 2021)&lt;/a&gt; demonstrate that our best model outperforms competing methods with a large margin while requiring significantly fewer parameters.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;triggering-failures-out-of-distribution-detection-by-learning-from-local-adversarial-attacks-in-semantic-segmentation&quot;&gt;Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation&lt;/h2&gt;
&lt;h4 id=&quot;authors-victor-besnier-andrei-bursuc-alexandre-briot-david-picard&quot;&gt;Authors: &lt;a href=&quot;https://scholar.google.com/citations?user=n_C2h-QAAAAJ&quot;&gt;Victor Besnier&lt;/a&gt;, &lt;a href=&quot;https://abursuc.github.io/&quot;&gt;Andrei Bursuc&lt;/a&gt;, Alexandre Briot, &lt;a href=&quot;https://davidpicard.github.io/&quot;&gt;David Picard&lt;/a&gt;&lt;/h4&gt;

&lt;h4 align=&quot;center&quot;&gt; [&lt;a href=&quot;https://arxiv.org/abs/2108.01634&quot;&gt;Paper&lt;/a&gt;] &amp;nbsp;&amp;nbsp; [&lt;a href=&quot;https://github.com/valeoai/obsnet&quot;&gt;Code&lt;/a&gt;] &amp;nbsp;&amp;nbsp; [&lt;a href=&quot;https://valeoai.github.io/blog/publications/obsnet/&quot;&gt;Project page&lt;/a&gt;]&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/2021_iccv/obsnet_qualitative.png&quot; alt=&quot;&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Uncertainty map visualization on the BDD-Anomaly dataset.&lt;/b&gt; 1st col.: We highlight the ground truth locations of the OOD objects to help visualize them (red bounding box). 2nd col.: Segmentation map of the SegNet. 3rd to 5th col.: Uncertainty Map highlighted in yellow. Our method produces stronger responses on OOD regions compared to other methods, while being as strong on regular error regions, e.g., boundaries. 
&lt;/div&gt;

&lt;p&gt;For real-world decision systems such as autonomous vehicles, accuracy is not the only performance requirement and it often comes second to &lt;em&gt;reliability&lt;/em&gt;, &lt;em&gt;robustness&lt;/em&gt;, and &lt;em&gt;safety concerns&lt;/em&gt;, as any failure carries serious consequences. Component modules of such systems frequently rely on powerful Deep Neural Networks (DNNs), that however do not always generalize to objects unseen in the training data.  Simple uncertainty estimation techniques, e.g., entropy of softmax predictions, are less effective since modern DNNs are consistently overconfident on both in-domain and out-of-distribution (OOD) data samples. This hinders further the performance of downstream components relying on their predictions. Dealing successfully with the &lt;em&gt;“unknown unknown”&lt;/em&gt;, e.g., by launching an alert or failing gracefully, is crucial.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/2021_iccv/robot_hit.gif&quot; alt=&quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;By making our target model to fail we can learn its behavior when failing and more reliably detect it at test time.&lt;/div&gt;

&lt;p&gt;In this work we take inspiration from practices in industrial validation, where the performance of a target model is tested in various extreme cases. Instead of simply verifying the performance of the model we learn how this model behaves in face of failures. To this end we propose a new OOD detection architecture called ObsNet and an associated training scheme based on Local Adversarial Attacks (LAA). Finding failure modes in a trained DNN is quite challenging as such models typically achieve high accuracy, i.e., are rarely wrong, and corner-case samples are rather inserted in the training set than used for validation. LAA triggers failure modes in the target model that are a good proxy for failures in face of unknown OOD data. 
ObsNet achieves reliable detection of failure and OOD objects without compromising on predictive accuracy and computational time.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;multi-target-adversarial-frameworks-for-domain-adaptation-in-semantic-segmentation&quot;&gt;Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation&lt;/h2&gt;
&lt;h4 id=&quot;authors--antoine-saporta-tuan-hung-vu-matthieu-cord-patrick-pérez&quot;&gt;Authors:  &lt;a href=&quot;https://scholar.google.com/citations?user=jSwfIU4AAAAJ&quot;&gt;Antoine Saporta&lt;/a&gt;, &lt;a href=&quot;https://tuanhungvu.github.io/&quot;&gt;Tuan-Hung Vu&lt;/a&gt;, &lt;a href=&quot;http://webia.lip6.fr/~cord/&quot;&gt;Matthieu Cord&lt;/a&gt;, &lt;a href=&quot;https://ptrckprz.github.io/&quot;&gt;Patrick Pérez&lt;/a&gt;&lt;/h4&gt;

&lt;h4 align=&quot;center&quot;&gt; [&lt;a href=&quot;https://arxiv.org/abs/2108.06962&quot;&gt;Paper&lt;/a&gt;] &amp;nbsp;&amp;nbsp; [&lt;a href=&quot;https://github.com/valeoai/MTAF&quot;&gt;Code&lt;/a&gt;] &amp;nbsp;&amp;nbsp; [&lt;a href=&quot;https://valeoai.github.io/blog/publications/mtaf/&quot;&gt;Project page&lt;/a&gt;]&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/2021_iccv/mtaf_teaser.png&quot; alt=&quot;&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Single-target unsupervised domain adaptation fails to produce models that perform on multiple target domains.&lt;/b&gt; The aim of multi-target unsupervised domain adaptation is to train a model that excels on these multiple target domains.&lt;/div&gt;

&lt;p&gt;Autonomous vehicles rely on perception models that require a tremendous amount of annotated data to be trained in a supervised fashion. To reduce the reliance on manual annotation which can get extremely expensive when we consider semantic segmentation of urban scenes for instance, domain adaptation is a popular topic that leverages annotated data from a source domain to train a model on a target domain. More specifically, the unsupervised domain adaptation (UDA) setting only relies on unlabeled data from the target domain and aims at bridging the gap between target and source domains. Most UDA approaches tackle the alignment between a single source domain and a single target domain but don’t generalize well to more domains. Yet, real-world perception systems need to be confronted to a variety of scenarios, such as multiple cities or multiple weather conditions, motivating to extend UDA to multi-target settings.&lt;/p&gt;

&lt;p&gt;In our work, &lt;a href=&quot;&amp;quot;https://arxiv.org/abs/2108.06962&quot;&gt;Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation&lt;/a&gt;, we introduce two UDA frameworks to tackle multi-target adaptation: &lt;strong&gt;(i)&lt;/strong&gt; multi-discriminator, which extends single target UDA approaches to multiple target domains by explicitly aligning each target domain to its counterparts; &lt;strong&gt;(ii)&lt;/strong&gt; multi-target knowledge transfer, which learns a target-agnostic model thanks to a multiple teachers/single student distillation mechanism. We also propose multiple new challenging evaluation benchmarks for multi-target UDA in semantic segmentation based on existing urban scenes datasets.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;pcam-product-of-cross-attention-matrices-for-rigid-registration-of-point-clouds&quot;&gt;PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds&lt;/h2&gt;
&lt;h4 id=&quot;authors-anh-quan-cao-gilles-puy-alexandre-boulch-renaud-marlet&quot;&gt;Authors: &lt;a href=&quot;https://anhquancao.github.io&quot;&gt;Anh-Quan Cao&lt;/a&gt;, &lt;a href=&quot;https://sites.google.com/site/puygilles/home&quot;&gt;Gilles Puy&lt;/a&gt;, &lt;a href=&quot;https://www.boulch.eu/&quot;&gt;Alexandre Boulch&lt;/a&gt;, &lt;a href=&quot;http://imagine.enpc.fr/~marletr/&quot;&gt;Renaud Marlet&lt;/a&gt;&lt;/h4&gt;

&lt;h4 align=&quot;center&quot;&gt; [&lt;a href=&quot;http://arxiv.org/abs/2110.01269&quot;&gt;Paper&lt;/a&gt;] &amp;nbsp;&amp;nbsp; [&lt;a href=&quot;https://github.com/valeoai/PCAM&quot;&gt;Code&lt;/a&gt;] &amp;nbsp;&amp;nbsp; [&lt;a href=&quot;https://valeoai.github.io/blog/publications/pcam/&quot;&gt;Project page&lt;/a&gt;]&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/2021_iccv/pcam_overview.png&quot; alt=&quot;&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Point cloud registration has many applications in various domains such as autonomous driving, motion and pose estimation, 3D reconstruction, simultaneous localisation and mapping (SLAM), and augmented reality. The most famous method to solve this task is ICP, but is mostly suited for small transformations. Several improvements have been made and the most recent techniques leverage deep learning.&lt;/p&gt;

&lt;p&gt;The typical pipeline for point cloud registration is &lt;strong&gt;(a)&lt;/strong&gt; point matching followed by &lt;strong&gt;(b)&lt;/strong&gt; point-pairs filtering to remove incorrect matches in, e.g., non-overlapping regions. One natural way to improve this pipeline is to use deep learning in step &lt;strong&gt;(a)&lt;/strong&gt; to obtain point features of high quality and get pairs of matching points with a nearest neighbors search in this learned feature space. Then, one can typically rely on a classical RANSAC-based method in step &lt;strong&gt;(b)&lt;/strong&gt;. Another category of methods exploits deep learning in step &lt;strong&gt;(a)&lt;/strong&gt; and step &lt;strong&gt;(b)&lt;/strong&gt;, as proposed by, e.g., DCP, PRNet, DGR. PCAM belongs to this second category where a first network outputs pairs of matching points and a second network filters incorrect pairs.&lt;/p&gt;

&lt;p&gt;We construct PCAM by observing that one needs two types of information to correctly match points between two point clouds. First, one needs local fine geometric information to precisely select the best corresponding point. Second, one also needs high-level contextual information to differentiate between points with similar local geometry but from different parts of the scene. Therefore, we compute point correspondences at every layer of our deep network via cross-attention matrices, and combine these matrices via a pointwise multiplication. This simple yet very effective solution naturally ensures that both low-level geometric and high-level context information are exploited when matching points. It also permits to remove spurious matches found only at one scale. Furthermore, these cross-attention matrices are also exploited to exchange information between the point clouds at each layer, allowing the network to use context information to find the best matching point within the overlapping regions.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;ouaknine2021carrada&quot;&gt;Ouaknine, A., Newson, A., Rebut, J., Tupin, F., &amp;amp; Pérez, P. (2021). CARRADA Dataset: Camera and Automotive Radar with Range- Angle- Doppler Annotations. &lt;i&gt;2020 25th International Conference on Pattern Recognition (ICPR)&lt;/i&gt;, 5068–5075.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name></name></author><summary type="html">The International Conference on Computer Vision (ICCV) is a top event for researchers and engineers working on computer vision and machine learning. The valeo.ai team will present six papers in the main conference, four of which are presented below. Join us to find out more about these projects and ideas, meet our team and learn about our exciting ongoing research. See you at ICCV!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io/blog/images/posts/2021_iccv/iccv_logo.png" /><media:content medium="image" url="https://valeoai.github.io/blog/images/posts/2021_iccv/iccv_logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">How can we make driving systems explainable?</title><link href="https://valeoai.github.io/blog/2021/02/18/explainable-driving.html" rel="alternate" type="text/html" title="How can we make driving systems explainable?" /><published>2021-02-18T00:00:00-06:00</published><updated>2021-02-18T00:00:00-06:00</updated><id>https://valeoai.github.io/blog/2021/02/18/explainable-driving</id><content type="html" xml:base="https://valeoai.github.io/blog/2021/02/18/explainable-driving.html">&lt;p&gt;&lt;em&gt;This post is an introduction to our survey on the explainability of vision-based driving systems, which can be found on arXiv &lt;a href=&quot;https://arxiv.org/abs/2101.05307&quot;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Research on autonomous vehicles is blooming thanks to recent advances in deep learning and computer vision, as well as the development of autonomous driving datasets and simulators.
The number of academic publications on this subject is rising in most machine learning, computer vision, robotics and transportation conferences, and journals.
On the industry side, several suppliers are already producing cars equipped with advanced computer vision technologies for automatic lane following, assisted parking, or collision detection among other things. Meanwhile, constructors are working on and designing prototypes with level 4 and 5 autonomy.&lt;/p&gt;

&lt;p&gt;In the 2010s, we observe an interest in approaches aiming to &lt;em&gt;train&lt;/em&gt; driving systems, usually in the form of neural networks, either by leveraging large quantities of expert recordings or through simulation.
In both cases, these systems learn a highly complex transformation that operates over input sensor data and produces end-commands (steering angle, throttle). 
While these neural driving models overcome some of the limitations of the traditional modular pipeline stack, they are sometimes described as &lt;em&gt;black-boxes&lt;/em&gt; for their critical lack of transparency and interpretability. 
Thus, being able to explain the behavior of neural driving models is of paramount importance for their deployment and social acceptance.&lt;/p&gt;

&lt;h3 id=&quot;explainability&quot;&gt;Explainability?&lt;/h3&gt;

&lt;p&gt;Many terms are related to the concept of explainability and several definitions have been proposed for each of these terms. The boundaries between concepts are fuzzy and constantly evolving. 
In human-machine interactions, explainability is defined as the ability for the human user to understand the agent’s logic &lt;a class=&quot;citation&quot; href=&quot;#RosenfeldR19&quot;&gt;(Rosenfeld &amp;amp; Richardson, 2019)&lt;/a&gt;. 
The explanation is based on how the human user understands the connections between inputs and outputs of the model. 
According to &lt;a class=&quot;citation&quot; href=&quot;#doshi2017accountability&quot;&gt;(Doshi-Velez &amp;amp; Kortz, 2017)&lt;/a&gt;, an explanation is a human-interpretable description of the process by which a decision-maker took a particular set of inputs and reached a particular conclusion. They state that in practice, an explanation should answer at least one of the three following questions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;What were the main factors in the decision?&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Would changing a certain factor have changed the decision?&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Why did two similar-looking cases get different decisions, or vice versa?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The term &lt;em&gt;explainability&lt;/em&gt; often co-occurs with the concept of &lt;em&gt;interpretability&lt;/em&gt;.
Some recent work of &lt;a class=&quot;citation&quot; href=&quot;#beaudouin2020identifying&quot;&gt;(Beaudouin et al., 2020)&lt;/a&gt; simply advocate that explainability and interpretability are synonyms.
However, &lt;a class=&quot;citation&quot; href=&quot;#GilpinBYBSK18&quot;&gt;(Gilpin et al., 2018)&lt;/a&gt; provide a nuance between these terms that we find interesting. According to them, interpretability designates to which extent an explanation is understandable by a human. 
They state that an explanation should be designed and assessed in a trade-off between its interpretability and its completeness, which measures how accurate the explanation is as it describes the inner workings of the system. 
&lt;!-- For example, an exhaustive and completely faithful explanation is a description of the system itself and all its processing: this is a complete explanation although the exhaustive description of the processing may be incomprehensible. --&gt;
The whole challenge in explaining neural networks is to provide explanations that are both interpretable and complete.&lt;/p&gt;

&lt;p&gt;Interestingly, depending on who is the explanation geared towards, it is expected to have varying nature, form and should convey different types of information.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;End-users&lt;/strong&gt; and citizens need to trust the autonomous system and to be reassured. They put their life in the hands of the driving system and thus need to gain trust in it. 
&lt;!-- There is a long and dense line of research trying to define, characterize, evaluate, and increase the trust between an individual and a machine.  --&gt;
It appears that user trust is heavily impacted by the system transparency &lt;a class=&quot;citation&quot; href=&quot;#trusthci20&quot;&gt;(Zhang et al., 2020)&lt;/a&gt;: providing information that helps the user understand how the system functions foster his or her trust in the system. 
&lt;!-- Interestingly, research on human-computer interactions argues that the timing of explanations is important for trust: they should be provided before the vehicle takes an action, in a formulation which is concise and direct. &lt;a class=&quot;citation&quot; href=&quot;#RosenfeldR19&quot;&gt;(Rosenfeld &amp;amp; Richardson, 2019)&lt;/a&gt;,&lt;a class=&quot;citation&quot; href=&quot;#haspiel2018explanations&quot;&gt;(Haspiel et al., 2018)&lt;/a&gt;,&lt;a class=&quot;citation&quot; href=&quot;#du2019look&quot;&gt;(Du et al., 2019)&lt;/a&gt; --&gt;
Interestingly, research on human-computer interactions argues that an explanation should be provided before the vehicle takes an action, in a formulation which is concise and direct.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Designers&lt;/strong&gt; of self-driving models need to understand their limitations to validate them and improve future versions.
The concept of Operational Design Domain (ODD) is often used by carmakers to designate the conditions under which the car is expected to behave safely.
Thus, whenever a machine learning model is built to address the task of driving, it is crucial to know and understand its failure modes, and to verify that these situations do not overlap with the ODD. 
A common practice is to stratify the evaluation into situations, as is done by the European New Car Assessment Program (Euro NCAP) to test and assess assisted driving functionalities in new vehicles.
But even if these in-depth performance analyses are helpful to improve the model’s performance, it is not possible to exhaustively list and evaluate every situation the model may possibly encounter. 
As a fallback solution, explainability can help delving deeper into the inner workings of the model and to understand why it makes these errors and correct the model/training data accordingly.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Legal and regulatory bodies&lt;/strong&gt; are interested in explanations for &lt;em&gt;liability&lt;/em&gt; and &lt;em&gt;accountability&lt;/em&gt; purposes, especially when a self-driving system is involved in a car accident. 
 Notably, explanations generated for legal or regulatory institutions are likely to be different from those addressed to the end-user, as all aspects of the decision process could be required to identify the reasons for a malfunction.
&lt;!-- These explanations are directed towards experts who will likely spend large amounts of time studying the system, and who are thus inclined to receive rich explanations with great amounts of detail.  --&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;driving-system&quot;&gt;Driving system?&lt;/h3&gt;

&lt;p&gt;The history of autonomous driving systems started in the late ’80s and early ’90s with the European Eureka project called Prometheus.
This has later been followed by &lt;a href=&quot;https://www.youtube.com/watch?v=7a6GrKqOxeU&quot;&gt;driving challenges&lt;/a&gt; proposed by the Defense Advanced Research Projects Agency (DARPA). 
The vast majority of autonomous systems competing in these challenges is characterized by their modularity: several sub-modules are assembled, each completing a very specific task. 
Broadly speaking, these subtasks deal with sensing the environment, forecasting future events, planning, taking high-level decisions, and controlling the vehicle.&lt;/p&gt;

&lt;p&gt;As pipeline architectures split the driving task into easier-to-solve problems, they offer somewhat interpretable processing of sensor data through specialized modules (perception, planning, decision, control).
However, these approaches have several drawbacks:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;First, they rely on human heuristics and manually-chosen intermediate representations, which are not proven to be optimal for the driving task.&lt;/li&gt;
  &lt;li&gt;Second, they lack flexibility to account for real-world uncertainties and to generalize to unplanned scenarios.
&lt;!-- Moreover, from an engineering point of view, these systems are hard to scale and to maintain as the various modules are entangled together. --&gt;&lt;/li&gt;
  &lt;li&gt;Finally, they are prone to error propagation between the multiple sub-modules.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To circumvent these issues, and nurtured by the deep learning revolution, researchers put more and more efforts on machine learning-based driving systems, and in particular on deep neural networks which can leverage large quantities of data.&lt;/p&gt;

&lt;p&gt;We can distinguish four key elements involved in the design of a neural driving system: input sensors, input representations, output type, and learning paradigm&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/explainable_driving/driving_architecture.png&quot; alt=&quot;driving_architecture&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 1. Overview of neural network-based autonomous driving systems.&lt;/b&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Sensors&lt;/strong&gt;. They are the hardware interface through which the neural network perceives its environment.
&lt;!-- Typical neural driving systems rely on sensors from two families: *proprioceptive* sensors and *exteroceptive* sensors. *Proprioceptive* sensors provide information about the internal vehicle state such as speed, acceleration, yaw, change of position, and velocity. They are measured through tachometers, inertial measurement units (IMU), and odometers.  All these sensors communicate through the controller area network (CAN) bus, which allows signals to be easily accessible. In contrast, *exteroceptive* sensors acquire information about the surrounding environment.  --&gt;
They include cameras, radars, LiDARs, GPS, but also sensors about internal vehicle state such as speed or yaw. For a thorough review of driving sensors, we refer the reader to &lt;a class=&quot;citation&quot; href=&quot;#survey_sensors&quot;&gt;(Yurtsever et al., 2020)&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Input representation&lt;/strong&gt;. Once sensory inputs are acquired by the system, they are processed by computer vision models to build a structured representation, before being passed to the neural driving system. In the &lt;em&gt;mediated perception&lt;/em&gt; approach, several perception systems provide their understanding of the world, and their outputs are aggregated to build an input for the driving model.
An example of such vision tasks is object detection and semantic segmentation, tracking objects across time, extracting depth information (&lt;em&gt;i.e.&lt;/em&gt; knowing the distance that separates the vehicle from each point in the space), recognizing pedestrian intent…
Mediated perception contrasts with the &lt;em&gt;direct perception&lt;/em&gt; approach, which instead extracts visual affordances from an image.
Affordances are scalar indicators that describe the road situation such as curvature, deviation to neighboring lanes, or distances between ego and other vehicles.
&lt;!-- These human-interpretable features are usually recognized using neural networks as in &lt;a class=&quot;citation&quot; href=&quot;#deepdrivingaffordance&quot;&gt;(Chen et al., 2015)&lt;/a&gt;. --&gt;
&lt;!-- Then, they are passed at the input of a driving controller which is usually hard-coded, even if some recent approaches use affordance recognition to provide compact inputs to learning-based driving systems &lt;a class=&quot;citation&quot; href=&quot;#marinaffordance&quot;&gt;(Toromanoff et al., 2020)&lt;/a&gt;.  --&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Outputs&lt;/strong&gt;. Ultimately, the goal is to generate vehicle controls. Some approaches, called end-to-&lt;em&gt;end&lt;/em&gt;, tackle this problem by training the deep network to directly output the commands.
However, in practice most methods instead predict the future trajectory of the autonomous vehicle; they are called end-to-&lt;em&gt;mid&lt;/em&gt; methods. The trajectory is then expected to be followed by a low-level controller, such as the proportional–integral–derivative (PID) controller.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Learning&lt;/strong&gt;.
Two families of methods coexist for training self-driving neural models: &lt;em&gt;behavior cloning&lt;/em&gt; approaches, which leverage datasets of human driving sessions, and &lt;em&gt;reinforcement learning&lt;/em&gt; approaches, which train models through trial-and-error simulation.
    &lt;ul&gt;
      &lt;li&gt;Behavior cloning (BC) approaches leverage huge quantities of recorded human driving sessions to learn the input-output driving mapping by imitation. 
  In this setting, the network is trained to mimic the commands applied by the expert driver (end-to-end models), or the future trajectory (end-to-mid models), in a supervised fashion. 
  An initial attempt to behavior cloning of vehicle controls was made by &lt;a class=&quot;citation&quot; href=&quot;#Pomerleau88&quot;&gt;(Pomerleau, 1988)&lt;/a&gt;, and continued later in &lt;a class=&quot;citation&quot; href=&quot;#pilotnet&quot;&gt;(Bojarski et al., 2016)&lt;/a&gt;.&lt;/li&gt;
      &lt;li&gt;Reinforcement learning (RL) was alternatively explored by researchers to train neural driving systems. This paradigm learns a policy by balancing self-exploration and reinforcement.
  This training paradigm relies on a simulator (such as CARLA &lt;a class=&quot;citation&quot; href=&quot;#carla&quot;&gt;(Dosovitskiy et al., 2017)&lt;/a&gt;).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-challenges-of-explainability-of-neural-driving-systems&quot;&gt;The challenges of explainability of neural driving systems&lt;/h3&gt;

&lt;p&gt;Introducing explainability in the design of learning-based self-driving systems is a challenging task.
These concerns arise from two aspects:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;From a &lt;strong&gt;Deep Learning perspective&lt;/strong&gt;, explainability hurdles of self-driving models are shared with most deep learning models, across many application domains. Indeed, decisions of deep systems are intrinsically hard to explain as the functions these systems represent, mapping from inputs to outputs, are not transparent. 
In particular, although it may be possible for an expert to broadly understand the structure of the model, the parameter values, which have been learned, are yet to be explained.
Several factors cause interpretability issues for self-driving machine learning models.
First, a finite training dataset cannot exhaustively cover all possible driving situations. It will likely under- and over-represent some specific cases, and questions such as &lt;em&gt;Has the model encountered situations like X?&lt;/em&gt; are legitimate. 
Moreover, datasets contain numerous biases of various nature (omitted variable bias, cause-effect bias, sampling bias), which also gives rise to explainability issues related to fairness.
Second, the mapping function represented by the trained model is poorly understood and is considered as a &lt;em&gt;black-box&lt;/em&gt;. The model is highly non-linear and does not provide any robustness guarantee as small input changes may dramatically change the output behavior. 
Explainability issues thus occur regarding the generalizability and robustness aspects: &lt;em&gt;How will the model behave under these new scenarios?&lt;/em&gt; 
Third, the learning phase is not perfectly understood. Among other things, there are no guarantees that the model will settle at a minimum point that generalizes well to new situations. Thus, the model may learn to ground its decisions on spurious correlations during training instead of on the true causes. We aim at finding answers to questions like &lt;em&gt;Which factors caused this decision to be taken?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/explainable_driving/ml_challenges.png&quot; alt=&quot;ml_challenges&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 2. Explainability hurdles and questions for autonomous driving models, as seen from a machine learning point of view.&lt;/b&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;From a &lt;strong&gt;driving perspective&lt;/strong&gt;, it has been shown that humans tackle this task by solving many intermediate sub-problems, at different levels of hierarchy &lt;a class=&quot;citation&quot; href=&quot;#michon1984critical&quot;&gt;(Michon, 1984)&lt;/a&gt;.
In the effort towards building an autonomous driving system, researchers aim at providing the machine with these intermediate capabilities. Thus, explaining the general behavior of an autonomous vehicle inevitably requires understanding how each of these intermediate steps is carried and how it interacts with others. We can categorize these capabilities into three types:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Perception&lt;/em&gt;: information about the system’s understanding of its local environment. This includes the objects that have been recognized and assigned to a semantic label (persons, cars, urban furniture, driveable area, crosswalks, traffic lights), their localization, properties of their motion (velocity, acceleration), intentions of other agents, &lt;em&gt;etc&lt;/em&gt;.;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Reasoning&lt;/em&gt;: information about how the different components of the perceived environment are organized and assembled by the system. This includes global explanations about the rules that are learned by the model, instance-wise explanation showing which objects are relevant in a given scene, traffic pattern recognition, object occlusion reasoning, &lt;em&gt;etc.&lt;/em&gt;;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Decision&lt;/em&gt;: information about how the system processes the perceived environment and its associated reasoning to produce a decision. This decision can be a high-level goal stating that the car should turn right, a prediction of the ego vehicle’s trajectory, its low-level relative motion or even the raw controls, &lt;em&gt;etc&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/explainable_driving/driving_challenges.png&quot; alt=&quot;driving_challenges&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 3. Explainability hurdles and questions for autonomous driving models, as seen from an autonomous driving point of view.&lt;/b&gt;&lt;/div&gt;

&lt;p&gt;While the separation between perception, reasoning, and decision is clear in modular driving systems, some recent end-to-end neural networks such as PilotNet &lt;a class=&quot;citation&quot; href=&quot;#pilotnet&quot;&gt;(Bojarski et al., 2016)&lt;/a&gt; blur the lines and perform these simultaneously. Indeed, when an explanation method is developed for a neural driving system, it is often not clear whether it attempts to explain the perception, the reasoning, or the decision step.
Considering the nature of neural networks architecture and training, disentangling perception, reasoning, and decision in neural driving systems constitutes a non-trivial challenge.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;As an answer to such problems, many explanation methods have been proposed and are usually organized into two categories: applying &lt;em&gt;post-hoc methods&lt;/em&gt; on an already-trained driving model, and directly building driving models which are inherently interpretable &lt;em&gt;by design&lt;/em&gt;. 
In our &lt;a href=&quot;https://arxiv.org/abs/2101.05307&quot;&gt;survey&lt;/a&gt;, we provide details on existing explainability techniques, show how they tackle to the problem of explaining driving models and highlight their limitations. 
In addition, we detail remaining challenges and open research avenues to increase explainability of self-driving models.
We hope our survey will enable increased awareness in this area from researchers and practitioners in the field, as well as from other potentially related fields.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;RosenfeldR19&quot;&gt;Rosenfeld, A., &amp;amp; Richardson, A. (2019). Explainability in human-agent systems. &lt;i&gt;Auton. Agents Multi Agent Syst.&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;doshi2017accountability&quot;&gt;Doshi-Velez, F., &amp;amp; Kortz, M. A. (2017). Accountability of AI Under the Law: The Role of Explanation. &lt;i&gt;CoRR&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;beaudouin2020identifying&quot;&gt;Beaudouin, V., Bloch, I., Bounie, D., Clémençon, S., d’Alché-Buc, F., Eagan, J., Maxwell, W., Mozharovskyi, P., &amp;amp; Parekh, J. (2020). Flexible and Context-Specific AI Explainability: A Multidisciplinary
               Approach. &lt;i&gt;CoRR&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;GilpinBYBSK18&quot;&gt;Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., &amp;amp; Kagal, L. (2018). Explaining Explanations: An Overview of Interpretability of Machine
               Learning. &lt;i&gt;DSSA&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;trusthci20&quot;&gt;Zhang, Q., Yang, X. J., &amp;amp; Robert, L. P. (2020). Expectations and Trust in Automated Vehicles. &lt;i&gt;CHI&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;haspiel2018explanations&quot;&gt;Haspiel, J., Du, N., Meyerson, J., Jr., L. P. R., Tilbury, D. M., Yang, X. J., &amp;amp; Pradhan, A. K. (2018). Explanations and Expectations: Trust Building in Automated Vehicles. &lt;i&gt;HRI&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;du2019look&quot;&gt;Du, N., Haspiel, J., Zhang, Q., Tilbury, D., Pradhan, A. K., Yang, X. J., &amp;amp; Robert Jr, L. P. (2019). Look who’s talking now: Implications of AV’s explanations on driver’s trust, AV preference, anxiety and mental workload. &lt;i&gt;Transportation Research Part C: Emerging Technologies&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;survey_sensors&quot;&gt;Yurtsever, E., Lambert, J., Carballo, A., &amp;amp; Takeda, K. (2020). A Survey of Autonomous Driving: Common Practices and Emerging Technologies. &lt;i&gt;IEEE Access&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;deepdrivingaffordance&quot;&gt;Chen, C., Seff, A., Kornhauser, A. L., &amp;amp; Xiao, J. (2015). DeepDriving: Learning Affordance for Direct Perception in Autonomous
               Driving. &lt;i&gt;ICCV&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;marinaffordance&quot;&gt;Toromanoff, M., Émilie Wirbel, &amp;amp; Moutarde, F. (2020). End-to-End Model-Free Reinforcement Learning for Urban Driving Using
               Implicit Affordances. &lt;i&gt;CVPR&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Pomerleau88&quot;&gt;Pomerleau, D. (1988). ALVINN: An Autonomous Land Vehicle in a Neural Network. &lt;i&gt;NIPS&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;pilotnet&quot;&gt;Bojarski, M., Testa, D. D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., Jackel, L. D., Monfort, M., Muller, U., Zhang, J., Zhang, X., Zhao, J., &amp;amp; Zieba, K. (2016). End to End Learning for Self-Driving Cars. &lt;i&gt;CoRR&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;carla&quot;&gt;Dosovitskiy, A., Ros, G., Codevilla, F., López, A., &amp;amp; Koltun, V. (2017). CARLA: An Open Urban Driving Simulator. &lt;i&gt;CoRL&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;michon1984critical&quot;&gt;Michon, J. A. (1984). &lt;i&gt;A Critical View of Driver Behavior Models: What Do We Know, what Should We Do?&lt;/i&gt; Human behavior and traffic safety.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name></name></author><summary type="html">This post is an introduction to our survey on the explainability of vision-based driving systems, which can be found on arXiv here.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io/blog/images/posts/explainable_driving/logo_explainable.png" /><media:content medium="image" url="https://valeoai.github.io/blog/images/posts/explainable_driving/logo_explainable.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving</title><link href="https://valeoai.github.io/blog/2020/11/26/plop-trajectory-prediction.html" rel="alternate" type="text/html" title="PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving" /><published>2020-11-26T00:00:00-06:00</published><updated>2020-11-26T00:00:00-06:00</updated><id>https://valeoai.github.io/blog/2020/11/26/plop-trajectory-prediction</id><content type="html" xml:base="https://valeoai.github.io/blog/2020/11/26/plop-trajectory-prediction.html">&lt;p&gt;&lt;em&gt;This post describes our &lt;a href=&quot;https://drive.google.com/file/d/1--QAL2sR7KMk9R4DwxyfJAT5iGCheFrn/view&quot;&gt;recent work&lt;/a&gt;
 on probabilistic trajectory prediction for autonomous driving presented
 at &lt;a href=&quot;https://www.robot-learning.org/home&quot;&gt;CORL 2020&lt;/a&gt;. PLOP is a trajectory prediction method that
intent to control an autonomous vehicle (ego vehicle) in urban environment while considering 
and predicting the intents of other road users (neighbors). We focus here on predicting
 multiple feasible future trajectories for both ego vehicle and neighbors through a 
probabilistic framework and rely on a conditional imitation learning algorithm, conditioned by a
 navigation command for the ego vehicle (e.g., ``turn right’’). Our model processes only onboard
sensor data (camera and lidars) along with detections of past and presents objects relaxing the
necessity of an HDMap and is computationally efficient as it can run in real time (25 fps) on 
an embedded board in the real vehicle. We evaluate our method offline on the
 publicly available dataset nuScenes &lt;a class=&quot;citation&quot; href=&quot;#holger2020nuscenes&quot;&gt;(Caesar et al., 2020)&lt;/a&gt;,
 achieving state-of-the-art performance, investigate 
the impact of our architecture choices on online simulated experiments and show preliminary
 insights for real vehicle control.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/plop/plop.png&quot; alt=&quot;plop_teaser&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 1. Qualitative example of trajectory predictions on a test
 sample from nuScenes dataset.&lt;/b&gt; The top image show a bird's eye view of PLOP's predictions
 for the ego and neighbor vehicles (to be compared with the ground truth in green). The bottom
 row present the input image (left) in which we added object correspondance with the bird's
 eye view and the auxiliary semantic segmentation of this image (right)&lt;/div&gt;

&lt;p&gt;Predicting the future positions of other agents of the road, or of the
 autonomous vehicle itself, is critical for autonomous driving.
  This trajectory prediction must not only respect the rules of
   the road, but capture the interactions of the agents over time.
    It is also important to allow multiple possible predictions, as
     there is usually not a single valid trajectory.&lt;/p&gt;

&lt;p&gt;Some approaches such as ChauffeurNet &lt;a class=&quot;citation&quot; href=&quot;#bansal2018chauffeutnet&quot;&gt;(Bansal et al., 2018)&lt;/a&gt; use a high-levelscene representation 
(road map, traffic lights, speed limit, route, dynamic bounding boxes, etc.).
 More recently, MultiPath &lt;a class=&quot;citation&quot; href=&quot;#chai2019multipath&quot;&gt;(Chai et al., 2019)&lt;/a&gt; uses trajectory anchors, used in one-step object detection,
 extracted from the training data for ego vehicle prediction. &lt;a class=&quot;citation&quot; href=&quot;#hong2019rules&quot;&gt;(Hong et al., 2019)&lt;/a&gt; use a high level
 representation which includes some dynamic context.
In contrast, we choose to leverage also low level sensor data, here Lidar
 point clouds and camera image. In that domain, recent approaches address
 the variation in agent behaviors by predicting multiple trajectories, often
 in a stochastic way. Many works, e.g., PRECOG &lt;a class=&quot;citation&quot; href=&quot;#rhinehart2019precog&quot;&gt;(Rhinehart et al., 2019)&lt;/a&gt;,
 MFP &lt;a class=&quot;citation&quot; href=&quot;#tang2019mfp&quot;&gt;(Tang &amp;amp; Salakhutdinov, 2019)&lt;/a&gt;, SocialGAN &lt;a class=&quot;citation&quot; href=&quot;#gupta2018socialgan&quot;&gt;(Gupta et al., 2018)&lt;/a&gt; 
and others &lt;a class=&quot;citation&quot; href=&quot;#rhinehart2018deepim&quot;&gt;(Rhinehart et al., 2018)&lt;/a&gt;,
 focus on this aspect through a probabilistic framework on the network output
 or latent representations, producing multiple trajectories for ego vehicle, nearby vehicles 
or both. &lt;a class=&quot;citation&quot; href=&quot;#phan2020covernet&quot;&gt;(Phan-Minh et al., 2020)&lt;/a&gt; generate a trajectory set, then classify 
correct trajectories.
 &lt;a class=&quot;citation&quot; href=&quot;#marchetti2020mantra&quot;&gt;(Marchetti et al., 2020)&lt;/a&gt; generate multiple futures from encodings of similar 
trajectories stored
 in a memory. &lt;a class=&quot;citation&quot; href=&quot;#ohn2020learning&quot;&gt;(Ohn-Bar et al., 2020)&lt;/a&gt; learn a weighted mixture of expert policies trained to mimic
 agents with specific behaviors. In PRECOG, &lt;a class=&quot;citation&quot; href=&quot;#rhinehart2019precog&quot;&gt;(Rhinehart et al., 2019)&lt;/a&gt; advance a 
probabilistic 
formulation that explicitly models interactions between agents, using latent variables
 to model their plausible reactions, with the possibility to precondition the trajectory 
of the ego vehicle by a goal.&lt;/p&gt;

&lt;h2 id=&quot;plop-method&quot;&gt;PLOP method&lt;/h2&gt;

&lt;h3 id=&quot;contributions&quot;&gt;Contributions&lt;/h3&gt;

&lt;p&gt;Our main goal is to produce a trajectory prediction which can be used to drive the 
ego vehicle relying on a conditional imitation learning algorithm, conditioned by 
a navigation command for the ego vehicle (e.g., “turn right”).
To do so, we propose  a single-shot, anchor-less trajectory prediction method,
 based on Mixture Desity Networks (MDNs) and polynomial trajectory constraints, 
relying only on on-board sensors which relaxes the HD map requirement and allow 
more flexibility for driving in the real world.
The polynomial formulation ensures that the predicted
 trajectories are coherent and smooth, while providing more
 learning flexibility through the extra parameters. We find
 that this mitigates training instability and mode collapse
 that are common to MDNs &lt;a class=&quot;citation&quot; href=&quot;#cui2019multimodal&quot;&gt;(Cui et al., 2019)&lt;/a&gt;.
PLOP is trainable end-to-end from imitation learning,
 where data is relatively easier to obtain and it is 
computationally efficient during both training and inference as it predicts
 trajectory coefficients in a single step, without requiring a RNN-based decoder. 
The polynomial function trajectory coefficients eschew the need for anchors &lt;a class=&quot;citation&quot; href=&quot;#chai2019multipath&quot;&gt;(Chai et al., 2019)&lt;/a&gt;,
 whose quality can vary across datasets.&lt;/p&gt;

&lt;p&gt;We propose an extensive evaluation of PLOP and show its effectiveness across datasets 
and settings. We conduct a comparison showing the improvement over state-of-the-art 
PRECOG &lt;a class=&quot;citation&quot; href=&quot;#rhinehart2019precog&quot;&gt;(Rhinehart et al., 2019)&lt;/a&gt; on the public dataset nuScenes &lt;a class=&quot;citation&quot; href=&quot;#holger2020nuscenes&quot;&gt;(Caesar et al., 2020)&lt;/a&gt;;&lt;/p&gt;

&lt;p&gt;Then for a better evaluation of the driving capacities of PLOP, 
we study closed loop performance for the ego vehicle, on simulation
 and with preliminary insights for real vehicle control.&lt;/p&gt;

&lt;h3 id=&quot;network-architecture&quot;&gt;Network architecture&lt;/h3&gt;

&lt;p&gt;PLOP takes as inputs: the ego and neighbor vehicles past positions
 represented as time sequences of x and y over the last 2 seconds,
 the frontal camera image of the ego vehicle, and 2 second history 
of bird’s eye views with a cell resolution of 1m square containing 
the lidar point cloud and the object detections information represented 
in Figure 2. The objects detections being the output of a state of the art
 perception algorithm.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/plop/inputs_plop_post.PNG&quot; alt=&quot;plop_inputs&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 2. Image and Bird's eye view.&lt;/b&gt; The left image is an example of a front camera input image of PLOP and the diagram on the right is a representation of the bird'eye view input.&lt;/div&gt;

&lt;p&gt;We pass these inputs through a multibranch neural network represented 
in Figure 3 to predict the ego vehicle future trajectory and two auxiliary 
tasks that are the future trajectory prediction for the neighbors vehicles 
and the semantic segmentation of the camera image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/plop/archi_outputs.png&quot; alt=&quot;plop_archi_outputs&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 3. PLOP's Architecture.&lt;/b&gt; PLOP's architecture is reprented on the left while the polynomial multimodal gaussian trajectory representation is on the right&lt;/div&gt;

&lt;p&gt;The front camera image features, the bird’s eye view features and the ego vehicle 
past positions features are passed down to conditional fully connected architecture
 to output multiple future trajectories for the ego vehicle regarding the 
 current navigation order. The trajectories are predicted using MDNs where
 gaussian means are generated using polynomial functions of degree 4 over x and y&lt;/p&gt;

&lt;p&gt;To improve the learning stability of our training and inject awareness about
the scene layouts into the camera features 
we pass them through a U-Net decoder to output semantic segmentation and then use 
an auxiliary cross entropy loss.
To improve the encoding of interactions between the differents agents of the scene 
in the bird’s eye features,
 we predict the future possible trajectories for each neighbor feeding the bird’s
 eye views encoding and its past positions encoded through a LSTM layer to a small
  fully connected network. The weights of LSTMs and fully connected layers are shared between
   all neighbors. This output allows us to get useful information about the ego
    vehicle environment that can be used online to improve the ego vehicle driving
     with safety collision checks for example.&lt;/p&gt;

&lt;h2 id=&quot;offline-evaluation&quot;&gt;Offline evaluation&lt;/h2&gt;

&lt;p&gt;To evaluate PLOP, we use the nuScenes dataset to train the trajectory loss 
along with the Audi &lt;a class=&quot;citation&quot; href=&quot;#geyer2019a2d2&quot;&gt;(Geyer et al., 2019)&lt;/a&gt; dataset to train the semantic segmentation loss. 
We choose to compare our method with the DESIRE &lt;a class=&quot;citation&quot; href=&quot;#lee2017desire&quot;&gt;(Lee et al., 2017)&lt;/a&gt; baseline and against 
two state of the art methods that are PRECOG and ESP &lt;a class=&quot;citation&quot; href=&quot;#rhinehart2019precog&quot;&gt;(Rhinehart et al., 2019)&lt;/a&gt;
 using the minimum
 Mean Squared Deviation metric to avoid penalizing valid trajectories that
 are not matching the ground truth. For one agent, meaning ego vehicle only, 
 PRECOG and ESP have access to the future desired target position and PRECOG 
 return significantly better results than PLOP but PLOP still reaches similar 
 results as ESP. For multiple agents PLOP outperforms other presented methods
 . We note that the comparison if fairer for neighbor trajectories and the
  performance is relevant since they are by definition open loop.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/plop/offline_results.PNG&quot; alt=&quot;plop_offline_results&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 4. Comparison with state-of-the-art:&lt;/b&gt; Against the DESIRE, ESP and PRECOG
 for predicting a trajectory of 4 seconds into the future&lt;/div&gt;

&lt;p&gt;But we argue that such evaluation is not totally relevant for controling the ego 
vehicle in real conditions. 
Such metrics does not value the situations in which the errors are made, failing to brake
at a traffic light is a critical error for example but it is 
quick and represent a very small part of the test set so it will impact very poorly the 
overall metrics. However, making a small constant error such as driving 2kph too slow over 
the whole test set set might be an acceptable and non impacting error but will lead to a 
considerable overall error. Also, using only offline metrics where the method can’t
control the vehicle does not allow us to evaluate its capacities to react to its own
mistakes.&lt;/p&gt;

&lt;h2 id=&quot;online-evaluation-through-simulation&quot;&gt;Online Evaluation through simulation&lt;/h2&gt;

&lt;p&gt;To simulate driving, we developped a data driven simulator that 
allows us to use real driving data to simulate applying
the prediction to the ego vehicle. We can generate the input data that corresponds
to the new vehicle position after following the trajectory using reprojections (for 
the image and the pointcloud), then use it to predict 
a new trajectory, and so on. This allows us the measure the performance in closed
 loop, and in particular to count failures which would have resulted in a takeover. 
 We rely on 3 metrics: lateral (&amp;gt;1m from expert), high speed (catching up to a vehicle
 15% faster than the real vehicle up to 0.6s in the future) and low speed (&amp;gt; 20kph
 under the expert speed) errors count.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/plop/simu_plop.png&quot; alt=&quot;plop_online_results&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 5. Evaluation using the simulator.&lt;/b&gt; Comparison with PLOP 
without semantic segmentation loss, Constant velocity baseline and Multi-Layer
 Perceptron baseline in the table on the left. Additionnal qualitative
 results about the errors positioning on the differents test tracks are on
 the right.&lt;/div&gt;

&lt;p&gt;We trained PLOP on an internal dataset combining both open road and urban test
track and compared PLOP, PLOP without auxiliary semantic loss, the constant velocity 
baseline and a MLP baseline in our simulator using test data. We note that semantic
segmentation improve the driving performance and that MLP has better offline metrics
than constant velocity approach but still perform worse due to the simulated driving
conditions.
As expected, offline metrics are not discriminating enough for the online behavior
since the best model checkpoints in simulation are not necessarily the ones with 
the better offline metrics. An additionnal ablation study where we remove mandatory 
information (such as the camera image input) shows that it may even be dangerous 
to trust them blindly.&lt;/p&gt;

&lt;div class=&quot;publication-teaser&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/94FwahFmc5A?start=94&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this work, we demonstrate the interest of our multi-input multimodal 
approach PLOP for vehicle trajectory prediction in an urban environment. 
Our architecture leverages frontal camera and Lidar inputs, to produce multiple 
trajectories using reparameterized Mixture Density Networks, with an auxiliary 
semantic segmentation task. We show that we can improve open loop state-of-the-art 
performance in a multi-agent system, by evaluating the vehicle trajectories from the 
nuScenes dataset. We also provide a simulated closed loop evaluation, to go towards
 real vehicle online application. Please check out our paper along with supplementary materials 
for greater details about our approach and experiments and feel free to contact us for any
question.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;holger2020nuscenes&quot;&gt;Caesar, H., Bankiti, V., Lang, A. H., Vora, S., Liong, V. E., Xu, Q., Krishnan, A., Pan, Y., Baldan, G., &amp;amp; Beijbom, O. (2020). nuScenes: A Multimodal Dataset for Autonomous Driving. &lt;i&gt;Cvpr&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bansal2018chauffeutnet&quot;&gt;Bansal, M., Krizhevsky, A., &amp;amp; Ogale, A. S. (2018). ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst. &lt;i&gt;CoRR&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;chai2019multipath&quot;&gt;Chai, Y., Sapp, B., Bansal, M., &amp;amp; Anguelov, D. (2019). &lt;i&gt;MultiPath: Multiple Probabilistic Anchor Trajectory Hypotheses for Behavior Prediction&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hong2019rules&quot;&gt;Hong, J., Sapp, B., &amp;amp; Philbin, J. (2019). Rules of the Road: Predicting Driving Behavior with a Convolutional Model of Semantic Interactions. &lt;i&gt;CoRR&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rhinehart2019precog&quot;&gt;Rhinehart, N., McAllister, R., Kitani, K., &amp;amp; Levine, S. (2019). Precog: Prediction conditioned on goals in visual multi-agent settings. &lt;i&gt;Iccv&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;tang2019mfp&quot;&gt;Tang, C., &amp;amp; Salakhutdinov, R. R. (2019). Multiple futures prediction. &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;, 15424–15434.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;gupta2018socialgan&quot;&gt;Gupta, A., Johnson, J., Fei-Fei, L., Savarese, S., &amp;amp; Alahi, A. (2018). Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks. &lt;i&gt;CoRR&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rhinehart2018deepim&quot;&gt;Rhinehart, N., McAllister, R., &amp;amp; Levine, S. (2018). Deep Imitative Models for Flexible Inference, Planning, and Control. &lt;i&gt;CoRR&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;phan2020covernet&quot;&gt;Phan-Minh, T., Grigore, E. C., Boulton, F. A., Beijbom, O., &amp;amp; Wolff, E. M. (2020). Covernet: Multimodal behavior prediction using trajectory sets. &lt;i&gt;Cvpr&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;marchetti2020mantra&quot;&gt;Marchetti, F., Becattini, F., Seidenari, L., &amp;amp; Bimbo, A. D. (2020). Mantra: Memory augmented networks for multiple trajectory prediction. &lt;i&gt;Cvpr&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ohn2020learning&quot;&gt;Ohn-Bar, E., Prakash, A., Behl, A., Chitta, K., &amp;amp; Geiger, A. (2020). Learning Situational Driving. &lt;i&gt;Cvpr&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;cui2019multimodal&quot;&gt;Cui, H., Radosavljevic, V., Chou, F.-C., Lin, T.-H., Nguyen, T., Huang, T.-K., Schneider, J., &amp;amp; Djuric, N. (2019). Multimodal trajectory predictions for autonomous driving using deep convolutional networks. &lt;i&gt;Icra&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;geyer2019a2d2&quot;&gt;Geyer, J., Kassahun, Y., Mahmudi, M., Ricou, X., Durgesh, R., Chung, A. S., Hauswald, L., Pham, V. H., Mühlegg, M., Dorn, S., Fernandez, T., Jänicke, M., Mirashi, S., Savani, C., Sturm, M., Vorobiov, O., &amp;amp; Schuberth, P. (2019). &lt;i&gt;A2D2: AEV Autonomous Driving Dataset&lt;/i&gt;. http://www.a2d2.audi&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;lee2017desire&quot;&gt;Lee, N., Choi, W., Vernaza, P., Choy, C., Torr, P., &amp;amp; Chandraker, M. (2017). &lt;i&gt;DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents&lt;/i&gt;. 2165–2174. https://doi.org/10.1109/CVPR.2017.233&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name></name></author><summary type="html">This post describes our recent work on probabilistic trajectory prediction for autonomous driving presented at CORL 2020. PLOP is a trajectory prediction method that intent to control an autonomous vehicle (ego vehicle) in urban environment while considering and predicting the intents of other road users (neighbors). We focus here on predicting multiple feasible future trajectories for both ego vehicle and neighbors through a probabilistic framework and rely on a conditional imitation learning algorithm, conditioned by a navigation command for the ego vehicle (e.g., ``turn right’’). Our model processes only onboard sensor data (camera and lidars) along with detections of past and presents objects relaxing the necessity of an HDMap and is computationally efficient as it can run in real time (25 fps) on an embedded board in the real vehicle. We evaluate our method offline on the publicly available dataset nuScenes (Caesar et al., 2020), achieving state-of-the-art performance, investigate the impact of our architecture choices on online simulated experiments and show preliminary insights for real vehicle control.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io/blog/images/publications/plop/plop.png" /><media:content medium="image" url="https://valeoai.github.io/blog/images/publications/plop/plop.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Is Deep Reinforcement Learning Really Superhuman on Atari?</title><link href="https://valeoai.github.io/blog/2020/10/19/saber.html" rel="alternate" type="text/html" title="Is Deep Reinforcement Learning Really Superhuman on Atari?" /><published>2020-10-19T00:00:00-05:00</published><updated>2020-10-19T00:00:00-05:00</updated><id>https://valeoai.github.io/blog/2020/10/19/saber</id><content type="html" xml:base="https://valeoai.github.io/blog/2020/10/19/saber.html">&lt;p&gt;&lt;em&gt;This post describes our recent &lt;a href=&quot;https://arxiv.org/pdf/1908.04683.pdf&quot;&gt;work&lt;/a&gt; on Deep Reinforcement Learning (DRL) on the Atari benchmark. DRL appears today as one of the closest paradigm to Artificial General Intelligence and large progress in this field has been enabled by the popular Atari benchmark. However, training and evaluation protocols on Atari vary across papers leading to biased comparisons, difficulty in reproducing results and in estimating the true contributions of the methods. Here we attempt to mitigate this problem with SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms. SABER allows us to compare multiple methods under the same conditions against a human baseline and to note that previous claims of superhuman performance on DRL do not hold. Finally, we propose a new state-of-the-art algorithm R-IQN combining Rainbow with Implicit Quantile Networks (IQN). Our code is &lt;a href=&quot;https://github.com/valeoai/rainbow-iqn-apex&quot;&gt;available&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Deep Reinforcement Learning is a learning scheme based on trial-and-error in which an agent learns an optimal policy from its own experiments and a reward signal. The goal of the agent is to maximize the sum of future accumulated rewards and thus the agent needs to think about sequences of actions rather than instantaneous ones. The Atari benchmark is valuable for evaluating general AI algorithms as it includes more than 50 games displaying high variability in the task to solve, ranging from simple paddle control in the ball game Pong to complex labyrinth exploration in Montezuma’s Revenge which remains unsolved by general algorithms up to today.&lt;/p&gt;

&lt;p&gt;We notice however that training and evaluation procedures on Atari can be different from paper to paper and thus leading to bias in comparison. Moreover this leads to difficulties to reproduce results of published works as some training or evaluation parameters are barely explained or sometimes not mentioned. In order to facilitate reproducible and comparable DRL, we introduce SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms. Furthermore, we introduce a human world record baseline and argue that previous claims of superhuman performance of DRL might not hold. Finally, we propose a new state-of-the-art algorithm R-IQN by combining the current state-of-the-art &lt;strong&gt;Rainbow&lt;/strong&gt; &lt;a class=&quot;citation&quot; href=&quot;#hessel2017rainbow&quot;&gt;(Hessel et al., 2018)&lt;/a&gt; along with Implicit Quantile Networks (&lt;strong&gt;IQN&lt;/strong&gt; &lt;a class=&quot;citation&quot; href=&quot;#dabney2018implicit&quot;&gt;(Dabney et al., 2018)&lt;/a&gt;). We release an open-source &lt;a href=&quot;https://github.com/valeoai/rainbow-iqn-apex&quot;&gt;implementation&lt;/a&gt; of distributed R-IQN following the ideas from Ape-X!&lt;/p&gt;

&lt;h2 id=&quot;dqns-human-baseline-vs-human-world-record-on-atari-games&quot;&gt;DQN’s human baseline vs human world record on Atari Games&lt;/h2&gt;

&lt;p&gt;A common way to evaluate AI for games is to let agents compete against the best humans. Recent examples for DRL include the victory of &lt;a href=&quot;https://deepmind.com/alphago-korea&quot;&gt;AlphaGo versus Lee Sedol&lt;/a&gt; for Go, &lt;a href=&quot;https://openai.com/blog/openai-five/&quot;&gt;OpenAI Five&lt;/a&gt; on Dota 2 or &lt;a href=&quot;https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii&quot;&gt;AlphaStar versus Mana&lt;/a&gt; for StarCraft 2. For this reason one of the most used metrics for evaluating RL agents on Atari is to compare them to the human baseline introduced in DQN.&lt;/p&gt;

&lt;p&gt;Previous works use the normalized human score, &lt;em&gt;i.e&lt;/em&gt;., 0% is the score of a random player and 100% is the score of the human baseline, which allows one to summarize the performance on the whole Atari set in one number, instead of individually comparing raw scores for each of the 61 games. However we argue that this human baseline is far from being representative of the best human player, which means that using it to claim superhuman performance is misleading.&lt;/p&gt;

&lt;p&gt;The current world records are available online for 58 of the 61 evaluated Atari games. For example, on VideoPinball, the world record is 50,000 times higher than the human baseline of DQN. Evaluating these world records scores using the usual human normalized score has a median of 4,400% and a mean of 99,300% (see Figure below for details on each game), to be compared to 200% and 800% of the current state-of-the-art Rainbow!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/saber/pro_vs_beginner.png&quot; alt=&quot;pro_vs_beginner&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 1: World record scores vs. the usual beginner human baseline&lt;/b&gt; (log scale) &lt;a class=&quot;citation&quot; href=&quot;#dabney2018implicit&quot;&gt;(Dabney et al., 2018)&lt;/a&gt; baseline can be up to 50k times lower than registered world records. Beating that baseline does not necessarily make the agent superhuman.&lt;/div&gt;

&lt;p&gt;We estimate that evaluating the algorithms under the world record baseline instead of the DQN human baseline will give a better view of the gap remaining between best human players and DRL agents.
Results are confirming this, as Rainbow reaches only a median human-normalized score of 3% (see Figure 2 below) meaning that for half of Atari games, the agent doesn’t even reach 3% of the way from random to best human run.&lt;/p&gt;

&lt;p&gt;In the video below we analyze agents previously claimed as above human-level but far from the world record. By taking a closer look at the AI playing, we discovered that on most of Atari games DRL agents fail to understand the goal of the game. Sometimes they just don’t explore other levels than the initial one, sometimes they are stuck in a loop giving small amount of reward, etc. A compelling example of this is the game Riverraid. In this game, the agent must shoot everything and take fuel to survive: the agent dies if there is a collision with an enemy or if out of fuel. But as shooting fuel actually gives points, the agent doesn’t understand that he could play way longer and win even more points by actually taking this fuel bonus and not shooting them!&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/oH6P3ksYLek&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot; align=&quot;center&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;h2 id=&quot;saber-a-standardized-atari-benchmark-for-general-reinforcement-learning-algorithms&quot;&gt;SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms&lt;/h2&gt;

&lt;p&gt;In our work we extend the recommendations proposed by &lt;a class=&quot;citation&quot; href=&quot;#machado2018revisiting&quot;&gt;(Machado et al., 2018)&lt;/a&gt; They also point out divergences in training and evaluating agents on Atari. Consequently they compile and propose a set of recommendations for more reproducible and comparable RL including sticky actions, ignoring life signal and using full action set.&lt;/p&gt;

&lt;p&gt;There is however one major parameter that is left out in &lt;a class=&quot;citation&quot; href=&quot;#machado2018revisiting&quot;&gt;(Machado et al., 2018)&lt;/a&gt;: the maximum number of frames allowed per episode. This parameter ends the episode after a fixed number of time steps even if the game is not over. In most of the recent works, this is set to 30 min of game play (i.e., 108k frames) and only to 5 min in some others (i.e.,18k frames). This means that the reported scores can not be compared fairly. For example, in easy games (e.g., Atlantis), the agent never dies and the score is more or less linear with the allowed time: the reported score will be 6 times higher if capped at 30 minutes instead of 5 minutes.&lt;/p&gt;

&lt;p&gt;Another issue with this time cap comes from the fact that some games are designed for much longer gameplay than 5 or 30 minutes. On those games (e.g., Atlantis, Video Pinball, Enduro) the scores reported of Ape-X, Rainbow and IQN are almost the same. This is due to all agents reaching the time limit and getting the maximum possible score in 30 minutes: the difference in scores is due to minor variations, not algorithmic difference and thus the comparison is not significant. As a consequence, the more successful agents are, the more games are incomparable because they reach the maximum possible score in the time cap while still being far behind human world record.&lt;/p&gt;

&lt;p&gt;This parameter can also be a source of ambiguity and error. The best score on Atlantis (2,311,815) is reported by Proximal Policy Optimization by &lt;a class=&quot;citation&quot; href=&quot;#schulman2017proximal&quot;&gt;(Schulman et al., 2017)&lt;/a&gt;, however this score is likely wrong: it seems impossible to reach in 30 minutes only! The first distributional paper by (&lt;a href=&quot;http://proceedings.mlr.press/v70/bellemare17a.html&quot;&gt;Bellemare et al.&lt;/a&gt;) also did this mistake and reported wrong results before adding an erratum in a later version on &lt;em&gt;ArXiv&lt;/em&gt; &lt;a class=&quot;citation&quot; href=&quot;#bellemare2017distributional&quot;&gt;(Bellemare et al., 2017)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/saber/saber_params.png&quot; alt=&quot;saber_params&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Table 1:&lt;/b&gt; Game parameters of SABER.&lt;/div&gt;

&lt;p&gt;We first re-implemented the current state-of-the-art Rainbow and evaluated it on SABER. We noticed that the same algorithm under different evaluation settings can lead to significantly different results. This showed again the necessity of a common and standardized benchmark, more details can be found in the paper.&lt;/p&gt;

&lt;p&gt;Then we implemented a new algorithm, R-IQN, by replacing the C51 algorithm (which is one of the 6 components of Rainbow) by Implicit Quantile Network (IQN). Both C51 and IQN belong to the field of Distributional RL which aims to predict the full distribution of the Q-value function instead of just predicting the mean of it. The fundamental difference between these two algorithms is how they parametrize the Q-value distribution. C51, which is the first algorithm of Distributional RL, approximates the Q-value as a categorical distribution with fixed support and just learns the mass to attribute to each category. On the other hand, IQN approximates the Q-value with quantile regression and both the support and the mass arelearned resulting in a major improvement in performance and data-efficiency over C51. As IQN arises much better performance than C51 while still designed for the same goal (predict the full distribution of the Q-function), combining Rainbow with IQN is relevant and natural.&lt;/p&gt;

&lt;p&gt;As shown in the graph below, R-IQN outperforms Rainbow and thus becomes the new state-of-the-art on Atari. However, we acknowledge that in order to make a more confident state-of-the-art claim we should run multiple times with different seeds. Testing an increased number of random seeds across the 60 Atari games is a computationally costly endeavor and is beyond the scope of this study. We test the stability of the performances of R-IQN across 5 random seeds on a subset of 14 games. We compare against Rainbow and report similar results in Figure 3.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/saber/median_human_normalised_Rainbow_vs_Rainbow_IQN.png&quot; alt=&quot;median_rainbow_vs_us&quot; height=&quot;90%&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 2: Comparison of Rainbow and Rainbow-IQN on SABER.&lt;/b&gt; We report median normalized scores w.r.t training steps.&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/saber/median_human_normalised_5_seeds_both_rainbow_riqn.png&quot; alt=&quot;median_rainbow_vs_us_seeds&quot; height=&quot;90%&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 3: Comparison of Rainbow and Rainbow-IQN on a subset of 14 games using 5 seeds.&lt;/b&gt; We report median normalized scores w.r.t training steps.&lt;/div&gt;

&lt;h2 id=&quot;open-source-implementation-of-distributed-rainbow-iqn-r-iqn-ape-x&quot;&gt;Open-source implementation of distributed Rainbow-IQN: R-IQN Ape-X&lt;/h2&gt;

&lt;p&gt;We release our code of a distributed version of Rainbow-IQN following ideas from Ape-X &lt;a class=&quot;citation&quot; href=&quot;#horgan2018distributed&quot;&gt;(Horgan et al., 2018)&lt;/a&gt;. The distributed part is our main practical advantage over some existing DRL repositories (particularly &lt;a href=&quot;https://github.com/google/dopamine&quot;&gt;Dopamine&lt;/a&gt; a popular open-source implementation of DQN, C51, IQN and a &lt;em&gt;small-Rainbow&lt;/em&gt; but in which all algorithms are single worker).
Indeed, when using DRL algorithms for other tasks (other than Atari and MuJoCO) a major bottleneck is the &lt;em&gt;speed&lt;/em&gt; of the environment. DRL algorithms often need a huge amount of data before reaching reasonable performance. This amount may be practically impossible to reach if the environment is real-time and if collecting data from multiple environments at the same time is not possible.&lt;/p&gt;

&lt;p&gt;Building upon ideas from Ape-X, we use REDIS as a side server to store our replay memory. Multiple actors will act in their own instances of the environment to fill as fast as they can the replay memory. Finally, the learner will sample from this replay memory (the learner is actually completely independent of the environment) for backprop. The learner will also periodically update the weight of each actor as shown in the schema below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/saber/appex_arch.png&quot; alt=&quot;apex_arch&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 4: Ape-X architecture.&lt;/b&gt; image taken from &lt;a class=&quot;citation&quot; href=&quot;#horgan2018distributed&quot;&gt;(Horgan et al., 2018)&lt;/a&gt;&lt;/div&gt;

&lt;p&gt;This scheme allowed us to use R-IQN Ape-X for the task of autonomous driving using &lt;a href=&quot;http://carla.org/&quot;&gt;CARLA&lt;/a&gt; as environment. This enabled us to win the &lt;a href=&quot;https://carlachallenge.org/results-challenge-2019/&quot;&gt;CARLA Challenge&lt;/a&gt; on Track 2 &lt;em&gt;Cameras Only&lt;/em&gt; showing the strength of R-IQN Ape-X as a general algorithm.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this work, we confirm the impact of standardized guidelines for DRL evaluation, and build a consolidated benchmark, SABER. In order to provide a more significant comparison, we build a new baseline based on human world records and show that the state-of-the-art Rainbow agent is in fact far from human world record performance. In the paper we share possible reasons for this failure. We hope that SABER will facilitate better comparisons and enable new exciting methods to prove their effectiveness without ambiguity.&lt;/p&gt;

&lt;p&gt;Check out our paper to find out more about intuitions, experiments and interpretations.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;hessel2017rainbow&quot;&gt;Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., &amp;amp; Silver, D. (2018). Rainbow: Combining improvements in deep reinforcement learning. &lt;i&gt;AAAI&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;dabney2018implicit&quot;&gt;Dabney, W., Ostrovski, G., Silver, D., &amp;amp; Munos, R. (2018). Implicit quantile networks for distributional reinforcement learning. &lt;i&gt;ICML&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;machado2018revisiting&quot;&gt;Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., &amp;amp; Bowling, M. (2018). Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. &lt;i&gt;Journal of Artificial Intelligence Research&lt;/i&gt;, &lt;i&gt;61&lt;/i&gt;, 523–562.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;schulman2017proximal&quot;&gt;Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp;amp; Klimov, O. (2017). Proximal policy optimization algorithms. &lt;i&gt;ArXiv Preprint ArXiv:1707.06347&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bellemare2017distributional&quot;&gt;Bellemare, M. G., Dabney, W., &amp;amp; Munos, R. (2017). A distributional perspective on reinforcement learning. &lt;i&gt;ArXiv Preprint ArXiv:1707.06887&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;horgan2018distributed&quot;&gt;Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., Van Hasselt, H., &amp;amp; Silver, D. (2018). Distributed prioritized experience replay. &lt;i&gt;ArXiv Preprint ArXiv:1803.00933&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name></name></author><summary type="html">This post describes our recent work on Deep Reinforcement Learning (DRL) on the Atari benchmark. DRL appears today as one of the closest paradigm to Artificial General Intelligence and large progress in this field has been enabled by the popular Atari benchmark. However, training and evaluation protocols on Atari vary across papers leading to biased comparisons, difficulty in reproducing results and in estimating the true contributions of the methods. Here we attempt to mitigate this problem with SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms. SABER allows us to compare multiple methods under the same conditions against a human baseline and to note that previous claims of superhuman performance on DRL do not hold. Finally, we propose a new state-of-the-art algorithm R-IQN combining Rainbow with Implicit Quantile Networks (IQN). Our code is available.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io/blog/images/posts/saber/space_invaders.jpg" /><media:content medium="image" url="https://valeoai.github.io/blog/images/posts/saber/space_invaders.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">ADVENT - Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation</title><link href="https://valeoai.github.io/blog/2020/07/07/advent-domain-adaptation.html" rel="alternate" type="text/html" title="ADVENT - Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation" /><published>2020-07-07T00:00:00-05:00</published><updated>2020-07-07T00:00:00-05:00</updated><id>https://valeoai.github.io/blog/2020/07/07/advent-domain-adaptation</id><content type="html" xml:base="https://valeoai.github.io/blog/2020/07/07/advent-domain-adaptation.html">&lt;p&gt;&lt;em&gt;This post describes our &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/html/Vu_ADVENT_Adversarial_Entropy_Minimization_for_Domain_Adaptation_in_Semantic_Segmentation_CVPR_2019_paper.html&quot;&gt;recent work&lt;/a&gt; on unsupervised domain adaptation for semantic segmentation presented at &lt;a href=&quot;http://cvpr2019.thecvf.com/&quot;&gt;CVPR 2019&lt;/a&gt;. ADVENT is a flexible technique for bridging the gap between two different domains through entropy minimization. Our work builds upon a simple observation: models trained only on source domain tend to produce over-confident, i.e., low-entropy, predictions on source-like images and under-confident, i.e., high-entropy, predictions on target-like ones. Consequently by minimizing the entropy on the target domain, we make the feature distributions from the two domains more similar. We show that our approach achieves competitive performances on standard semantic segmentation benchmarks and that it can be successfully extended to other tasks such as object detection.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Visual perception is a remarkable ability that human drivers leverage for understanding their surroundings and for supporting the multiple micro-decisions needed in traffic. Since many years, researchers have been working on mimicking this human capability by means of computer algorithms. This research field is known as computer vision and it has seen impressive progress and wide adoption. Most of the modern &lt;em&gt;computer vision&lt;/em&gt; systems rely on Deep Neural Networks (DNNs) which are powerful and widely employed tools able to learn from large amounts of data and make accurate predictions. In autonomous driving, DNN-based visual perception is also at the heart of the complex architectures under intelligent cars, and supports downstream decisions of the vehicle, &lt;em&gt;e.g.,&lt;/em&gt; steering, braking, signaling, etc.&lt;/p&gt;

&lt;p&gt;The diversity and complexity of the situations encountered in real-world driving is tremendous. Unlike humans who can extrapolate effortlessly from previous experience in order to adapt to new environments and conditions, the scope of DNNs beyond the types of conditions and scenes seen during training is limited. For instance a model trained on data from a sunny country, would have a hard time delivering the same performance on streets with mixed weather conditions in a different country (with different urban architecture, furniture, vegetation, types of cars and pedestrian appearance and clothing). Similarly a model trained on a particular type of camera, is expected to see a drop in performance with images coming from a camera with different specifications. This difference between environments that leads to performance drops is referred to as &lt;em&gt;domain gap&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;bridging-domains&quot;&gt;Bridging domains&lt;/h2&gt;

&lt;p&gt;We can resort to two options for narrowing the domain gap: (i) annotate more data; (ii) leverage the experience acquired on an initial environment and transfer it to the new environment. More annotated data has been shown to always improve performance of DNNs &lt;a class=&quot;citation&quot; href=&quot;#sun2017revisiting&quot;&gt;(Sun et al., 2017)&lt;/a&gt;. However the labeling process brings a significant financial and temporal burden. The time required for a high-quality annotation, such as the ones from the popular Cityscapes dataset is ∼90 minutes per image &lt;a class=&quot;citation&quot; href=&quot;#cordts2016cityscapes&quot;&gt;(Cordts et al., 2016)&lt;/a&gt;. The amount of images required to train high performance DNNs typically counts in hundreds of thousands. The acquisition of diverse data across seasons and weather conditions adds up even more time. It makes then sense to look for a solution elsewhere and the second option seems now more appealing, though achieving it remains technically challenging. This is actually the area of research of domain adaptation (DA) which addresses the domain-gap problem by transferring knowledge from a source domain (with full annotations) to a target domain (with fewer annotations if any), aiming to reach good performances on target samples. DA has consistently attracted interest from different communities across years &lt;a class=&quot;citation&quot; href=&quot;#csurka2017domain&quot;&gt;(Csurka, 2017)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here we are working on &lt;em&gt;Unsupervised DA&lt;/em&gt; (UDA), which is a more challenging task where we have access to labeled source samples and only unlabeled target samples. We use as source, data generated by a simulator or video game engine, while for target we consider real-data from car-mounted cameras. In &lt;em&gt;Figure 1&lt;/em&gt; we illustrate the difficulty of this task and the impact of our UDA technique, ADVENT.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/advent/advent_teaser.png&quot; alt=&quot;advent_teaser&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 1. Proposed entropy-based unsupervised domain adaptation for semantic segmentation.&lt;/b&gt; The top two rows show results on source and target domain scenes of the model trained without adaptation. The bottom row shows the result on the same target domain scene of the model trained with entropy-based adaptation. The left and right columns visualize respectively the semantic segmentation outputs and the corresponding prediction entropy maps.&lt;/div&gt;

&lt;p&gt;The main approaches for UDA include discrepancy minimization between source and target feature distributions usually achieved via adversarial training  &lt;a class=&quot;citation&quot; href=&quot;#ganin2015unsupervised&quot;&gt;(Ganin &amp;amp; Lempitsky, 2015)&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#tzeng2017adversarial&quot;&gt;(Tzeng et al., 2017)&lt;/a&gt;, self-training with pseudo-labels &lt;a class=&quot;citation&quot; href=&quot;#zou2018unsupervised&quot;&gt;(Zou et al., 2018)&lt;/a&gt; and generative approaches &lt;a class=&quot;citation&quot; href=&quot;#hoffman2018cycada&quot;&gt;(Hoffman et al., 2018)&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#wu2018dcan&quot;&gt;(Wu et al., 2018)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Entropy minimization&lt;/em&gt; has been shown to be useful for semi-supervised learning &lt;a class=&quot;citation&quot; href=&quot;#grandvalet2005semi&quot;&gt;(Grandvalet &amp;amp; Bengio, 2005)&lt;/a&gt;, clustering &lt;a class=&quot;citation&quot; href=&quot;#jain2018learning&quot;&gt;(Jain et al., 2018)&lt;/a&gt; and more recently to domain adaptation for classification &lt;a class=&quot;citation&quot; href=&quot;#long2016unsupervised&quot;&gt;(Long et al., 2016)&lt;/a&gt;. We chose to explore entropy based UDA training to obtain competitive performance on semantic segmentation.&lt;/p&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;p&gt;We present our two proposed approaches for entropy minimization using (i) an unsupervised entropy loss and (ii) adversarial training. To build our models, we start from existing semantic segmentation frameworks and add an additional network branch used for domain adaptation. &lt;em&gt;Figure 2&lt;/em&gt; illustrates our architectures.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/advent/advent_approach.jpg&quot; alt=&quot;&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 2. Approach overview.&lt;/b&gt; First, direct entropy minimization decreases the entropy of the target $P_{x_t}$, which is equivalent to minimizing the sum of weighted self-information maps $I_{x_t}$. In the second approach, we use adversarial training to enforce the consistency in $P_x$ across domains. Red arrows are used for target domain, blue arrows for source.
&lt;/div&gt;

&lt;h3 id=&quot;direct-entropy-minimization&quot;&gt;Direct entropy minimization&lt;/h3&gt;

&lt;p&gt;On the source domain we train our model, denoted as $F$, as usual using a supervised loss. For the target domain, we do not have annotations and we can no longer use the segmentation loss to train $F$. We notice that models trained only on source domain tend to produce over-confident predictions on source-like images and under-confident predictions on target-like ones. Motivated by this observation, we propose a supervision signal that could leverage visual information from the target samples, in spite of the lack of annotations. The objective is to constrain $F$ to produce high-confident predictions on target samples similarly to source samples. To this effect, we introduce the entropy loss $\mathcal{L}_{ent}$​ to maximize directly the prediction confidence in the target domain. Here we consider the Shannon Entropy (&lt;a href=&quot;http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf&quot;&gt;Shannon&lt;/a&gt;). During training, we jointly optimize the supervised segmentation loss $\mathcal{L}_{seg}$ on source samples and the unsupervised entropy loss $\mathcal{L}_{ent}$​​ on target samples.&lt;/p&gt;

&lt;h3 id=&quot;entropy-minimization-by-adverarial-learning&quot;&gt;Entropy minimization by adverarial learning&lt;/h3&gt;

&lt;p&gt;A limitation of the entropy loss is related to the absence of structural dependencies between local semantics. This is caused by the aggregation of the pixel-wise prediction entropies by summation. We address this through a unified adversarial training framework which minimizes indirectly the entropy of target data, by encouraging it to become similar to the source one. This allows the exploitation of the structural consistency between the domains. To this end, we formulate the UDA task as minimizing distribution distance between source and target on the &lt;em&gt;weighted self-information&lt;/em&gt; space. Since the trained model produces naturally low-entropy predictions on source-like images, by aligning weighted self-information distributions of target and source domains, we reach the same behavior on target-like data.&lt;/p&gt;

&lt;p&gt;We perform the adversarial adaptation on weighted self-information maps using a fully-convolutional discriminator network $D$. The discriminator produces domain classification outputs, &lt;em&gt;i.e.,&lt;/em&gt; class label $1$ (resp. $0$) for the source (resp. target) domain. We train $D$ to discriminate outputs coming from source and target images, and at the same time, train the segmentation network to fool the discriminator.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;We evaluate our approaches on the challenging &lt;em&gt;synthetic-2-real&lt;/em&gt; unsupervised domain adaptation set-ups. Models are trained on fully-annotated synthetic data and are validated on real-world data. In such set-ups, the models have access to some unlabeled real images during training.&lt;/p&gt;

&lt;h3 id=&quot;semantic-segmentation&quot;&gt;Semantic Segmentation&lt;/h3&gt;

&lt;p&gt;To train our models, we use either GTA5 &lt;a class=&quot;citation&quot; href=&quot;#richter2016playing&quot;&gt;(Richter et al., 2016)&lt;/a&gt; or SYNTHIA &lt;a class=&quot;citation&quot; href=&quot;#ros2016synthia&quot;&gt;(Ros et al., 2016)&lt;/a&gt; as source synthetic data, along with the training split of Cityscapes dataset &lt;a class=&quot;citation&quot; href=&quot;#cordts2016cityscapes&quot;&gt;(Cordts et al., 2016)&lt;/a&gt; as target domain data.&lt;/p&gt;

&lt;p&gt;In &lt;em&gt;Table 1&lt;/em&gt; we report our results on semantic segmentation from models trained on GTA5 $\rightarrow$ Cityscapes and from SYNTHIA $\rightarrow$ Cityscapes. We compare here only with the top performing method Adapt-SegMap &lt;a class=&quot;citation&quot; href=&quot;#tsai2018learning&quot;&gt;(Tsai et al., 2018)&lt;/a&gt;, while additional baselines and related methods are covered in the paper.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/advent/advent_table1.png&quot; alt=&quot;&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Table 1. Segmentation performance in mIoU with ResNet-101 based model and Deeplab-V2 as the segmentation network.&lt;/b&gt;We show results of our approaches using the direct entropy loss (MinEnt) and using adversarial training (AdvEnt).&lt;/div&gt;

&lt;p&gt;Our first approach of direct entropy minimization (&lt;em&gt;MinEnt&lt;/em&gt;) achieves comparable performance to state-of-the-art baselines. The light overhead of the entropy loss makes training time shorter for the MinEnt model, while being easier train compared to adversarial networks. Our second approach using adversarial training on the weighted self-information space, noted as &lt;em&gt;AdvEnt&lt;/em&gt;, shows consistent improvement to the baselines. In general, AdvEnt works better than MinEnt, confirming the importance of structural adaptation. The two approaches are complementary as their combination boosts performance further.&lt;/p&gt;

&lt;p&gt;In &lt;em&gt;Figure 3&lt;/em&gt;, we illustrate a few qualitative results of our models. Without domain adaptation, the model trained only on source supervision produces noisy segmentation predictions as well as high entropy activations, with a few exceptions on some classes like &lt;em&gt;“building”&lt;/em&gt; and &lt;em&gt;“car”&lt;/em&gt;. However, there are many confident predictions (low entropy) which are completely wrong. Our models, on the other hand, manage to produce correct predictions at high level of confidence.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/advent/advent_qualitative.png&quot; alt=&quot;&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Figure 3. Segmentation and detection qualitative results.&lt;/b&gt; Segmentation on Cityscapes validation set with ResNet-101 + DeepLab-V2; Detection on Cityscapes-foggy with VGG-16 as the backbone and SSD.&lt;/div&gt;

&lt;h3 id=&quot;uda-for-object-detection&quot;&gt;UDA for object detection&lt;/h3&gt;

&lt;p&gt;The proposed entropy-based approaches are not limited to semantic segmentation and can be applied to UDA for other recognition tasks, e.g. object detection. We conducted experiments in the UDA object detection set-up Cityscapes $\rightarrow$ Cityscapes-Foggy, similar to the one in &lt;a class=&quot;citation&quot; href=&quot;#chen2018domain&quot;&gt;(Chen et al., 2018)&lt;/a&gt;. We report quantitative results in &lt;em&gt;Table 2&lt;/em&gt; and qualitative ones in &lt;em&gt;Figure 3&lt;/em&gt;. In spite of the unfavorable factors, our improvement over the baseline ($+11.5\%$ mAP using AdvEnt) is larger than the one reported in &lt;a class=&quot;citation&quot; href=&quot;#chen2018domain&quot;&gt;(Chen et al., 2018)&lt;/a&gt; ($+8.8\%$). Additional experiments and implementation details can be found in the paper. These encouraging preliminary results suggest the feasibility of applying entropy-based approached on UDA for detection.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/advent/advent_table2.png&quot; alt=&quot;&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;b&gt;Table 2. Object detection performance on Cityscapes Foggy.&lt;/b&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this work, we propose two approaches for unsupervised domain adaptation reaching state-of-the-art performances on standard synthetic-2-real benchmarks. Interestingly the method can be easily extended to UDA for object detection with promising preliminary results.
Check out our &lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/html/Vu_ADVENT_Adversarial_Entropy_Minimization_for_Domain_Adaptation_in_Semantic_Segmentation_CVPR_2019_paper.html&quot;&gt;paper&lt;/a&gt; to find out more about intuitions, experiments and implementation details for AdvEnt and try out our &lt;a href=&quot;https://github.com/valeoai/ADVENT&quot;&gt;code&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;sun2017revisiting&quot;&gt;Sun, C., Shrivastava, A., Singh, S., &amp;amp; Gupta, A. (2017). Revisiting unreasonable effectiveness of data in deep learning era. &lt;i&gt;Proceedings of the IEEE International Conference on Computer Vision&lt;/i&gt;, 843–852.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;cordts2016cityscapes&quot;&gt;Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., &amp;amp; Schiele, B. (2016). The cityscapes dataset for semantic urban scene understanding. &lt;i&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 3213–3223.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;csurka2017domain&quot;&gt;Csurka, G. (2017). &lt;i&gt;Domain adaptation in computer vision applications&lt;/i&gt;. &lt;i&gt;8&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ganin2015unsupervised&quot;&gt;Ganin, Y., &amp;amp; Lempitsky, V. (2015). Unsupervised domain adaptation by backpropagation. &lt;i&gt;International Conference on Machine Learning&lt;/i&gt;, 1180–1189.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;tzeng2017adversarial&quot;&gt;Tzeng, E., Hoffman, J., Saenko, K., &amp;amp; Darrell, T. (2017). Adversarial discriminative domain adaptation. &lt;i&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 7167–7176.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;zou2018unsupervised&quot;&gt;Zou, Y., Yu, Z., Vijaya Kumar, B. V. K., &amp;amp; Wang, J. (2018). Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. &lt;i&gt;Proceedings of the European Conference on Computer Vision (ECCV)&lt;/i&gt;, 289–305.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hoffman2018cycada&quot;&gt;Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y., Isola, P., Saenko, K., Efros, A., &amp;amp; Darrell, T. (2018). Cycada: Cycle-consistent adversarial domain adaptation. &lt;i&gt;International Conference on Machine Learning&lt;/i&gt;, 1989–1998.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;wu2018dcan&quot;&gt;Wu, Z., Han, X., Lin, Y.-L., Gokhan Uzunbas, M., Goldstein, T., Nam Lim, S., &amp;amp; Davis, L. S. (2018). Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation. &lt;i&gt;Proceedings of the European Conference on Computer Vision (ECCV)&lt;/i&gt;, 518–534.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;grandvalet2005semi&quot;&gt;Grandvalet, Y., &amp;amp; Bengio, Y. (2005). Semi-supervised learning by entropy minimization. &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;, 529–536.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;jain2018learning&quot;&gt;Jain, H., Zepeda, J., Pérez, P., &amp;amp; Gribonval, R. (2018). Learning a complete image indexing pipeline. &lt;i&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 4933–4941.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;long2016unsupervised&quot;&gt;Long, M., Zhu, H., Wang, J., &amp;amp; Jordan, M. I. (2016). Unsupervised domain adaptation with residual transfer networks. &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;, 136–144.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;richter2016playing&quot;&gt;Richter, S. R., Vineet, V., Roth, S., &amp;amp; Koltun, V. (2016). Playing for data: Ground truth from computer games. &lt;i&gt;European Conference on Computer Vision&lt;/i&gt;, 102–118.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ros2016synthia&quot;&gt;Ros, G., Sellart, L., Materzynska, J., Vazquez, D., &amp;amp; Lopez, A. M. (2016). The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. &lt;i&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 3234–3243.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;tsai2018learning&quot;&gt;Tsai, Y.-H., Hung, W.-C., Schulter, S., Sohn, K., Yang, M.-H., &amp;amp; Chandraker, M. (2018). Learning to adapt structured output space for semantic segmentation. &lt;i&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 7472–7481.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;chen2018domain&quot;&gt;Chen, Y., Li, W., Sakaridis, C., Dai, D., &amp;amp; Van Gool, L. (2018). Domain adaptive faster r-cnn for object detection in the wild. &lt;i&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 3339–3348.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name></name></author><summary type="html">This post describes our recent work on unsupervised domain adaptation for semantic segmentation presented at CVPR 2019. ADVENT is a flexible technique for bridging the gap between two different domains through entropy minimization. Our work builds upon a simple observation: models trained only on source domain tend to produce over-confident, i.e., low-entropy, predictions on source-like images and under-confident, i.e., high-entropy, predictions on target-like ones. Consequently by minimizing the entropy on the target domain, we make the feature distributions from the two domains more similar. We show that our approach achieves competitive performances on standard semantic segmentation benchmarks and that it can be successfully extended to other tasks such as object detection.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://valeoai.github.io/blog/images/posts/advent/qualitative_results.png" /><media:content medium="image" url="https://valeoai.github.io/blog/images/posts/advent/qualitative_results.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>