{
  
    
        "post0": {
            "title": "ADVENT - Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation",
            "content": "This post describes our recent work on unsupervised domain adaptation for semantic segmentation presented at CVPR 2019. ADVENT is a flexible technique for bridging the gap between two different domains through entropy minimization. Our work builds upon a simple observation: models trained only on source domain tend to produce over-confident, i.e., low-entropy, predictions on source-like images and under-confident, i.e., high-entropy, predictions on target-like ones. Consequently by minimizing the entropy on the target domain, we make the feature distributions from the two domains more similar. We show that our approach achieves competitive performances on standard semantic segmentation benchmarks and that it can be successfully extended to other tasks such as object detection. . Visual perception is a remarkable ability that human drivers leverage for understanding their surroundings and for supporting the multiple micro-decisions needed in traffic. Since many years, researchers have been working on mimicking this human capability by means of computer algorithms. This research field is known as computer vision and it has seen impressive progress and wide adoption. Most of the modern computer vision systems rely on Deep Neural Networks (DNNs) which are powerful and widely employed tools able to learn from large amounts of data and make accurate predictions. In autonomous driving, DNN-based visual perception is also at the heart of the complex architectures under intelligent cars, and supports downstream decisions of the vehicle, e.g., steering, braking, signaling, etc. . The diversity and complexity of the situations encountered in real-world driving is tremendous. Unlike humans who can extrapolate effortlessly from previous experience in order to adapt to new environments and conditions, the scope of DNNs beyond the types of conditions and scenes seen during training is limited. For instance a model trained on data from a sunny country, would have a hard time delivering the same performance on streets with mixed weather conditions in a different country (with different urban architecture, furniture, vegetation, types of cars and pedestrian appearance and clothing). Similarly a model trained on a particular type of camera, is expected to see a drop in performance with images coming from a camera with different specifications. This difference between environments that leads to performance drops is referred to as domain gap. . Bridging domains . We can resort to two options for narrowing the domain gap: (i) annotate more data; (ii) leverage the experience acquired on an initial environment and transfer it to the new environment. More annotated data has been shown to always improve performance of DNNs (Sun et al., 2017). However the labeling process brings a significant financial and temporal burden. The time required for a high-quality annotation, such as the ones from the popular Cityscapes dataset is ∼90 minutes per image (Cordts et al., 2016). The amount of images required to train high performance DNNs typically counts in hundreds of thousands. The acquisition of diverse data across seasons and weather conditions adds up even more time. It makes then sense to look for a solution elsewhere and the second option seems now more appealing, though achieving it remains technically challenging. This is actually the area of research of domain adaptation (DA) which addresses the domain-gap problem by transferring knowledge from a source domain (with full annotations) to a target domain (with fewer annotations if any), aiming to reach good performances on target samples. DA has consistently attracted interest from different communities across years (Csurka, 2017). . Here we are working on Unsupervised DA (UDA), which is a more challenging task where we have access to labeled source samples and only unlabeled target samples. We use as source, data generated by a simulator or video game engine, while for target we consider real-data from car-mounted cameras. In Figure 1 we illustrate the difficulty of this task and the impact of our UDA technique, ADVENT. . . Figure 1. Proposed entropy-based unsupervised domain adaptation for semantic segmentation. The top two rows show results on source and target domain scenes of the model trained without adaptation. The bottom row shows the result on the same target domain scene of the model trained with entropy-based adaptation. The left and right columns visualize respectively the semantic segmentation outputs and the corresponding prediction entropy maps. The main approaches for UDA include discrepancy minimization between source and target feature distributions usually achieved via adversarial training (Ganin &amp; Lempitsky, 2015), (Tzeng et al., 2017), self-training with pseudo-labels (Zou et al., 2018) and generative approaches (Hoffman et al., 2018), (Wu et al., 2018). . Entropy minimization has been shown to be useful for semi-supervised learning (Grandvalet &amp; Bengio, 2005), clustering (Jain et al., 2018) and more recently to domain adaptation for classification (Long et al., 2016). We chose to explore entropy based UDA training to obtain competitive performance on semantic segmentation. . Approach . We present our two proposed approaches for entropy minimization using (i) an unsupervised entropy loss and (ii) adversarial training. To build our models, we start from existing semantic segmentation frameworks and add an additional network branch used for domain adaptation. Figure 2 illustrates our architectures. . . Figure 2. Approach overview. First, direct entropy minimization decreases the entropy of the target $P_{x_t}$, which is equivalent to minimizing the sum of weighted self-information maps $I_{x_t}$. In the second approach, we use adversarial training to enforce the consistency in $P_x$ across domains. Red arrows are used for target domain, blue arrows for source. Direct entropy minimization . On the source domain we train our model, denoted as $F$, as usual using a supervised loss. For the target domain, we do not have annotations and we can no longer use the segmentation loss to train $F$. We notice that models trained only on source domain tend to produce over-confident predictions on source-like images and under-confident predictions on target-like ones. Motivated by this observation, we propose a supervision signal that could leverage visual information from the target samples, in spite of the lack of annotations. The objective is to constrain $F$ to produce high-confident predictions on target samples similarly to source samples. To this effect, we introduce the entropy loss $ mathcal{L}_{ent}$​ to maximize directly the prediction confidence in the target domain. Here we consider the Shannon Entropy (Shannon). During training, we jointly optimize the supervised segmentation loss $ mathcal{L}_{seg}$ on source samples and the unsupervised entropy loss $ mathcal{L}_{ent}$​​ on target samples. . Entropy minimization by adverarial learning . A limitation of the entropy loss is related to the absence of structural dependencies between local semantics. This is caused by the aggregation of the pixel-wise prediction entropies by summation. We address this through a unified adversarial training framework which minimizes indirectly the entropy of target data, by encouraging it to become similar to the source one. This allows the exploitation of the structural consistency between the domains. To this end, we formulate the UDA task as minimizing distribution distance between source and target on the weighted self-information space. Since the trained model produces naturally low-entropy predictions on source-like images, by aligning weighted self-information distributions of target and source domains, we reach the same behavior on target-like data. . We perform the adversarial adaptation on weighted self-information maps using a fully-convolutional discriminator network $D$. The discriminator produces domain classification outputs, i.e., class label $1$ (resp. $0$) for the source (resp. target) domain. We train $D$ to discriminate outputs coming from source and target images, and at the same time, train the segmentation network to fool the discriminator. . Experiments . We evaluate our approaches on the challenging synthetic-2-real unsupervised domain adaptation set-ups. Models are trained on fully-annotated synthetic data and are validated on real-world data. In such set-ups, the models have access to some unlabeled real images during training. . Semantic Segmentation . To train our models, we use either GTA5 (Richter et al., 2016) or SYNTHIA (Ros et al., 2016) as source synthetic data, along with the training split of Cityscapes dataset (Cordts et al., 2016) as target domain data. . In Table 1 we report our results on semantic segmentation from models trained on GTA5 $ rightarrow$ Cityscapes and from SYNTHIA $ rightarrow$ Cityscapes. We compare here only with the top performing method Adapt-SegMap (Tsai et al., 2018), while additional baselines and related methods are covered in the paper. . . Table 1. Segmentation performance in mIoU with ResNet-101 based model and Deeplab-V2 as the segmentation network.We show results of our approaches using the direct entropy loss (MinEnt) and using adversarial training (AdvEnt). Our first approach of direct entropy minimization (MinEnt) achieves comparable performance to state-of-the-art baselines. The light overhead of the entropy loss makes training time shorter for the MinEnt model, while being easier train compared to adversarial networks. Our second approach using adversarial training on the weighted self-information space, noted as AdvEnt, shows consistent improvement to the baselines. In general, AdvEnt works better than MinEnt, confirming the importance of structural adaptation. The two approaches are complementary as their combination boosts performance further. . In Figure 3, we illustrate a few qualitative results of our models. Without domain adaptation, the model trained only on source supervision produces noisy segmentation predictions as well as high entropy activations, with a few exceptions on some classes like “building” and “car”. However, there are many confident predictions (low entropy) which are completely wrong. Our models, on the other hand, manage to produce correct predictions at high level of confidence. . . Figure 3. Segmentation and detection qualitative results. Segmentation on Cityscapes validation set with ResNet-101 + DeepLab-V2; Detection on Cityscapes-foggy with VGG-16 as the backbone and SSD. UDA for object detection . The proposed entropy-based approaches are not limited to semantic segmentation and can be applied to UDA for other recognition tasks, e.g. object detection. We conducted experiments in the UDA object detection set-up Cityscapes $ rightarrow$ Cityscapes-Foggy, similar to the one in (Chen et al., 2018). We report quantitative results in Table 2 and qualitative ones in Figure 3. In spite of the unfavorable factors, our improvement over the baseline ($+11.5 %$ mAP using AdvEnt) is larger than the one reported in (Chen et al., 2018) ($+8.8 %$). Additional experiments and implementation details can be found in the paper. These encouraging preliminary results suggest the feasibility of applying entropy-based approached on UDA for detection. . . Table 2. Object detection performance on Cityscapes Foggy. Conclusion . In this work, we propose two approaches for unsupervised domain adaptation reaching state-of-the-art performances on standard synthetic-2-real benchmarks. Interestingly the method can be easily extended to UDA for object detection with promising preliminary results. Check out our paper to find out more about intuitions, experiments and implementation details for AdvEnt and try out our code. . References . Sun, C., Shrivastava, A., Singh, S., &amp; Gupta, A. (2017). Revisiting unreasonable effectiveness of data in deep learning era. Proceedings of the IEEE International Conference on Computer Vision, 843–852. | Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., &amp; Schiele, B. (2016). The cityscapes dataset for semantic urban scene understanding. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3213–3223. | Csurka, G. (2017). Domain adaptation in computer vision applications. 8. | Ganin, Y., &amp; Lempitsky, V. (2015). Unsupervised domain adaptation by backpropagation. International Conference on Machine Learning, 1180–1189. | Tzeng, E., Hoffman, J., Saenko, K., &amp; Darrell, T. (2017). Adversarial discriminative domain adaptation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 7167–7176. | Zou, Y., Yu, Z., Vijaya Kumar, B. V. K., &amp; Wang, J. (2018). Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. Proceedings of the European Conference on Computer Vision (ECCV), 289–305. | Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y., Isola, P., Saenko, K., Efros, A., &amp; Darrell, T. (2018). Cycada: Cycle-consistent adversarial domain adaptation. International Conference on Machine Learning, 1989–1998. | Wu, Z., Han, X., Lin, Y.-L., Gokhan Uzunbas, M., Goldstein, T., Nam Lim, S., &amp; Davis, L. S. (2018). Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation. Proceedings of the European Conference on Computer Vision (ECCV), 518–534. | Grandvalet, Y., &amp; Bengio, Y. (2005). Semi-supervised learning by entropy minimization. Advances in Neural Information Processing Systems, 529–536. | Jain, H., Zepeda, J., Pérez, P., &amp; Gribonval, R. (2018). Learning a complete image indexing pipeline. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 4933–4941. | Long, M., Zhu, H., Wang, J., &amp; Jordan, M. I. (2016). Unsupervised domain adaptation with residual transfer networks. Advances in Neural Information Processing Systems, 136–144. | Richter, S. R., Vineet, V., Roth, S., &amp; Koltun, V. (2016). Playing for data: Ground truth from computer games. European Conference on Computer Vision, 102–118. | Ros, G., Sellart, L., Materzynska, J., Vazquez, D., &amp; Lopez, A. M. (2016). The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3234–3243. | Tsai, Y.-H., Hung, W.-C., Schulter, S., Sohn, K., Yang, M.-H., &amp; Chandraker, M. (2018). Learning to adapt structured output space for semantic segmentation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 7472–7481. | Chen, Y., Li, W., Sakaridis, C., Dai, D., &amp; Van Gool, L. (2018). Domain adaptive faster r-cnn for object detection in the wild. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3339–3348. |",
            "url": "https://valeoai.github.io/blog/2020/07/07/advent-domain-adaptation.html",
            "relUrl": "/2020/07/07/advent-domain-adaptation.html",
            "date": " • Jul 7, 2020"
        }
        
    
  

  
  
      ,"page0": {
          "title": "Projects",
          "content": "Projects . Multi-sensor perception . Automated driving relies first on a diverse range of sensors, like Valeo’s fish-eye cameras, LiDARs, radars and ultrasonics. Exploiting at best the outputs of each of these sensors at any instant is fundamental to understand the complex environment of the vehicle and gain robustness. To this end, we explore various machine learning approaches where sensors are considered either in isolation (as radar in Carrada at ICPR’20) or collectively (as in xMUDA at CVPR’20). . . 3D perception . Each sensor delivers information about the 3D world around the vehicle. Making sense of this information in terms of drivable space and important objects (road users, curb, obstacles, street furnitures) in 3D is required for the driving system to plan and act in the safest and most confortable way. This encompasses several challenging tasks, in particular detection and segmentation of objects in point clouds as in FKAConv at ACCV’20. . . Frugal learning . Collecting diverse enough data, and annotating it precisely, is complex, costly and time-consuming. To reduce dramatically these needs, we explore various alternatives to fully-supervised learning, e.g, training that is unsupervised (as rOSD at ECCCV’20), self-supervised (as BoWNet at CVPR’20), semi-supervised, active, zero-shot (as ZS3 at NeurIPS’19) or few-shot. We also investigate training with fully-synthetic data (in combination with unsupervised domain adaptation) and with GAN-augmented data. . . Domain adaptation . Deep learning and reinforcement learning are key technologies for autonomous driving. One of the challenges they face is to adapt to conditions which differ from those met during training. To improve systems’ performance in such situations, we explore so-called “domain adaptation” techniques, as in AdvEnt at CVPR’19 and DADA its extension at ICCV’19. . . Reliability . When the unexpected happens, when the weather badly degrades, when a sensor gets blocked, the embarked perception system should diagnose the situation and react accordingly, e.g., by calling an alternative system or the human driver. With this in mind, we investigate ways to improve the robustness of neural nets to input variations, including to adversarial attacks, and to predict automatically the performance and the confidence of their predictions as in ConfidNet at NeurIPS’19. . . Driving in action . Getting from sensory inputs to car control goes either through a modular stack (perception &gt; localization &gt; forecast &gt; planning &gt; actuation) or, more radically, through a single end-to-end model. We work on both strategies, more specificaly on action forecasting, automatic interpretation of decisions taken by a driving system, and reinforcement / imitation learning for end-to-end systems (as in RL work at CVPR’20). . . Core Deep Learning . Deep learning being now a key component of AD systems, it is important to get a better understanding of its inner workings, in particular the link between the specifics of the learning optimization and the key properities (performance, regularity, robustness, generalization) of the trained models. Among other things, we investigate the impact of popular batch normalization on standard learning procedures and the ability to learn through unsupervised distillation. .",
          "url": "https://valeoai.github.io/blog/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
      ,"page1": {
          "title": "Publications",
          "content": "Publications . FLOT: Scene Flow on Point Clouds guided by Optimal Transport . Gilles Puy, Alexandre Boulch, and Renaud Marlet European Conference on Computer Vision (ECCV), 2020 . xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation . Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Émilie Wirbel, and Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2020 . TRADI: Tracking deep neural network weight distributions . Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson, and Isabelle Bloch European Conference on Computer Vision (ECCV), 2020 . QuEST: Quantized Embedding Space for Transferring Knowledge . Himalaya Jain, Spyros Gidaris, Nikos Komodakis, Patrick Pérez, and Matthieu Cord European Conference on Computer Vision (ECCV), 2020 . Learning Representations by Predicting Bags of Visual Words . Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pérez, and Matthieu Cord Computer Vision and Pattern Recognition (CVPR), 2020 . This dataset does not exist: training models from generated images . Victor Besnier, Himalaya Jain, Andrei Bursuc, Matthieu Cord, and Patrick Pérez International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2020 . Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems . Isabelle Leang, Ganesh Sistu, Fabian Burger, Andrei Bursuc, and Senthil Yogamani IEEE International Conference on Intelligent Transportation Systems (ITSC), 2020 . FKAConv: Feature-Kernel Alignment for Point Cloud Convolutiont . Alexandre Boulch, Gilles Puy, and Renaud Marlet Asian Conference on Computer Vision (ACCV), 2020 . End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances . Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde Computer Vision and Pattern Recognition (CVPR), 2020 . Zero-Shot Semantic Segmentation . Maxime Bucher, Tuan Hung Vu, Matthieu Cord, and Patrick Pérez Neural Information Processing Systems (NeurIPS), 2019 . DADA: Depth-aware Domain Adaptation in Semantic Segmentation . Tuan Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Pérez International Conference on Computer Vision (ICCV), 2019 . Addressing Failure Prediction by Learning Model Confidence . Charles Corbière, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez Neural Information Processing Systems (NeurIPS), 2019 . Boosting Few-Shot Visual Learning With Self-Supervision . Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pérez, and Matthieu Cord International Conference on Computer Vision (ICCV), 2019 . AdvEnt: Adversarial Entropy minimization for domain adaptation in semantic segmentation . Tuan Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2019 .",
          "url": "https://valeoai.github.io/blog/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "About",
          "content": "Overview . valeo.ai is an international team based in Paris, conducting AI research for Valeo automotive applications, in collaboraton with world-class academics. Our main research is towards better, clearer &amp; safer automotive AI. . You can find out more about our research through our papers, released code, tweets, and this blog. . Blog purpose . The aim of this blog is to provide an accessible, general-audience medium for valeo.ai researchers to communicate research publications and findings, as well as perspectives on the field. Posts are written by students and research scientists at valeo.ai. They are intended to provide insights and walk-throughs for our findings and results, both to experts and the general audience. . Translating Posts . If you wish to translate our blog posts, please contact the authors of the posts, as they own the copyright, and copy the editors, andrei.bursuc@valeo.com in your email. . Acknowledgments . This site is built with fastpages and hosted on Github. The design is based upon the Jekyll theme Minima. .",
          "url": "https://valeoai.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  

  

  
  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://valeoai.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  


  
  
      ,"page0": {
          "title": "Multi-sensor perception",
          "content": "Multi-sensor perception . Automated driving relies first on a diverse range of sensors, like Valeo’s fish-eye cameras, LiDARs, radars and ultrasonics. Exploiting at best the outputs of each of these sensors at any instant is fundamental to understand the complex environment of the vehicle and gain robustness. To this end, we explore various machine learning approaches where sensors are considered either in isolation (as radar in Carrada at ICPR’20) or collectively (as in xMUDA at CVPR’20). . Publications . xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation . Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Émilie Wirbel, and Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2020 . . Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems . Isabelle Leang, Ganesh Sistu, Fabian Burger, Andrei Bursuc, and Senthil Yogamani IEEE International Conference on Intelligent Transportation Systems (ITSC), 2020 .",
          "url": "https://valeoai.github.io/blog/projects/multi-sensor",
          "relUrl": "/projects/multi-sensor",
          "date": ""
      }
      
  

  
      ,"page1": {
          "title": "3D perception",
          "content": "3D perception . Each sensor delivers information about the 3D world around the vehicle. Making sense of this information in terms of drivable space and important objects (road users, curb, obstacles, street furnitures) in 3D is required for the driving system to plan and act in the safest and most confortable way. This encompasses several challenging tasks, in particular detection and segmentation of objects in point clouds as in FKAConv at ACCV’20. . Publications . FLOT: Scene Flow on Point Clouds guided by Optimal Transport . Gilles Puy, Alexandre Boulch, and Renaud Marlet European Conference on Computer Vision (ECCV), 2020 . . FKAConv: Feature-Kernel Alignment for Point Cloud Convolutiont . Alexandre Boulch, Gilles Puy, and Renaud Marlet Asian Conference on Computer Vision (ACCV), 2020 .",
          "url": "https://valeoai.github.io/blog/projects/3d-perception",
          "relUrl": "/projects/3d-perception",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Frugal learning",
          "content": "Frugal learning . Collecting diverse enough data, and annotating it precisely, is complex, costly and time-consuming. To reduce dramatically these needs, we explore various alternatives to fully-supervised learning, e.g, training that is unsupervised (as rOSD at ECCCV’20), self-supervised (as BoWNet at CVPR’20), semi-supervised, active, zero-shot (as ZS3 at NeurIPS’19) or few-shot. We also investigate training with fully-synthetic data (in combination with unsupervised domain adaptation) and with GAN-augmented data. . Publications . Learning Representations by Predicting Bags of Visual Words . Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pérez, and Matthieu Cord Computer Vision and Pattern Recognition (CVPR), 2020 . . This dataset does not exist: training models from generated images . Victor Besnier, Himalaya Jain, Andrei Bursuc, Matthieu Cord, and Patrick Pérez International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2020 . . Zero-Shot Semantic Segmentation . Maxime Bucher, Tuan Hung Vu, Matthieu Cord, and Patrick Pérez Neural Information Processing Systems (NeurIPS), 2019 . . Boosting Few-Shot Visual Learning With Self-Supervision . Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pérez, and Matthieu Cord International Conference on Computer Vision (ICCV), 2019 .",
          "url": "https://valeoai.github.io/blog/projects/limited-supervision",
          "relUrl": "/projects/limited-supervision",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "Domain adaptation",
          "content": "Domain adaptation . Deep learning and reinforcement learning are key technologies for autonomous driving. One of the challenges they face is to adapt to conditions which differ from those met during training. To improve systems’ performance in such situations, we explore so-called “domain adaptation” techniques, as in AdvEnt at CVPR’19 and DADA its extension at ICCV’19. . Publications . xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation . Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Émilie Wirbel, and Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2020 . . DADA: Depth-aware Domain Adaptation in Semantic Segmentation . Tuan Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Pérez International Conference on Computer Vision (ICCV), 2019 . . AdvEnt: Adversarial Entropy minimization for domain adaptation in semantic segmentation . Tuan Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2019 .",
          "url": "https://valeoai.github.io/blog/projects/domain-adaptation",
          "relUrl": "/projects/domain-adaptation",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Reliability",
          "content": "Reliability . When the unexpected happens, when the weather badly degrades, when a sensor gets blocked, the embarked perception system should diagnose the situation and react accordingly, e.g., by calling an alternative system or the human driver. With this in mind, we investigate ways to improve the robustness of neural nets to input variations, including to adversarial attacks, and to predict automatically the performance and the confidence of their predictions as in ConfidNet at NeurIPS’19. . Publications . TRADI: Tracking deep neural network weight distributions . Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson, and Isabelle Bloch European Conference on Computer Vision (ECCV), 2020 . . Addressing Failure Prediction by Learning Model Confidence . Charles Corbière, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez Neural Information Processing Systems (NeurIPS), 2019 .",
          "url": "https://valeoai.github.io/blog/projects/reliability",
          "relUrl": "/projects/reliability",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "Driving in action",
          "content": "Driving in action . Getting from sensory inputs to car control goes either through a modular stack (perception &gt; localization &gt; forecast &gt; planning &gt; actuation) or, more radically, through a single end-to-end model. We work on both strategies, more specificaly on action forecasting, automatic interpretation of decisions taken by a driving system, and reinforcement / imitation learning for end-to-end systems (as in RL work at CVPR’20). . Publications . End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances . Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde Computer Vision and Pattern Recognition (CVPR), 2020 .",
          "url": "https://valeoai.github.io/blog/projects/driving",
          "relUrl": "/projects/driving",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "Core Deep Learning",
          "content": "Core Deep Learning . Deep learning being now a key component of AD systems, it is important to get a better understanding of its inner workings, in particular the link between the specifics of the learning optimization and the key properities (performance, regularity, robustness, generalization) of the trained models. Among other things, we investigate the impact of popular batch normalization on standard learning procedures and the ability to learn through unsupervised distillation. . Publications . QuEST: Quantized Embedding Space for Transferring Knowledge . Himalaya Jain, Spyros Gidaris, Nikos Komodakis, Patrick Pérez, and Matthieu Cord European Conference on Computer Vision (ECCV), 2020 .",
          "url": "https://valeoai.github.io/blog/projects/deep-learning",
          "relUrl": "/projects/deep-learning",
          "date": ""
      }
      
  



  
      ,"page0": {
          "title": "AdvEnt: Adversarial Entropy minimization for domain adaptation in semantic segmentation",
          "content": "AdvEnt: Adversarial Entropy minimization for domain adaptation in semantic segmentation . Tuan-Hung Vu&nbsp;&nbsp; Himalaya Jain&nbsp;&nbsp; Maxime Bucher&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . CVPR 2019 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; Blog &nbsp;&nbsp; . . Abstract . Semantic segmentation is a key problem for many computer vision tasks. While approaches based on convolutional neural networks constantly break new records on different benchmarks, generalizing well to diverse testing environments remains a major challenge. In numerous real world applications, there is indeed a large gap between data distributions in train and test domains, which results in severe performance loss at run-time. In this work, we address the task of unsupervised domain adaptation in semantic segmentation with losses based on the entropy of the pixel-wise predictions. To this end, we propose two novel, complementary methods using (i) an entropy loss and (ii) an adversarial loss respectively. We demonstrate state-of-the-art performance in semantic segmentation on two challenging synthetic-2-real set-ups and show that the approach can also be used for detection. . . Video . . . . BibTeX . @inproceedings{vu2018advent, title={ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation}, author={Vu, Tuan-Hung and Jain, Himalaya and Bucher, Maxime and Cord, Mathieu and P{ &#39;e}rez, Patrick}, booktitle={CVPR}, year={2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/advent/",
          "relUrl": "/publications/advent/",
          "date": ""
      }
      
  

  
      ,"page1": {
          "title": "Boosting Few-Shot Visual Learning With Self-Supervision",
          "content": "Boosting Few-Shot Visual Learning With Self-Supervision . Spyros Gidaris&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Nikos Komodakis&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Matthieu Cord . ICCV 2019 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Few-shot learning and self-supervised learning address different facets of the same problem: how to train a model with little or no labeled data. Few-shot learning aims for optimization methods and models that can learn efficiently to recognize patterns in the low data regime. Self-supervised learning focuses instead on unlabeled data and looks into it for the supervisory signal to feed high capacity deep neural networks. In this work we exploit the complementarity of these two domains and propose an approach for improving few-shot learning through self-supervision. We use self-supervision as an auxiliary task in a few-shot learning pipeline, enabling feature extractors to learn richer and more transferable visual representations while still using few annotated samples. Through self-supervision, our approach can be naturally extended towards using diverse unlabeled data from other datasets in the few-shot setting. We report consistent improvements across an array of architectures, datasets and self-supervision techniques. We provide the implementation code at: https://github.com/valeoai/BF3S . . . BibTeX . @inproceedings{gidaris2019boosting, title={Boosting few-shot visual learning with self-supervision}, author={Gidaris, Spyros and Bursuc, Andrei and Komodakis, Nikos and P{ &#39;e}rez, Patrick and Cord, Matthieu}, booktitle={Proceedings of the IEEE International Conference on Computer Vision}, pages={8059--8068}, year={2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/bf3s/",
          "relUrl": "/publications/bf3s/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Learning Representations by Predicting Bags of Visual Words",
          "content": "Learning Representations by Predicting Bags of Visual Words . Spyros Gidaris&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Nikos Komodakis&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Matthieu Cord . CVPR 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Self-supervised representation learning targets to learn convnet-based image representations from unlabeled data. Inspired by the success of NLP methods in this area, in this work we propose a self-supervised approach based on spatially dense image descriptions that encode discrete visual concepts, here called visual words. To build such discrete representations, we quantize the feature maps of a first pre-trained self-supervised convnet, over a k-means based vocabulary. Then, as a self-supervised task, we train another convnet to predict the histogram of visual words of an image (i.e., its Bag-of-Words representation) given as input a perturbed version of that image. The proposed task forces the convnet to learn perturbation-invariant and context-aware image features, useful for downstream image understanding tasks. We extensively evaluate our method and demonstrate very strong empirical results, e.g., our pre-trained self-supervised representations transfer better on detection task and similarly on classification over classes &quot;unseen&quot; during pre-training, when compared to the supervised case. This also shows that the process of image discretization into visual words can provide the basis for very powerful self-supervised approaches in the image domain, thus allowing further connections to be made to related methods from the NLP domain that have been extremely successful so far. . . . BibTeX . @inproceedings{gidaris2020learning, title={Learning Representations by Predicting Bags of Visual Words}, author={Gidaris, Spyros and Bursuc, Andrei and Komodakis, Nikos and P{ &#39;e}rez, Patrick and Cord, Matthieu}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={6928--6938}, year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/bownet/",
          "relUrl": "/publications/bownet/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "Addressing Failure Prediction by Learning Model Confidence",
          "content": "Addressing Failure Prediction by Learning Model Confidence . Charles Corbière&nbsp;&nbsp; Nicolas Thome&nbsp;&nbsp; Avner Bar-Hen&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . NeurIPS 2019 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; BibTeX&nbsp;&nbsp; . . Abstract . Assessing reliably the confidence of a deep neural net and predicting its failures is of primary importance for the practical deployment of these models. In this paper, we propose a new target criterion for model confidence, corresponding to the True Class Probability (TCP). We show how using the TCP is more suited than relying on the classic Maximum Class Probability (MCP). We provide in addition theoretical guarantees for TCP in the context of failure prediction. Since the true class is by essence unknown at test time, we propose to learn TCP criterion on the training set, introducing a specific learning scheme adapted to this context. Extensive experiments are conducted for validating the relevance of the proposed approach. We study various network architectures, small and large scale datasets for image classification and semantic segmentation. We show that our approach consistently outperforms several strong methods, from MCP to Bayesian uncertainty, as well as recent approaches specifically designed for failure prediction. . . BibTeX . @incollection{NIPS2019_8556, title = {Addressing Failure Prediction by Learning Model Confidence}, author = {Corbi `{e}re, Charles and THOME, Nicolas and Bar-Hen, Avner and Cord, Matthieu and &#39;{e}rez, Patrick}, booktitle = {Advances in Neural Information Processing Systems 32}, pages = {2902--2913}, year = {2019}, } . .",
          "url": "https://valeoai.github.io/blog/publications/confidnet/",
          "relUrl": "/publications/confidnet/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "DADA: Depth-aware Domain Adaptation in Semantic Segmentation",
          "content": "DADA: Depth-aware Domain Adaptation in Semantic Segmentation . Tuan-Hung Vu&nbsp;&nbsp; Himalaya Jain&nbsp;&nbsp; Maxime Bucher&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . ICCV 2019 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Unsupervised domain adaptation (UDA) is important for applications where large scale annotation of representative data is challenging. For semantic segmentation in particular, it helps deploy on real &quot;target domain&quot; data models that are trained on annotated images from a different &quot;source domain&quot;, notably a virtual environment. To this end, most previous works consider semantic segmentation as the only mode of supervision for source domain data, while ignoring other, possibly available, information like depth. In this work, we aim at exploiting at best such a privileged information while training the UDA model. We propose a unified depth-aware UDA framework that leverages in several complementary ways the knowledge of dense depth in the source domain. As a result, the performance of the trained semantic segmentation model on the target domain is boosted. Our novel approach indeed achieves state-of-the-art performance on different challenging synthetic-2-real benchmarks. . . . BibTeX . @inproceedings{vu2019dada, title={Dada: Depth-aware domain adaptation in semantic segmentation}, author={Vu, Tuan-Hung and Jain, Himalaya and Bucher, Maxime and Cord, Matthieu and P{ &#39;e}rez, Patrick}, booktitle={Proceedings of the IEEE International Conference on Computer Vision}, pages={7364--7373}, year={2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/dada/",
          "relUrl": "/publications/dada/",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems",
          "content": "Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems . Isabelle Liang&nbsp;&nbsp; Ganesh Sistu&nbsp;&nbsp; Fabian Burger&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Senthil Yogamani&nbsp;&nbsp; . ITSC 2020 . Paper&nbsp;&nbsp; . . Abstract . Deep multi-task networks are of particular interest for autonomous driving systems. They can potentially strike an excellent trade-off between predictive performance, hardware constraints and efficient use of information from multiple types of annotations and modalities. However, training such models is non-trivial and requires balancing learning over all tasks as their respective losses display different scales, ranges and dynamics across training. Multiple task weighting methods that adjust the losses in an adaptive way have been proposed recently on different datasets and combinations of tasks, making it difficult to compare them. In this work, we review and systematically evaluate nine task weighting strategies on common grounds on three automotive datasets (KITTI, Cityscapes and WoodScape). We then propose a novel method combining evolutionary meta-learning and task-based selective backpropagation, for computing task weights leading to reliable network training. Our method outperforms state-of-the-art methods by a significant margin on a two-task application. . . . Results . . Comparison of various task-weighting methods for two-task network training. . Task weights and asynchronous backpropagation frequencies computed by several task-weighting methods. . BibTeX . @article{leang2020dynamic, title={Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems}, author={Leang, Isabelle and Sistu, Ganesh and Burger, Fabian and Bursuc, Andrei and Yogamani, Senthil}, journal={arXiv preprint arXiv:2001.02223}, year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/dynamic-mtl/",
          "relUrl": "/publications/dynamic-mtl/",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances",
          "content": "End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances . Marin Toromanoff&nbsp;&nbsp; Emilie Wirbel&nbsp;&nbsp; Fabien Moutarde . CVPR 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Reinforcement Learning (RL) aims at learning an optimal behavior policy from its own experiments and not rule-based control methods. However, there is no RL algorithm yet capable of handling a task as difficult as urban driving. We present a novel technique, coined implicit affordances, to effectively leverage RL for urban driving thus including lane keeping, pedestrians and vehicles avoidance, and traffic light detection. To our knowledge we are the first to present a successful RL agent handling such a complex task especially regarding the traffic light detection. Furthermore, we have demonstrated the effectiveness of our method by winning the Camera Only track of the CARLA challenge. . . . Video . . . . BibTeX . @inproceedings{toromanoff2020end, title={End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances}, author={Toromanoff, Marin and Wirbel, Emilie and Moutarde, Fabien}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={7153--7162}, year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/e2e-rl-driving/",
          "relUrl": "/publications/e2e-rl-driving/",
          "date": ""
      }
      
  

  
      ,"page7": {
          "title": "FKAConv: Feature-Kernel Alignment for Point Cloud Convolutiont",
          "content": "FKAConv: Feature-Kernel Alignment for Point Cloud Convolutiont . Alexandre Boulch &nbsp;&nbsp; Gilles Puy &nbsp;&nbsp; Renaud Marlet&nbsp;&nbsp; . ACCV 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Recent state-of-the-art methods for point cloud processing are based on the notion of point convolution, for which several approaches have been proposed. In this paper, inspired by discrete convolution in image processing, we provide a formulation to relate and analyze a number of point convolution methods. We also propose our own convolution variant, that separates the estimation of geometry-less kernel weights and their alignment to the spatial support of features. Additionally, we define a point sampling strategy for convolution that is both effective and fast. Finally, using our convolution and sampling strategy, we show competitive results on classification and semantic segmentation benchmarks while being time and memory efficient. . . BibTeX . @inproceedings{boulch2020fka, title={FKAConv: Feature-Kernel Alignment for Point Cloud Convolutions}, author={Boulch, Alexandre and Puy, Gilles and Marlet, Renaud}, booktitle={15th Asian Conference on Computer Vision (ACCV 2020)}, year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/fkaconv/",
          "relUrl": "/publications/fkaconv/",
          "date": ""
      }
      
  

  
      ,"page8": {
          "title": "FLOT: Scene Flow on Point Clouds guided by Optimal Transport",
          "content": "FLOT: Scene Flow on Point Clouds guided by Optimal Transport . Gilles Puy &nbsp;&nbsp; Alexandre Boulch &nbsp;&nbsp; Renaud Marlet . ECCV 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . We propose and study a method called FLOT that estimates scene flow on point clouds. We start the design of FLOT by noticing that scene flow estimation on point clouds reduces to estimating a permutation matrix in a perfect world. Inspired by recent works on graph matching, we build a method to find these correspondences by borrowing tools from optimal transport. Then, we relax the transport constraints to take into account real-world imperfections. The transport cost between two points is given by the pairwise similarity between deep features extracted by a neural network trained under full supervision using synthetic datasets. Our main finding is that FLOT can perform as well as the best existing methods on synthetic and real-world datasets while requiring much less parameters and without using multiscale analysis. Our second finding is that, on the training datasets considered, most of the performance can be explained by the learned transport cost. This yields a simpler method, FLOT0, which is obtained using a particular choice of optimal transport parameters and performs nearly as well as FLOT. . . BibTeX . @inproceedings{puy20flot, title={FLOT: Scene Flow on Point Clouds Guided by Optimal Transport}, author={Puy, Gilles and Boulch, Alexandre and Marlet, Renaud}, booktitle={European Conference on Computer Vision} year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/flot/",
          "relUrl": "/publications/flot/",
          "date": ""
      }
      
  

  
      ,"page9": {
          "title": "This dataset does not exist: training models from generated images",
          "content": "This dataset does not exist: training models from generated images . Victor Besnier&nbsp;&nbsp; Himalaya Jain&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . ICASSP 2020 . Paper&nbsp;&nbsp; . . Abstract . Current generative networks are increasingly proficient in generating high-resolution realistic images. These generative networks, especially the conditional ones, can potentially become a great tool for providing new image datasets. This naturally brings the question: Can we train a classifier only on the generated data? This potential availability of nearly unlimited amounts of training data challenges standard practices for training machine learning models, which have been crafted across the years for limited and fixed size datasets. In this work we investigate this question and its related challenges. We identify ways to improve significantly the performance over naive training on randomly generated images with regular heuristics. We propose three standalone techniques that can be applied at different stages of the pipeline, i.e., data generation, training on generated data, and deploying on real data. We evaluate our proposed approaches on a subset of the ImageNet dataset and show encouraging results compared to classifiers trained on real images. . . . Results . . Effect of applying HSM at different iterations during classifier training. The first image of each category is sampled randomly. The other two images are generated from HSM-computed codes at two different steps during training. The difference between the images shows that the effect of HSM is specific to the classifier. . Results for ImageNet-10 real test images. Performance of classifiers trained on generated images with all combinations of the proposed methods. Each classifier is trained for $150$ epochs (except Long training, where we let DS run for $150$ epochs) over a set of $N = 13K$ images; in case of continuous sampling we replace $50 %$ (i.e., $6,500$) of the images every epoch, while fixed dataset is the usual setup where no images are replaced during training. In all setups we use $N$ images per epoch. First column, without applying any of the proposed methods, is the baseline. Each of the proposed methods individually shows improvement over the baseline. The combination of the methods further improves the results. Effect of replacement fraction $r$ in DS. Classification accuracy using DS on real images with varying $r$, i.e., fraction of the dataset being replaced with new images every epoch. The figure shows plots for DS with and without BNA. . . BibTeX . @inproceedings{besnier2020dataset, title={This dataset does not exist: training models from generated images}, author={Besnier, Victor and Jain, Himalaya and Bursuc, Andrei and Cord, Matthieu and P{ &#39;e}rez, Patrick}, booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages={1--5}, year={2020}, organization={IEEE} } . .",
          "url": "https://valeoai.github.io/blog/publications/gan-dataset/",
          "relUrl": "/publications/gan-dataset/",
          "date": ""
      }
      
  

  
      ,"page10": {
          "title": "Unsupervised Image Matching and Object Discovery as Optimization",
          "content": "Unsupervised Image Matching and Object Discovery as Optimization . Huy V. Vo&nbsp;&nbsp; Francis Bach&nbsp;&nbsp; Minsu Cho&nbsp;&nbsp; Kai Han&nbsp;&nbsp; Yann LeCun&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Jean Ponce . CVPR 2019 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Learning with complete or partial supervision is power- ful but relies on ever-growing human annotation efforts. As a way to mitigate this serious problem, as well as to serve specific applications, unsupervised learning has emerged as an important field of research. In computer vision, unsu- pervised learning comes in various guises. We focus here on the unsupervised discovery and matching of object cate- gories among images in a collection, following the work of Cho et al. [12]. We show that the original approach can be reformulated and solved as a proper optimization problem. Experiments on several benchmarks establish the merit of our approach. . . . BibTeX . @inproceedings{Vo19UOD, title = {Unsupervised image matching and object discovery as optimization}, author = {Vo, Huy V. and Bach, Francis and Cho, Minsu and Han, Kai and LeCun, Yann and P{ &#39;e}rez, Patrick and Ponce, Jean}, booktitle = {CVPR}, year = {2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/osd/",
          "relUrl": "/publications/osd/",
          "date": ""
      }
      
  

  
      ,"page11": {
          "title": "QuEST: Quantized Embedding Space for Transferring Knowledge",
          "content": "QuEST: Quantized Embedding Space for Transferring Knowledge . Himalaya Jain&nbsp;&nbsp; Spyros Gidaris&nbsp;&nbsp; Nikos Komodakis&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Matthieu Cord . ECCV 2020 . Paper&nbsp;&nbsp; . . Abstract . Knowledge distillation refers to the process of training a student network to achieve better accuracy by learning from a pre-trained teacher network. Most of the existing knowledge distillation methods direct the student to follow the teacher by matching the teacher&#39;s output, feature maps or their distribution. In this work, we propose a novel way to achieve this goal: by distilling the knowledge through a quantized visual words space. According to our method, the teacher&#39;s feature maps are first quantized to represent the main visual concepts (i.e., visual words) encompassed in these maps and then the student is asked to predict those visual word representations. Despite its simplicity, we show that our approach is able to yield results that improve the state of the art on knowledge distillation for model compression and transfer learning scenarios. To that end, we provide an extensive evaluation across several network architectures and most commonly used benchmark datasets. . . . Video . . . . BibTeX . @article{jain2019quest, title={QUEST: Quantized embedding space for transferring knowledge}, author={Jain, Himalaya and Gidaris, Spyros and Komodakis, Nikos and P{ &#39;e}rez, Patrick and Cord, Matthieu}, journal={arXiv preprint arXiv:1912.01540}, year={2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/quest/",
          "relUrl": "/publications/quest/",
          "date": ""
      }
      
  

  
      ,"page12": {
          "title": "Toward Unsupervised, Multi-Object Discovery in Large-Scale Image Collections",
          "content": "Toward Unsupervised, Multi-Object Discovery in Large-Scale Image Collections . Huy V. Vo&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Jean Ponce . CVPR 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . This paper addresses the problem of discovering the objects present in a collection of images without any supervision. We build on the optimization approach of Vo et al. [34] with several key novelties: (1) We propose a novel saliency-based region proposal algorithm that achieves significantly higher overlap with ground-truth objects than other competitive methods. This procedure leverages off-the-shelf CNN features trained on classification tasks without any bounding box information, but is otherwise unsupervised. (2) We exploit the inherent hierarchical structure of proposals as an effective regularizer for the approach to object discovery of [34], boosting its performance to significantly improve over the state of the art on several standard benchmarks. (3) We adopt a two-stage strategy to select promising proposals using small random sets of images before using the whole image collection to discover the objects it depicts, allowing us to tackle, for the first time (to the best of our knowledge), the discovery of multiple objects in each one of the pictures making up datasets with up to 20,000 images, an over five-fold increase compared to existing methods, and a first step toward true large-scale unsupervised image interpretation. . . . BibTeX . @inproceedings{Vo20rOSD, title = {Toward unsupervised, multi-object discovery in large-scale image collections}, author = {Vo, Huy V. and P{ &#39;e}rez, Patrick and Ponce, Jean}, booktitle = {Proceedings of the European Conference on Computer Vision ({ECCV})}, year = {2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/rosd/",
          "relUrl": "/publications/rosd/",
          "date": ""
      }
      
  

  
      ,"page13": {
          "title": "TRADI: Tracking deep neural network weight distributions",
          "content": "TRADI: Tracking deep neural network weight distributions . Gianni Franchi&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Emanuel Aldea&nbsp;&nbsp; Severine Dubuisson&nbsp;&nbsp; Isabelle Bloch . ECCV 2020 . Paper&nbsp;&nbsp; . . Abstract . During training, the weights of a Deep Neural Network (DNN) are optimized from a random initialization towards a nearly optimum value minimizing a loss function. Only this final state of the weights is typically kept for testing, while the wealth of information on the geometry of the weight space, accumulated over the descent towards the minimum is discarded. In this work we propose to make use of this knowledge and leverage it for computing the distributions of the weights of the DNN. This can be further used for estimating the epistemic uncertainty of the DNN by sampling an ensemble of networks from these distributions. To this end we introduce a method for tracking the trajectory of the weights during optimization, that does not require any changes in the architecture nor on the training procedure. We evaluate our method on standard classification and regression benchmarks, and on out-of-distribution detection for classification and semantic segmentation. We achieve competitive results, while preserving computational efficiency in comparison to other popular approaches. . . . Results . . Results on a synthetic regression task comparing MC dropout, Deep Ensembles, and TRADI. $x$-axis: spatial coordinate of the Gaussian process. Black lines: ground truth curve. Blue points: training points. Orange areas: estimated variance. . Distinguishing in- and out-of-distribution data for semantic segmentation (CamVid, StreetHazards, BDD Anomaly) and image classification (MNIST/notMNIST). . Qualitative results on CamVid-OOD. Columns: $(a)$ input image and ground truth; $(b)-(d)$ predictions and confidence scores by MC Dropout, Deep Ensembles, and TRADI. Rows: $(1)$ input and confidence maps; $(2)$ class predictions; $(3)$ zoomed-in area on input and confidence maps . BibTeX . @article{franchi2019tradi, title={TRADI: Tracking deep neural network weight distributions}, author={Franchi, Gianni and Bursuc, Andrei and Aldea, Emanuel and Dubuisson, S{ &#39;e}verine and Bloch, Isabelle}, journal={arXiv preprint arXiv:1912.11316}, year={2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/tradi/",
          "relUrl": "/publications/tradi/",
          "date": ""
      }
      
  

  
      ,"page14": {
          "title": "xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation",
          "content": "xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation . Maximilian Jaritz&nbsp;&nbsp; Tuan-Hung Vu&nbsp;&nbsp; Raoul de Charette&nbsp;&nbsp; Émilie Wirbel&nbsp;&nbsp; Patrick Pérez . CVPR 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; Blog &nbsp;&nbsp; . . Abstract . Unsupervised Domain Adaptation (UDA) is crucial to tackle the lack of annotations in a new domain. There are many multi-modal datasets, but most UDA approaches are uni-modal. In this work, we explore how to learn from multi-modality and propose cross-modal UDA (xMUDA) where we assume the presence of 2D images and 3D point clouds for 3D semantic segmentation. This is challenging as the two input spaces are heterogeneous and can be impacted differently by domain shift. In xMUDA, modalities learn from each other through mutual mimicking, disentangled from the segmentation objective, to prevent the stronger modality from adopting false predictions from the weaker one. We evaluate on new UDA scenarios including day-to-night, country-to-country and dataset-to-dataset, leveraging recent autonomous driving datasets. xMUDA brings large improvements over uni-modal UDA on all tested scenarios, and is complementary to state-of-the-art UDA techniques. Code is available at: https://github.com/valeoai/xmuda . . Video . . . . BibTeX . @inproceedings{jaritz2020xmuda, title={xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation}, author={Jaritz, Maximilian and Vu, Tuan-Hung and Charette, Raoul de and Wirbel, Emilie and P{ &#39;e}rez, Patrick}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={12605--12614}, year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/xmuda/",
          "relUrl": "/publications/xmuda/",
          "date": ""
      }
      
  

  
      ,"page15": {
          "title": "Zero-Shot Semantic Segmentation",
          "content": "Zero-Shot Semantic Segmentation . Maxime Bucher&nbsp;&nbsp; Tuan-Hung Vu&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . NeurIPS 2019 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Semantic segmentation models are limited in their ability to scale to large numbers of object classes. In this paper, we introduce the new task of zero-shot semantic segmentation: learning pixel-wise classifiers for never-seen object categories with zero training examples. To this end, we present a novel architecture, ZS3Net, combining a deep visual segmentation model with an approach to generate visual representations from semantic word embeddings. By this way, ZS3Net addresses pixel classification tasks where both seen and unseen categories are faced at test time (so called &quot;generalized&quot; zero-shot classification). Performance is further improved by a self-training step that relies on automatic pseudo-labeling of pixels from unseen classes. On the two standard segmentation datasets, Pascal-VOC and Pascal-Context, we propose zero-shot benchmarks and set competitive baselines. For complex scenes as ones in the Pascal-Context dataset, we extend our approach by using a graph-context encoding to fully leverage spatial context priors coming from class-wise segmentation maps. . . . BibTeX . @inproceedings{bucher2019zero, title={Zero-shot semantic segmentation}, author={Bucher, Maxime and Tuan-Hung, VU and Cord, Matthieu and P{ &#39;e}rez, Patrick}, booktitle={Advances in Neural Information Processing Systems}, pages={468--479}, year={2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/zs3/",
          "relUrl": "/publications/zs3/",
          "date": ""
      }
      
  


}