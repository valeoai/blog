{
  
    
        "post0": {
            "title": "valeo.ai at NeurIPS 2023",
            "content": "The Neural Information Processing Systems Conference (NeurIPS) is a major inter-disciplinary event that brings together researchers and practicioners in machine learning, computer vision, natural language processing, optimization, statistics, but also neuroscience, natural sciences, social sciences, etc. This year, at the thirty-seventh edition of NeurIPS, the valeo.ai team will present 4 papers in the main conference and 1 in the workshops. . Notably, we explore perception via different sensors, e.g., audio, on the path towards increasingly autonomous systems. We also study the interaction between different sensing modalities (images, language, Lidar point clouds) and advance a tri-modal self-supervised learning algorithm for 3D semantic voxel occupancy prediction from a rig of cameras mounted on a vehicle. We further show how to obtain robust deep models starting from pre-trained foundation models finetuned with reinforcement learning from human feedback. Finally, we analyze different generative models (diffusion models, GANs) and advance a unification framework considering them as instances of Particle Models. . We will be happy to discuss more about these projects and ideas, and share our exciting ongoing research. Take a quick view of our papers below and come meet us at the posters or catch us for a coffee in the hallways. . Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis . Authors: Victor Letzelter, Mathieu Fontaine, Mickaël Chen, Patrick Pérez, Slim Essid, Gaël Richard . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . In this work, we tackle ambiguous machine learning tasks, where single predictions don’t suffice due to the task’s nature or inherent uncertainties. We introduce a robust multi-hypotheses framework that is capable of deterministically offering a range of plausible predictions at inference time. Our experiments on both synthetic data and real-world audio data affirm the potential and versatility of our method. Check out the paper and the code for more details. . . This problem involves estimating a conditional distribution that is dependent on the input. The accompanying animation illustrates the early stages in the evolution of our model’s learning process, highlighting how it progressively refines its predictions (represented by shaded blue points) to the actual data distribution (indicated by green points), which varies with the input ‘t’. . . POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images . Authors: Antonin Vobecky, Oriane Siméoni, David Hurych, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Josef Sivic . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . POP-3D is an approach to predict open-vocabulary 3D semantic voxel occupancy map from input 2D images to enable 3D grounding, segmentation, and retrieval of free-form language queries. . . Given surround-view images on the input, our POP-3D outputs voxel occupancy with 3D-language features, which one can query using text, e.g., to obtain zero-shot semantic segmentation. We design a new model architecture for open-vocabulary 3D semantic occupancy prediction. The architecture consists of a 2D-3D encoder together with occupancy prediction and 3D-language heads. The output is a dense voxel map of 3D grounded language embeddings enabling a range of open-vocabulary tasks. Next, we develop a tri-modal self-supervised learning algorithm that leverages three modalities: (i) images, (ii) language, and (iii) LiDAR point clouds and enables training the proposed architecture using a strong pre-trained vision-language model without the need for any 3D manual language annotations. . . Overview of POP-3D architecture and training approach. Finally, we demonstrate the strengths of the proposed model quantitatively on several open-vocabulary tasks: Zero-shot 3D semantic segmentation using existing datasets; 3D grounding, and retrieval of free-form language queries, using a small dataset that we propose as an extension of nuScenes. . . . Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards . Authors: Alexandre Ramé, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste Gaya, Laure Soulier, Matthieu Cord . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . Foundation models are first pre-trained on vast unsupervised datasets and then fine-tuned on labeled data. Reinforcement learning, notably from human feedback (RLHF), can further align the network with the intended usage. Yet the imperfections in the proxy reward may hinder the training and lead to suboptimal results; the diversity of objectives in real-world tasks and human opinions exacerbate the issue. This paper proposes embracing the heterogeneity of diverse rewards by following a multi-policy strategy. Rather than focusing on a single a priori reward, we aim for Pareto-optimal generalization across the entire space of preferences. To this end, we propose rewarded soup, first specializing multiple networks independently (one for each proxy reward) and then interpolating their weights linearly. This succeeds empirically because we show that the weights remain linearly connected when fine-tuned on diverse rewards from a shared pre-trained initialization. We demonstrate the effectiveness of our approach for text-to-text (summarization, Q&amp;A, helpful assistant, review), text-image (image captioning, text-to-image generation, visual grounding, VQA), and control (locomotion) tasks. We hope to enhance the alignment of deep models, and how they interact with the world in all its diversity. . . Illustration of the different steps of our proposed rewarded soup (RS). After unsupervised pre-training and supervised fine-tuning, we launch $N$ independent RL fine-tunings on the proxy rewards $ {R_i }^{N}_{i=1}$. Then we combine the trained networks by interpolation in the weight space. The final weights are adapted at test time by selecting the coefficient $ lambda$. . Unifying GANs and Score-Based Diffusion as Generative Particle Models . Authors: Jean-Yves Franceschi, Mike Gartrell, Ludovic Dos Santos, Thibaut Issenhuth, Emmanuel de Bézenac, Mickaël Chen, Alain Rakotomamonjy . [Paper] &nbsp;&nbsp; [Code (coming soon)] . By describing the trajectories of GAN outputs during training with particle evolution equations, we propose an unifying framework for GAN and Diffusion Models. We provide a new insights on the role of the generator network, and as proof of concept validating our theories, we propose methods to train a generator with score-based gradient instead of a discriminator, or to use a discriminator’s gradient flow to generate instead of training a generator. . . . Evaluating the structure of cognitive tasks with transfer learning . NeurIPS Workshop on AI for Scientific Discovery: From Theory to Practice . Authors: Bruno Aristimunha, Raphael Y. de Camargo, Walter H. Lopez Pinaya, Sylvain Chevallier, Alexandre Gramfort, Cedric Rommel . [Paper] &nbsp;&nbsp; [Code (coming soon)] . Electroencephalography (EEG) decoding is a challenging task due to the limited availability of labeled data. While transfer learning is a promising technique to address this challenge, it assumes that transferable data domains and tasks are known, which is not the case in this setting. This work investigates the transferability of deep learning representations between different EEG decoding tasks. . . Learned transferability maps for both datasets. Each node corresponds to a distinct cognitive task. Arrow width represents the average transfer performance when using the representations learned from a source task to decode a target task. We conduct extensive experiments using state-of-the-art decoding models on two recently released EEG datasets, ERPCore and M3CV, containing over 140 subjects and 11 distinct cognitive tasks. . From an EEG processing perspective, our results can be used to leverage related datasets for alleviating EEG data scarcity with transfer learning. We show that even with a linear probing transfer method, we are able to boost by up to 28% the performance of some tasks. From a neuroscientific standpoint, our transfer maps provide insights into the hierarchical relations between cognitive tasks, hence enhancing our understanding of how these tasks are connected. We discover for example evidence that certain decoding paradigms elicit specific and narrow brain activities, while others benefit from pre-training on a broad range of representations. .",
            "url": "https://valeoai.github.io/blog/2023/12/08/valeoai-at-neurips-2023.html",
            "relUrl": "/2023/12/08/valeoai-at-neurips-2023.html",
            "date": " • Dec 8, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "valeo.ai at ICCV 2023",
            "content": "The IEEE / CVF International Conference on Computer Vision (ICCV) is a landmark event for the increasingly large and diverse community of researchers in computer vision and machine learning. This year, ICCV takes place in Paris, home of the valeo.ai team. From interns to senior researchers, the valeo.ai team will participate in mass at ICCV and will be looking forward to welcoming you and talking about the exciting progress and ideas in the field. . At ICCV 2023 we will present 5 papers in the main conference and 3 in the workshops. We are also organizing 2 tutorials with 2 challenges (BRAVO and UNCV) and a tutorial (Many Faces of Reliability). Take a quick view of our papers in the conference and come meet us at the posters, at our booth or in the hallway. . Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation . Authors: Gilles Puy, Alexandre Boulch, Renaud Marlet . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . Semantic segmentation of point clouds delivered by lidars permits autonomous vehicles to make sense of their 3D surrounding environment. Sparse convolutions have become a de-facto tool to process these large outdoor point clouds. The top performing methods on public benchmarks, such SemanticKITTI or nuScenes, all leverage sparse convolutions. Nevertheless, despite their undeniable success and efficiency, these convolutions remain available in a limited number of deep learning frameworks and hardware platforms. In this work, we propose an alternative backbone built with tools broadly available (such as 2D and 1D convolutions) but that still reaches the level of performance of the top methods on automotive datasets. . We propose a point-based backbone, called WaffleIron, which is essentially built using standard MLPs and dense 2D convolutions, both readily available in all deep learning frameworks thanks to their wide use in the field of computer vision. The architecture of this backbone is illustrated in the figure below. It is inspired by the recent MLP-Mixer. It takes as input a point cloud with a token associated to each point. All these point tokens are then updated by a sequence of layers, each containing a token-mixing step (made of dense 2D convolutions) and a channel-mixing step (made of a MLP shared across points). . . The WaffleIron backbone takes as input point tokens, provided by an embedding layer (not represented), and updates these point representations L times via a point token-mixing layer (containing the WI block) followed by a channel-mixing layer. The WI block consists of a 2D projection along one of the main axes, a feed-forward network (FFN) with two dense channel-wise 2D convolutions with a ReLU activation in the hidden layer, and a simple copy of the 2D features to the 3D points. The channel-mixing layer contains a batch-norm, a MLP shared across each point, and a residual connection. The WaffleIron backbone is free of any point downsampling or upsampling layer, farthest point sampling, nearest neighbor search, or sparse convolution. WaffleIron has three main hyperparameters to tune: the depth L, the width F and the resolution of the 2D grid. We show that these parameters are easy to tune: the performance increases with the network width F and depth L, until an eventual saturation; we observe stable results over a wide range of values for the resolution of the 2D grid. . In our paper, we also provide many details on how to train WaffleIron to reach the performance of top-entries on two autonomous driving benchmarks: SemanticKITTI and nuScenes. . . PØDA: Prompt-driven Zero-shot Domain Adaptation . Authors: Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Video] &nbsp;&nbsp; [Project page] . Domain adaptation has been vastly investigated in computer vision but still requires access to target images at train time, which might be intractable in some uncommon conditions. In this paper, we propose the task of ‘Prompt-driven Zero-shot Domain Adaptation’, where we adapt a model trained on a source domain using only a general description in natural language of the target domain, i.e., a prompt. First, we leverage a pre-trained contrastive vision-language model (CLIP) to optimize affine transformations of source features, steering them towards the target text embedding while preserving their content and semantics. To achieve this, we propose Prompt-driven Instance Normalization (PIN). Second, we show that these prompt-driven augmentations can be used to perform zero-shot domain adaptation for semantic segmentation. Experiments demonstrate that our method significantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand, even surpassing one-shot unsupervised domain adaptation. A similar boost is observed on object detection and image classification . . We perform zero-shot adaptation with natural language prompts. PØDA enables the adaptation of a segmenter model (here, DeepLabv3+ trained on the source dataset Cityscapes) to unseen conditions with only a prompt. Source-only predictions are shown as smaller segmentation masks to the left or right of the test images. . You Never Get a Second Chance To Make a Good First Impression: Seeding Active Learning for 3D Semantic Segmentation . Authors: Nermin Samet, Oriane Siméoni, Gilles Puy, Georgy Ponimatkin, Renaud Marlet, Vincent Lepetit . [Paper] &nbsp;&nbsp; [Code] . We are interested in the efficient annotation of sparse 3D point clouds (as captured indoors by depth cameras or outdoors by automotive lidars) for semantic segmentation. Active Learning (AL) iteratively selects relevant data fractions to annotate within a given budget, but requires a first fraction of the dataset (a ’seed’) to be already annotated to estimate the benefit of annotating other data fractions. We show that the choice of the seed can significantly affect the performance of many AL methods and propose a method, named SeedAL, for automatically constructing a seed that will ensure good performance for AL. Assuming that images of the point clouds are available, which is common, our method relies on powerful unsupervised image features to measure the diversity of the point clouds. It selects the point clouds for the seed by optimizing the diversity under an annotation budget, which can be done by solving a linear optimization problem. Our experiments demonstrate the effectiveness of our approach compared to random seeding and existing methods on both the S3DIS and SemanticKitti datasets. . . Impact of active learning seed on performance. We show the variability of results obtained with 20 different random seeds (blue dashed lines), within an initial annotation budget of 3% of the dataset, when using various active learning methods for 3D semantic segmentation of S3DIS. We compare it to the result obtained with our seed selection strategy (solid red line), named SeedAL, which performs better or on par with the best (lucky) random seeds among 20, and “protects” from very bad (unlucky) random seeds. . eP-ALM: Efficient Perceptual Augmentation of Language Models . Authors: Mustafa Shukor, Corentin Dancette, Matthieu Cord . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . eP-ALM aims to augment large language models (LLMs) with perception. While most existing approaches train a large number of parameters and rely on extensive multimodal pre-training, we investigate the minimal computational effort required to adapt unimodal models to multimodal tasks. We show that by freezing more than 99% of total parameters, training only one linear projection layer and prepending only one trainable token, our approach (dubbed eP-ALM) significantly outperforms other baselines on VQA and captioning for image, video and audio modalities. . . Illustration of the adaptation mechanism in eP-ALM. The perceptual input (image/video/audio) is fed to the perceptual encoder E (e.g., ViT) and the corresponding text to the LM (e.g., OPT), which then generates a text conditioned on the perceptual input. The multimodal interaction is done via the [CLS] tokens acting as Perceptual Prompt, and are extracted from the last layers of the encoder, then injected in the last layers of LM, after passing by the Linear Connection C. The previous [CLS] token is replaced by the new one coming from a deeper layer, keeping the number of tokens fixed. The first layers (grayed) of each model are kept intact without any modality interaction. We ease the adaptation with a Soft Prompt that is prepended to the input of LM. . Zero-shot spatial layout conditioning for text-to-image diffusion models . Authors: Guillaume Couairon, Marlène Careil, Matthieu Cord, Stéphane Lathuilière, Jakob Verbeek . [Paper] . Large-scale text-to-image diffusion models have considerably improved the state of the art in generative image modeling, and provide an intuitive and powerful user interface to drive the image generation process. In this paper, we propose ZestGuide, a “zero-shot” segmentation guidance approach that can be integrated into pre-trained text-image diffusion models, and requires no additional training. It exploits the implicit segmentation maps that can be extracted from cross-attention layers, and uses them to align generation with input masks. . . ZestGuide generates images conditioned on segmentation maps with corresponding free-form textual descriptions. . DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion . ICCV Workshop on Analysis and Modeling of Faces and Gestures . Authors: Cédric Rommel, Eduardo Valle, Mickaël Chen, Souhaiel Khalfaoui, Renaud Marlet, Matthieu Cord, Patrick Pérez . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . Diffusion models are making waves across various domains, including computer vision, natural language processing and time-series analysis. However, its application to purely predictive tasks, such as 3D human pose estimation (3D-HPE), remains largely unexplored. While a few pioneering works have shown promising performance metrics in 3D-HPE, the understanding of the benefits of diffusion models over classical supervision — as well as key design choices — is still in its infancy. In this work, we address those concerns, providing an in-depth analysis of the effects of diffusion models on 3D-HPE. . . Poses across the learned reverse diffusion process converge to an accurate 3D reconstruction of the corresponding 2D pose in pixel space. More precisely, we propose DiffHPE, a novel strategy to use diffusion models in 3D-HPE, and show that combining diffusion with pre-trained supervised models allows to outperform both pure diffusion and pure supervised models trained separately. Our analysis demonstrates not only that the diffusion framework can be used to enhance accuracy, as previously understood, but also that it can improve robustness and coherence. Namely, our experiments showcase how poses estimated with diffusion models’ display better bilateral and temporal coherence, and are more robust to occlusions, even when not perfectly trained for the latter. . . Challenges of Using Real-World Sensory Inputs for Motion Forecasting in Autonomous Driving . ROAD++: The Second Workshop and Challenge on Event Detection for Situation Awareness in Autonomous Driving . Authors: Yihong Xu, Loïck Chambon, Éloi Zablocki, Mickaël Chen, Matthieu Cord, Patrick Pérez . [Paper] &nbsp;&nbsp; [Project page] . Motion forecasting is crucial in enabling autonomous vehicles to anticipate the future trajectories of surrounding agents. To do so, it requires solving mapping, detection, tracking, and then forecasting problems, in a multi-step pipeline. In this complex system, advances in conventional forecasting methods have been made using curated data, i.e., with the assumption of perfect maps, detection, and tracking. This paradigm, however, ignores any errors from upstream modules. Meanwhile, an emerging end-to-end paradigm, that tightly integrates the perception and forecasting architectures into joint training, promises to solve this issue. So far, however, the evaluation protocols between the two methods were incompatible and their comparison was not possible. In fact, and perhaps surprisingly, conventional forecasting methods are usually not trained nor tested in real-world pipelines (e.g., with upstream detection, tracking, and mapping modules). In this work, we aim to bring forecasting models closer to real-world deployment. First, we propose a unified evaluation pipeline for forecasting methods with real-world perception inputs, allowing us to compare the performance of conventional and end-to-end methods for the first time. Second, our in-depth study uncovers a substantial performance gap when transitioning from curated to perception-based data. In particular, we show that this gap (1) stems not only from differences in precision but also from the nature of imperfect inputs provided by perception modules, and that (2) is not trivially reduced by simply finetuning on perception outputs. Based on extensive experiments, we provide recommendations for critical areas that require improvement and guidance towards more robust motion forecasting in the real world. We will release an evaluation library to benchmark models under standardized and practical conditions. . . Study overview. We study the challenges of deploying motion forecasting models into the real world when only predicted perception inputs are available. We compare: (1) (top) &quot;conventional methods&quot; (i.e., methods trained on curated input data) where (middle) we directly replace the curated inputs with real-world data, and (2) (bottom) &quot;end-to-end methods&quot; that are trained and used with perception modules. In the real-world setting, evaluation is challenging as the past tracks are estimated with arbitrary identities, making it difficult to establish a direct correspondence to GT identities. Therefore, we propose a matching process (purple) to assign predictions to GT and thus evaluate forecasting performances. Moreover, we study in depth the impact changing from curated data (green) to real-world (orange) mapping, or detection and tracking errors to motion forecasting. . POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images . ICCV 2023 Workshop on Open-Vocabulary 3D Scene Understanding (OpenSUN 3D) . Authors: Antonin Vobecky, Oriane Siméoni, David Hurych, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Josef Sivic . [Paper] . We propose an approach to predict a 3D semantic voxel occupancy map from input 2D images with features allowing 3D grounding, segmentation and retrieval of free-form language queries. To this end: We design a new architecture that consists of a 2D-3D encoder together with occupancy prediction and 3D-language heads; We develop a tri-modal self-supervised training that leverages three modalities – images, language and LiDAR point clouds– and enables learning the proposed architecture using a strong pre-trained vision-language model without the need for any 3D manual annotations. We quantitatively evaluate the proposed model on the task of zero-shot 3D semantic segmentation using existing datasets and show results on the tasks of 3D grounding and retrieval of free-form language queries. . . Method overview.Given surround-view images, POP-3D produces a voxel grid of text-aligned features that support open-vocabulary downstream tasks such as zero-shot occupancy segmentation or text-based grounding and retrieval.",
            "url": "https://valeoai.github.io/blog/2023/09/26/valeoai-at-iccv-2023.html",
            "relUrl": "/2023/09/26/valeoai-at-iccv-2023.html",
            "date": " • Sep 26, 2023"
        }
        
    
  
    
        ,"post2": {
            "title": "valeo.ai at CVPR 2023",
            "content": "The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR) is a key event for researchers and engineers working on computer vision and machine learning. At the 2023 edition the valeo.ai team will present six papers in the main conference, one workshop keynote and organize a tutorial. The team will be at CVPR to present these works and will be happy to discuss more about these projects and ideas, and share our exciting ongoing research. We outline four of our team papers below. . OCTET: Object-aware Counterfactual Explanations . Authors: Mehdi Zemni, Mickaël Chen, Éloi Zablocki, Hédi Ben-Younes, Patrick Pérez, Matthieu Cord . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Video] &nbsp;&nbsp; [Project page] . Nowadays, deep vision models are being widely deployed in safety-critical applications, e.g., autonomous driving, and explainability of such models is becoming a pressing concern. Among explanation methods, counterfactual explanations aim to find minimal and interpretable changes to the input image that would also change the output of the model to be explained. Such explanations point end-users at the main factors that impact the decision of the model. However, previous methods struggle to explain decision models trained on images with many objects, e.g., urban scenes, which are more difficult to work with but also arguably more critical to explain. In this work, we propose to tackle this issue with an object-centric framework for counterfactual explanation generation. Our method, inspired by recent generative modeling works, encodes the query image into a latent space that is structured in a way to ease object-level manipulations. Doing so, it provides the end-user with control over which search directions (e.g., spatial displacement of objects, style modification, etc.) are to be explored during the counterfactual generation. We conduct a set of experiments on counterfactual explanation benchmarks for driving scenes, and we show that our method can be adapted beyond classification, e.g., to explain semantic segmentation models. To complete our analysis, we design and run a user study that measures the usefulness of counterfactual explanations in understanding a decision model. . . Counterfactual explanations generated by OCTET. Given a classifier that predicts whether or not it is possible to go left, and a query image (top left), OCTET produces a counterfactual explanation where the most influential features that led to the decision are changed (top right). On the bottom row, we show that OCTET can also operate under different settings that result in different focused explanations. We report the prediction made by the decision model at the top left of each image. . ALSO: Automotive Lidar Self-supervision by Occupancy estimation . Authors: Alexandre Boulch, Corentin Sautier, Björn Michele, Gilles Puy, Renaud Marlet . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Video] &nbsp;&nbsp; [Project page] . We propose a new self-supervised method for pre-training the backbone of deep perception models operating on point clouds. The core idea is to train the model on a pretext task which is the reconstruction of the surface on which the 3D points are sampled, and to use the underlying latent vectors as input to the perception head. The intuition is that if the network is able to reconstruct the scene surface, given only sparse input points, then it probably also captures some fragments of semantic information that can be used to boost an actual perception task. This principle has a very simple formulation, which makes it both easy to implement and widely applicable to a large range of 3D sensors and deep networks performing semantic segmentation or object detection. In fact, it supports a single-stream pipeline, as opposed to most contrastive learning approaches, allowing training on limited resources. We conducted extensive experiments on various autonomous driving datasets, involving very different kinds of lidars, for both semantic segmentation and object detection. The results show the effectiveness of our method to learn useful representations without any annotation, compared to existing approaches. . . ALSO overview. The backbone to pre-train produces latent vectors for each input point. At pre-training time, the latent vector are fed into an volumetric occupancy head that classifies query points as full or empty. At semantic training or test time, the same latent vectors are fed into a semantic head, e.g., for semantic segmentation or object detection. . Unsupervised Object Localization: Observing the Background to Discover Objects . Authors: Oriane Siméoni, Chloé Sekkat, Gilles Puy, Antonin Vobecky, Éloi Zablocki, Patrick Pérez . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Video] &nbsp;&nbsp; [Project page] . Recent advances in self-supervised visual representation learning have paved the way for unsupervised methods tackling tasks such as object discovery and instance segmentation. However, discovering objects in an image with no supervision is a very hard task; what are the desired objects, when to separate them into parts, how many are there, and of what classes? The answers to these questions depend on the tasks and datasets of evaluation. In this work, we take a different approach and propose to look for the background instead. This way, the salient objects emerge as a by-product without any strong assumption on what an object should be. We propose FOUND, a simple model made of a single conv1 × 1 initialized with coarse background masks extracted from self-supervised patch-based representations. After fast training and refining these seed masks, the model reaches state-of-the-art results on unsupervised saliency detection and object discovery benchmarks. Moreover, we show that our approach yields good results in the unsupervised semantic segmentation retrieval task. . . Overview of FOUND. In the first stage (green upperpart), a background mask is discovered by mining a seed patch through a reweighting of the self-attention maps of a frozen DINO self-supervised features. This seed is then used to find similar patches likely belonging to the background. In the second stage (blue lower part), we train a lightweight 1 × 1 convolutional layer that produces refined masks from the self-supervised features. It is trained in a self-supervised fashion to predict both smoothed inverse coarse masks of the first step, and smoothed binarized version of its own output. Blue arrows denote where the gradients flow (in the reverse direction). . RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving . Authors: Angelika Ando, Spyros Gidaris, Andrei Bursuc, Gilles Puy, Alexandre Boulch, Renaud Marlet . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Video] &nbsp;&nbsp; [Project page] . Semantic segmentation of LiDAR point clouds permits vehicles to perceive their surrounding 3D environment independently of the lighting condition, providing useful information to build safe and reliable vehicles. A common approach to segment large scale LiDAR point clouds is to project the points on a 2D surface and then to use regular CNNs, originally designed for images, to process the projected point clouds. These projection-based methods usually benefit from fast computations and, when combined with techniques which use other point cloud representations, achieve state-of-the-art results. . Today, projection-based methods leverage 2D CNNs but recent advances in computer vision show that vision transformers (ViTs) have achieved state-of-the-art results in many image-based benchmarks. Despite the absence of almost any domain-specific inductive bias apart from the image tokenization process, ViTs have a strong representation learning capacity and achieve excellent results on various image perception tasks, such as image classification, object detection or semantic segmentation. Inspired by this success of ViTs for image understanding, in this work, we show that projection-based methods for 3D semantic segmentation can benefit from these latest improvements on ViTs when combined with three key ingredients, all described in our paper. . . Exploiting vision transformer (ViT) architectures and weights for LiDAR point cloud semantic segmentation. We leverage the flexibility of transformer-based architectures to re-purpose them with minimal changes for processing sparse point clouds in autonomous driving tasks. The common ViT backbone across modalities allows to effectively transfer weights pre-trained on large image repositories towards improving point cloud segmentation performance with fine-tuning. We answer positively but only after combining them with three key ingredients: (a) ViTs are notoriously hard to train and require a lot of training data to learn powerful representations. By preserving the same backbone architecture as for RGB images, we can exploit the knowledge from long training on large image collections that are much cheaper to acquire and annotate than point clouds. We reach our best results with pre-trained ViTs on large image datasets. (b) We compensate for ViTs’ lack of inductive bias by substituting a tailored non-linear convolutional stem for the classical linear embedding layer. (c) We refine pixel-wise predictions with a convolutional decoder and a skip connection from the convolutional stem to combine low-level but fine-grained features of the convolutional stem with the high-level but coarse predictions of the ViT encoder. With these ingredients, we show that our method, called RangeViT, outperforms prior projection-based methods on nuScenes and SemanticKITTI. . . Overview of RangeViT architecture. First, the point cloud is projected in a 2D space with range projection. Then, the produced range image is processed by the convolutional stem, the ViT encoder and the decoder to obtain a 2D feature map. It is then processed by a 3D refiner layer for 3D point-wise predictions. Note that there is a single skip connection between the convolutional stem and the decoder. In summary, our work offers the following contributions: . Exploiting the powerful representation learning capacity of vision transformers for LiDAR semantic segmentation. | Unifying the network architectures for processing LiDAR point clouds and images, enabling advancements in one domain to benefit both. | Demonstrating the utilization of pre-trained ViTs on large-scale natural image datasets for LiDAR point cloud segmentation. | . We believe that this finding is highly intriguing. The RangeViT approach can leverage off-the-shelf pre-trained ViT models, enabling direct benefits from ongoing and future advances in training ViT models with natural RGB images - a rapidly growing research field. .",
            "url": "https://valeoai.github.io/blog/2023/06/14/valeoai-at-cvpr-2023.html",
            "relUrl": "/2023/06/14/valeoai-at-cvpr-2023.html",
            "date": " • Jun 14, 2023"
        }
        
    
  
    
        ,"post3": {
            "title": "valeo.ai at CVPR 2022",
            "content": "The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR) is a major event for researchers and engineers working on computer vision and machine learning. At the 2022 edition the valeo.ai team will present four papers in the main conference, three papers in workshops and one workshop keynote. The team will be at CVPR to present these works and will be happy to discuss more about these projects and ideas, and share our exciting ongoing research. . Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data . Authors: Corentin Sautier, Gilles Puy, Spyros Gidaris, Alexandre Boulch, Andrei Bursuc, Renaud Marlet . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . Self-driving vehicles require object detection or segmentation to safely maneuver in their environment. Such safety-critical tasks are usually performed by neural networks demanding huge Lidar datasets with high quality annotations, and no domain shift between training and testing conditions. However, annotating 3D Lidar data for these tasks is tedious and costly. In Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data, we propose a self-supervised pre-training method for 3D perception models that is tailored to autonomous driving data and that does not require any annotation. Specifically, we leverage the availability of synchronized and calibrated image and Lidar data in autonomous driving setups for distilling self-supervised pre-trained image representations into 3D models, using neither point cloud nor image annotations. . . Synchronized Lidar and camera frames are encoded through two modality-specific features extractors. The camera backbone has pre-trained weights obtained with no annotations (e.g., with MoCo v2 (Chen et al., 2020)). Features are pooled at a pseudo-object level using image superpixels, and contrasted between both modalities A key ingredient of our method is the use of superpixels which are used to pool 3D point features and 2D pixel features in visually similar regions. We then train a 3D network on the self-supervised task of matching these pooled 3D-point features with the corresponding pooled image pixel features. Extensive experiments on autonomous driving datasets demonstrate the ability of our image-to-Lidar distillation strategy to produce 3D representations that transfer well to semantic segmentation and object detection tasks. . . The similarity between a query point&#39;s features (in red) and all other Lidar points is shown, to assert the quality of the learned representation. Color scale goes from purple (low similarity) to yellow (high similarity). With our pre-training, a Lidar network can learn features that are mostly consistent within an object class. This pre-training greatly improves data annotation efficiency, both in semantic segmentation and object detection, and is even applicable in cross-dataset setups. . . Raw High-Definition Radar for Multi-Task Learning . Authors: Julien Rebut, Arthur Ouaknine, Waqas Walik, Patrick Pérez . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . With their robustness to adverse weather conditions and their ability to measure speeds, radar sensors have been part of the automotive landscape for more than two decades. Recent progress toward High Definition (HD) Imaging radars has driven the angular resolution below the degree, thus approaching laser scanning performance. However, the amount of data a HD radar delivers and the computational cost to estimate the angular positions remain a challenge. In this paper, we propose a novel HD radar sensing model, FFT-RadNet, that eliminates the overhead of computing the range-azimuth-Doppler 3D tensor, learning instead to recover angles from a range-Doppler spectrum. This architecture can be leveraged for various perception tasks with raw HD radar signals. In particular we show how to train FFT-RadNet both to detect vehicles and to segment free driving space. On both tasks, it competes with the most recent radar-based models while requiring less compute and memory. . . Overview of FFT-RadNet for vehicle detection and drivable space segmentation in raw HD radar signal. Also, and importantly, we collected and annotated 2-hour worth of raw data from synchronized automotive-grade sensors (camera, laser, HD radar) in various environments (city street, highway, countryside road). This unique dataset, nick-named RADIal for “Radar, Lidar et al.”, is publicly available. . . Scene sample form RADIal dataset with (a) camera image, (b) radar power spectrum, (c) free-space in bird-eye view, (d) Range-azimuth map in Cartesian coordinates, and (e) GPS trace (red) and odometry trajectory (green); laser (resp. radar) points are in red (resp. indigo), annotated vehicle bounding boxes in orange and annotated drivable space in green. . POCO: Point convolution for surface reconstruction . Authors: Alexandre Boulch, Renaud Marlet . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . Implicit neural networks have been successfully used for surface reconstruction from point clouds. However, many of them face scalability issues as they encode the isosurface function of a whole object or scene into a single latent vector. To overcome this limitation, a few approaches infer latent vectors on a coarse regular 3D grid or on 3D patches, and interpolate them to answer occupancy queries. In doing so, they lose the direct connection with the input points sampled on the surface of objects, and they attach information uniformly in space rather than where it matters the most, i.e., near the surface. Besides, relying on fixed patch sizes may require discretization tuning. . In POCO, we propose to use point cloud convolution and compute a latent vector at each input point. We then perform a learning-based interpolation on nearest neighbors using inferred weights. On the one hand, using a convolutional backbone allows the aggregation of global information about the shape needed to correctly orientate the surface (decide which side of the surface is inside or outside). On the other hand, surface location is inferred via a local attention-based approach which enables accurate surface positioning. . . POCO overview. Top row: the decoding mechanism takes as input local latent vectors and local coordinates which are lifted with a point-wise MLP. The resulting representations are weighted with an attention mechanism in order to take the occupancy decision. Bottom row: reconstruction examples with POCO, scene reconstruction with a model trained on objects (left), object reconstruction with noisy point cloud (middle) and out of domain object reconstruction (right). We show that our approach, while being very simple to set up, reaches the state of the art on several reconstruction-from-point-cloud benchmarks. It underlines the importance of reasoning about the surface location at a local scale, close to the input points. POCO also shows good generalization properties including the possibility of learning on object datasets while being able to reconstruct complex scenes. . . DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion . Authors: Arthur Douillard, Alexandre Ramé, Guillaume Couairon, Matthieu Cord . [Paper] &nbsp;&nbsp; [Code] . Deep network architectures struggle to continually learn new tasks without forgetting the previous tasks. In this paper, we propose a transformer architecture based on a dedicated encoder/decoder framework. Critically, the encoder and decoder are shared among all tasks. Through a dynamic expansion of special tokens, we specialize each forward of our decoder network on a task distribution. Our strategy scales to a large number of tasks while having negligible memory and time overheads due to strict control of the parameters expansion. Moreover, this efficient strategy does not need any hyperparameter tuning to control the network’s expansion. Our model reaches excellent results on CIFAR100 and state-of-the-art performances on the large-scale ImageNet100 and ImageNet1000 while having fewer parameters than concurrent dynamic frameworks. . . DyTox transformer model. . FlexIT: Towards Flexible Semantic Image Translation . Authors: Guillaume Couairon, Asya Grechka, Jakob Verbeek, Holger Schwenk, Matthieu Cord . [Paper] . Deep generative models, like GANs, have considerably improved the state of the art in image synthesis, and are able to generate near photo-realistic images in structured domains such as human faces. We propose FlexIT, a novel method which can take any input image and a user-defined text instruction for editing. Our method achieves flexible and natural editing, pushing the limits of semantic image translation. First, FlexIT combines the input image and text into a single target point in the CLIP multimodal embedding space. Via the latent space of an auto-encoder, we iteratively transform the input image toward the target point, ensuring coherence and quality with a variety of novel regularization terms. We propose an evaluation protocol for semantic image translation, and thoroughly evaluate our method on ImageNet. . . FlexIT transformation examples. From top to bottom: input image, transformed image, and text query. . Raising context awareness in motion forecasting . CVPR 2022 Workshop on Autonomous Driving . Authors: Hédi Ben-Younes, Éloi Zablocki, Mickaël Chen, Patrick Pérez, Matthieu Cord . [Paper] . . Overview of CAB. CAB employs a CVAE backbone which produces distributions over the latent variable and the future trajectory. During training, a blind input is forwarded into the CVAE and the resulting distribution over the latent variable is used to encourage the prediction of the model to be different from the context-agnostic distribution, thanks to the CAB-KL loss. Learning-based trajectory prediction models have encountered great success, with the promise of leveraging contextual information in addition to motion history. Yet, we find that state-of-the-art forecasting methods tend to overly rely on the agent’s current dynamics, failing to exploit the semantic contextual cues provided at its input. To alleviate this issue, we introduce CAB, a motion forecasting model equipped with a training procedure designed to promote the use of semantic contextual information. We also introduce two novel metrics, dispersion and convergence-to-range, to measure the temporal consistency of successive forecasts, which we found missing in standard metrics. Our method is evaluated on the widely adopted nuScenes Prediction benchmark as well as on a subset of the most difficult examples from this benchmark. . . CSG0: Continual Urban Scene Generation with Zero Forgetting . CVPR 2022 Workshop on Continual Learning (CLVision) . Authors: Himalaya Jain, Tuan-Hung Vu, Patrick Pérez, Matthieu Cord . [Paper] &nbsp;&nbsp; [Project page] . With the rapid advances in generative adversarial networks (GANs), the visual quality of synthesized scenes keeps improving, including for complex urban scenes with applications to automated driving. We address in this work a continual scene generation setup in which GANs are trained on a stream of distinct domains; ideally, the learned models should eventually be able to generate new scenes in all seen domains. This setup reflects the real-life scenario where data are continuously acquired in different places at different times. In such a continual setup, we aim for learning with zero forgetting, i.e., with no degradation in synthesis quality over earlier domains due to catastrophic forgetting. To this end, we introduce a novel framework, named CSG0, that not only (i) enables seamless knowledge transfer in continual training but also (ii) guarantees zero forgetting with a small overhead cost. . . Overview of CSG0. Our continual setup for urban-scene generation involves a stream of datasets, with GANs trained from one dataset to another. Our framework makes use of the knowledge learned from previous domains and adapts to new ones with a small overhead. To showcase the merit of our framework, we conduct intensive experiments on various continual urban scene setups, covering both synthetic-to-real and real-to-real scenarios. Quantitative evaluations and qualitative visualizations demonstrate the interest of our CSG0 framework, which operates with minimal overhead cost (in terms of architecture size and training). Benefiting from continual learning, CSG0 outperforms the state-of-the-art OASIS model trained on single domains. We also provide experiments with three datasets to emphasize how well our strategy generalizes despite its cost constraints. Under extreme low-data regimes, our approach outperforms the baseline by a large margin. . . Multi-Head Distillation for Continual Unsupervised Domain Adaptation in Semantic Segmentation . CVPR 2022 Workshop on Continual Learning (CLVision) . Authors: Antoine Saporta, Arthur Douillard, Tuan-Hung Vu, Patrick Pérez, Matthieu Cord . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . This work focuses on a novel framework for learning UDA, continuous UDA, in which models operate on multiple target domains discovered sequentially, without access to previous target domains. We propose MuHDi, for Multi-Head Distillation, a method that solves the catastrophic forgetting problem, inherent in continual learning tasks. MuHDi performs distillation at multiple levels from the previous model as well as an auxiliary target-specialist segmentation head. We report both extensive ablation and experiments on challenging multi-target UDA semantic segmentation benchmarks to validate the proposed learning scheme and architecture. . . Predictions of continual baseline and MuHDi in a Cityscapes scene. The baseline model suffers from catastrophic forgetting when adapting from one domain to another. The proposed MuHDi is more resilient to continual adaptation and preserve predictive accuracy. References . Chen, X., Fan, H., Girshick, R., &amp; He, K. (2020). Improved baselines with momentum contrastive learning. ArXiv Preprint ArXiv:2003.04297. |",
            "url": "https://valeoai.github.io/blog/2022/06/14/valeoai-at-cvpr-2022.html",
            "relUrl": "/2022/06/14/valeoai-at-cvpr-2022.html",
            "date": " • Jun 14, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "valeo.ai at ICCV 2021",
            "content": "The International Conference on Computer Vision (ICCV) is a top event for researchers and engineers working on computer vision and machine learning. The valeo.ai team will present six papers in the main conference, four of which are presented below. Join us to find out more about these projects and ideas, meet our team and learn about our exciting ongoing research. See you at ICCV! . Multi-View Radar Semantic Segmentation . Authors: Arthur Ouaknine, Alasdair Newson, Patrick Pérez, Florence Tupin, Julien Rebut . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . . Example of a scene from the CARRADA dataset (Ouaknine et al., 2021). From left to right: camera image, range-angle view, range-Doppler view, angle, Doppler view. Understanding the scene around the ego-vehicle is key to assisted and autonomous driving. Nowadays, this is mostly conducted using cameras and laser scanners, despite their reduced performance in adverse weather conditions. Automotive radars are low-cost active sensors that measure properties of surrounding objects, including their relative speed, and have the key advantage of not being impacted by rain, snow or fog and could effectively complement the other perception sensors mounted on the car, e.g., cameras, LIDAR. However, they are seldom used for scene understanding due to the size and complexity of radar raw data and the lack of annotated datasets. Fortunately, recent open-sourced datasets have opened up research on classification, object detection and semantic segmentation with raw radar signals using end-to-end trainable models. . . Sequences of raw radar tensors are aggregated and used as input for our multi-view architecture to segment semantically range-angle and range-Doppler views simultaneously. In our paper, Multi-View Radar Semantic Segmentation, we propose a set of deep neural network architectures to segment simultaneously range-angle and range-Doppler radar representations, providing the location and the radial velocity of the detected objects. Our best model takes a sequence of radar views as input, extracts features using individual branches including ASPP blocks, and recovers the range-angle and range-Doppler view dimensions with two decoding branches. We also propose a combination of loss functions composed of a weighted cross entropy, a soft dice and an additional coherence term. We introduce a coherence loss to impose a spatial consistency between the segmented radar views. Our experiments on the CARRADA dataset (Ouaknine et al., 2021) demonstrate that our best model outperforms competing methods with a large margin while requiring significantly fewer parameters. . . Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation . Authors: Victor Besnier, Andrei Bursuc, Alexandre Briot, David Picard . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . . Uncertainty map visualization on the BDD-Anomaly dataset. 1st col.: We highlight the ground truth locations of the OOD objects to help visualize them (red bounding box). 2nd col.: Segmentation map of the SegNet. 3rd to 5th col.: Uncertainty Map highlighted in yellow. Our method produces stronger responses on OOD regions compared to other methods, while being as strong on regular error regions, e.g., boundaries. For real-world decision systems such as autonomous vehicles, accuracy is not the only performance requirement and it often comes second to reliability, robustness, and safety concerns, as any failure carries serious consequences. Component modules of such systems frequently rely on powerful Deep Neural Networks (DNNs), that however do not always generalize to objects unseen in the training data. Simple uncertainty estimation techniques, e.g., entropy of softmax predictions, are less effective since modern DNNs are consistently overconfident on both in-domain and out-of-distribution (OOD) data samples. This hinders further the performance of downstream components relying on their predictions. Dealing successfully with the “unknown unknown”, e.g., by launching an alert or failing gracefully, is crucial. . . By making our target model to fail we can learn its behavior when failing and more reliably detect it at test time. In this work we take inspiration from practices in industrial validation, where the performance of a target model is tested in various extreme cases. Instead of simply verifying the performance of the model we learn how this model behaves in face of failures. To this end we propose a new OOD detection architecture called ObsNet and an associated training scheme based on Local Adversarial Attacks (LAA). Finding failure modes in a trained DNN is quite challenging as such models typically achieve high accuracy, i.e., are rarely wrong, and corner-case samples are rather inserted in the training set than used for validation. LAA triggers failure modes in the target model that are a good proxy for failures in face of unknown OOD data. ObsNet achieves reliable detection of failure and OOD objects without compromising on predictive accuracy and computational time. . . Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation . Authors: Antoine Saporta, Tuan-Hung Vu, Matthieu Cord, Patrick Pérez . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . . Single-target unsupervised domain adaptation fails to produce models that perform on multiple target domains. The aim of multi-target unsupervised domain adaptation is to train a model that excels on these multiple target domains. Autonomous vehicles rely on perception models that require a tremendous amount of annotated data to be trained in a supervised fashion. To reduce the reliance on manual annotation which can get extremely expensive when we consider semantic segmentation of urban scenes for instance, domain adaptation is a popular topic that leverages annotated data from a source domain to train a model on a target domain. More specifically, the unsupervised domain adaptation (UDA) setting only relies on unlabeled data from the target domain and aims at bridging the gap between target and source domains. Most UDA approaches tackle the alignment between a single source domain and a single target domain but don’t generalize well to more domains. Yet, real-world perception systems need to be confronted to a variety of scenarios, such as multiple cities or multiple weather conditions, motivating to extend UDA to multi-target settings. . In our work, Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation, we introduce two UDA frameworks to tackle multi-target adaptation: (i) multi-discriminator, which extends single target UDA approaches to multiple target domains by explicitly aligning each target domain to its counterparts; (ii) multi-target knowledge transfer, which learns a target-agnostic model thanks to a multiple teachers/single student distillation mechanism. We also propose multiple new challenging evaluation benchmarks for multi-target UDA in semantic segmentation based on existing urban scenes datasets. . . PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds . Authors: Anh-Quan Cao, Gilles Puy, Alexandre Boulch, Renaud Marlet . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . . Point cloud registration has many applications in various domains such as autonomous driving, motion and pose estimation, 3D reconstruction, simultaneous localisation and mapping (SLAM), and augmented reality. The most famous method to solve this task is ICP, but is mostly suited for small transformations. Several improvements have been made and the most recent techniques leverage deep learning. . The typical pipeline for point cloud registration is (a) point matching followed by (b) point-pairs filtering to remove incorrect matches in, e.g., non-overlapping regions. One natural way to improve this pipeline is to use deep learning in step (a) to obtain point features of high quality and get pairs of matching points with a nearest neighbors search in this learned feature space. Then, one can typically rely on a classical RANSAC-based method in step (b). Another category of methods exploits deep learning in step (a) and step (b), as proposed by, e.g., DCP, PRNet, DGR. PCAM belongs to this second category where a first network outputs pairs of matching points and a second network filters incorrect pairs. . We construct PCAM by observing that one needs two types of information to correctly match points between two point clouds. First, one needs local fine geometric information to precisely select the best corresponding point. Second, one also needs high-level contextual information to differentiate between points with similar local geometry but from different parts of the scene. Therefore, we compute point correspondences at every layer of our deep network via cross-attention matrices, and combine these matrices via a pointwise multiplication. This simple yet very effective solution naturally ensures that both low-level geometric and high-level context information are exploited when matching points. It also permits to remove spurious matches found only at one scale. Furthermore, these cross-attention matrices are also exploited to exchange information between the point clouds at each layer, allowing the network to use context information to find the best matching point within the overlapping regions. . References . Ouaknine, A., Newson, A., Rebut, J., Tupin, F., &amp; Pérez, P. (2021). CARRADA Dataset: Camera and Automotive Radar with Range- Angle- Doppler Annotations. 2020 25th International Conference on Pattern Recognition (ICPR), 5068–5075. |",
            "url": "https://valeoai.github.io/blog/2021/10/08/valeoai-at-iccv-2021.html",
            "relUrl": "/2021/10/08/valeoai-at-iccv-2021.html",
            "date": " • Oct 8, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "How can we make driving systems explainable?",
            "content": "This post is an introduction to our survey on the explainability of vision-based driving systems, which can be found on arXiv here. . Research on autonomous vehicles is blooming thanks to recent advances in deep learning and computer vision, as well as the development of autonomous driving datasets and simulators. The number of academic publications on this subject is rising in most machine learning, computer vision, robotics and transportation conferences, and journals. On the industry side, several suppliers are already producing cars equipped with advanced computer vision technologies for automatic lane following, assisted parking, or collision detection among other things. Meanwhile, constructors are working on and designing prototypes with level 4 and 5 autonomy. . In the 2010s, we observe an interest in approaches aiming to train driving systems, usually in the form of neural networks, either by leveraging large quantities of expert recordings or through simulation. In both cases, these systems learn a highly complex transformation that operates over input sensor data and produces end-commands (steering angle, throttle). While these neural driving models overcome some of the limitations of the traditional modular pipeline stack, they are sometimes described as black-boxes for their critical lack of transparency and interpretability. Thus, being able to explain the behavior of neural driving models is of paramount importance for their deployment and social acceptance. . Explainability? . Many terms are related to the concept of explainability and several definitions have been proposed for each of these terms. The boundaries between concepts are fuzzy and constantly evolving. In human-machine interactions, explainability is defined as the ability for the human user to understand the agent’s logic (Rosenfeld &amp; Richardson, 2019). The explanation is based on how the human user understands the connections between inputs and outputs of the model. According to (Doshi-Velez &amp; Kortz, 2017), an explanation is a human-interpretable description of the process by which a decision-maker took a particular set of inputs and reached a particular conclusion. They state that in practice, an explanation should answer at least one of the three following questions: . What were the main factors in the decision? | Would changing a certain factor have changed the decision? | Why did two similar-looking cases get different decisions, or vice versa? | . The term explainability often co-occurs with the concept of interpretability. Some recent work of (Beaudouin et al., 2020) simply advocate that explainability and interpretability are synonyms. However, (Gilpin et al., 2018) provide a nuance between these terms that we find interesting. According to them, interpretability designates to which extent an explanation is understandable by a human. They state that an explanation should be designed and assessed in a trade-off between its interpretability and its completeness, which measures how accurate the explanation is as it describes the inner workings of the system. The whole challenge in explaining neural networks is to provide explanations that are both interpretable and complete. . Interestingly, depending on who is the explanation geared towards, it is expected to have varying nature, form and should convey different types of information. . End-users and citizens need to trust the autonomous system and to be reassured. They put their life in the hands of the driving system and thus need to gain trust in it. It appears that user trust is heavily impacted by the system transparency (Zhang et al., 2020): providing information that helps the user understand how the system functions foster his or her trust in the system. Interestingly, research on human-computer interactions argues that an explanation should be provided before the vehicle takes an action, in a formulation which is concise and direct. | Designers of self-driving models need to understand their limitations to validate them and improve future versions. The concept of Operational Design Domain (ODD) is often used by carmakers to designate the conditions under which the car is expected to behave safely. Thus, whenever a machine learning model is built to address the task of driving, it is crucial to know and understand its failure modes, and to verify that these situations do not overlap with the ODD. A common practice is to stratify the evaluation into situations, as is done by the European New Car Assessment Program (Euro NCAP) to test and assess assisted driving functionalities in new vehicles. But even if these in-depth performance analyses are helpful to improve the model’s performance, it is not possible to exhaustively list and evaluate every situation the model may possibly encounter. As a fallback solution, explainability can help delving deeper into the inner workings of the model and to understand why it makes these errors and correct the model/training data accordingly. | Legal and regulatory bodies are interested in explanations for liability and accountability purposes, especially when a self-driving system is involved in a car accident. Notably, explanations generated for legal or regulatory institutions are likely to be different from those addressed to the end-user, as all aspects of the decision process could be required to identify the reasons for a malfunction. | . Driving system? . The history of autonomous driving systems started in the late ’80s and early ’90s with the European Eureka project called Prometheus. This has later been followed by driving challenges proposed by the Defense Advanced Research Projects Agency (DARPA). The vast majority of autonomous systems competing in these challenges is characterized by their modularity: several sub-modules are assembled, each completing a very specific task. Broadly speaking, these subtasks deal with sensing the environment, forecasting future events, planning, taking high-level decisions, and controlling the vehicle. . As pipeline architectures split the driving task into easier-to-solve problems, they offer somewhat interpretable processing of sensor data through specialized modules (perception, planning, decision, control). However, these approaches have several drawbacks: . First, they rely on human heuristics and manually-chosen intermediate representations, which are not proven to be optimal for the driving task. | Second, they lack flexibility to account for real-world uncertainties and to generalize to unplanned scenarios. | Finally, they are prone to error propagation between the multiple sub-modules. | . To circumvent these issues, and nurtured by the deep learning revolution, researchers put more and more efforts on machine learning-based driving systems, and in particular on deep neural networks which can leverage large quantities of data. . We can distinguish four key elements involved in the design of a neural driving system: input sensors, input representations, output type, and learning paradigm . . Figure 1. Overview of neural network-based autonomous driving systems. Sensors. They are the hardware interface through which the neural network perceives its environment. They include cameras, radars, LiDARs, GPS, but also sensors about internal vehicle state such as speed or yaw. For a thorough review of driving sensors, we refer the reader to (Yurtsever et al., 2020). | Input representation. Once sensory inputs are acquired by the system, they are processed by computer vision models to build a structured representation, before being passed to the neural driving system. In the mediated perception approach, several perception systems provide their understanding of the world, and their outputs are aggregated to build an input for the driving model. An example of such vision tasks is object detection and semantic segmentation, tracking objects across time, extracting depth information (i.e. knowing the distance that separates the vehicle from each point in the space), recognizing pedestrian intent… Mediated perception contrasts with the direct perception approach, which instead extracts visual affordances from an image. Affordances are scalar indicators that describe the road situation such as curvature, deviation to neighboring lanes, or distances between ego and other vehicles. | Outputs. Ultimately, the goal is to generate vehicle controls. Some approaches, called end-to-end, tackle this problem by training the deep network to directly output the commands. However, in practice most methods instead predict the future trajectory of the autonomous vehicle; they are called end-to-mid methods. The trajectory is then expected to be followed by a low-level controller, such as the proportional–integral–derivative (PID) controller. | Learning. Two families of methods coexist for training self-driving neural models: behavior cloning approaches, which leverage datasets of human driving sessions, and reinforcement learning approaches, which train models through trial-and-error simulation. Behavior cloning (BC) approaches leverage huge quantities of recorded human driving sessions to learn the input-output driving mapping by imitation. In this setting, the network is trained to mimic the commands applied by the expert driver (end-to-end models), or the future trajectory (end-to-mid models), in a supervised fashion. An initial attempt to behavior cloning of vehicle controls was made by (Pomerleau, 1988), and continued later in (Bojarski et al., 2016). | Reinforcement learning (RL) was alternatively explored by researchers to train neural driving systems. This paradigm learns a policy by balancing self-exploration and reinforcement. This training paradigm relies on a simulator (such as CARLA (Dosovitskiy et al., 2017)). | . | . The challenges of explainability of neural driving systems . Introducing explainability in the design of learning-based self-driving systems is a challenging task. These concerns arise from two aspects: . From a Deep Learning perspective, explainability hurdles of self-driving models are shared with most deep learning models, across many application domains. Indeed, decisions of deep systems are intrinsically hard to explain as the functions these systems represent, mapping from inputs to outputs, are not transparent. In particular, although it may be possible for an expert to broadly understand the structure of the model, the parameter values, which have been learned, are yet to be explained. Several factors cause interpretability issues for self-driving machine learning models. First, a finite training dataset cannot exhaustively cover all possible driving situations. It will likely under- and over-represent some specific cases, and questions such as Has the model encountered situations like X? are legitimate. Moreover, datasets contain numerous biases of various nature (omitted variable bias, cause-effect bias, sampling bias), which also gives rise to explainability issues related to fairness. Second, the mapping function represented by the trained model is poorly understood and is considered as a black-box. The model is highly non-linear and does not provide any robustness guarantee as small input changes may dramatically change the output behavior. Explainability issues thus occur regarding the generalizability and robustness aspects: How will the model behave under these new scenarios? Third, the learning phase is not perfectly understood. Among other things, there are no guarantees that the model will settle at a minimum point that generalizes well to new situations. Thus, the model may learn to ground its decisions on spurious correlations during training instead of on the true causes. We aim at finding answers to questions like Which factors caused this decision to be taken? | . . Figure 2. Explainability hurdles and questions for autonomous driving models, as seen from a machine learning point of view. From a driving perspective, it has been shown that humans tackle this task by solving many intermediate sub-problems, at different levels of hierarchy (Michon, 1984). In the effort towards building an autonomous driving system, researchers aim at providing the machine with these intermediate capabilities. Thus, explaining the general behavior of an autonomous vehicle inevitably requires understanding how each of these intermediate steps is carried and how it interacts with others. We can categorize these capabilities into three types: Perception: information about the system’s understanding of its local environment. This includes the objects that have been recognized and assigned to a semantic label (persons, cars, urban furniture, driveable area, crosswalks, traffic lights), their localization, properties of their motion (velocity, acceleration), intentions of other agents, etc.; | Reasoning: information about how the different components of the perceived environment are organized and assembled by the system. This includes global explanations about the rules that are learned by the model, instance-wise explanation showing which objects are relevant in a given scene, traffic pattern recognition, object occlusion reasoning, etc.; | Decision: information about how the system processes the perceived environment and its associated reasoning to produce a decision. This decision can be a high-level goal stating that the car should turn right, a prediction of the ego vehicle’s trajectory, its low-level relative motion or even the raw controls, etc. | . | . . Figure 3. Explainability hurdles and questions for autonomous driving models, as seen from an autonomous driving point of view. While the separation between perception, reasoning, and decision is clear in modular driving systems, some recent end-to-end neural networks such as PilotNet (Bojarski et al., 2016) blur the lines and perform these simultaneously. Indeed, when an explanation method is developed for a neural driving system, it is often not clear whether it attempts to explain the perception, the reasoning, or the decision step. Considering the nature of neural networks architecture and training, disentangling perception, reasoning, and decision in neural driving systems constitutes a non-trivial challenge. . Conclusion . As an answer to such problems, many explanation methods have been proposed and are usually organized into two categories: applying post-hoc methods on an already-trained driving model, and directly building driving models which are inherently interpretable by design. In our survey, we provide details on existing explainability techniques, show how they tackle to the problem of explaining driving models and highlight their limitations. In addition, we detail remaining challenges and open research avenues to increase explainability of self-driving models. We hope our survey will enable increased awareness in this area from researchers and practitioners in the field, as well as from other potentially related fields. . References . Rosenfeld, A., &amp; Richardson, A. (2019). Explainability in human-agent systems. Auton. Agents Multi Agent Syst. | Doshi-Velez, F., &amp; Kortz, M. A. (2017). Accountability of AI Under the Law: The Role of Explanation. CoRR. | Beaudouin, V., Bloch, I., Bounie, D., Clémençon, S., d’Alché-Buc, F., Eagan, J., Maxwell, W., Mozharovskyi, P., &amp; Parekh, J. (2020). Flexible and Context-Specific AI Explainability: A Multidisciplinary Approach. CoRR. | Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., &amp; Kagal, L. (2018). Explaining Explanations: An Overview of Interpretability of Machine Learning. DSSA. | Zhang, Q., Yang, X. J., &amp; Robert, L. P. (2020). Expectations and Trust in Automated Vehicles. CHI. | Haspiel, J., Du, N., Meyerson, J., Jr., L. P. R., Tilbury, D. M., Yang, X. J., &amp; Pradhan, A. K. (2018). Explanations and Expectations: Trust Building in Automated Vehicles. HRI. | Du, N., Haspiel, J., Zhang, Q., Tilbury, D., Pradhan, A. K., Yang, X. J., &amp; Robert Jr, L. P. (2019). Look who’s talking now: Implications of AV’s explanations on driver’s trust, AV preference, anxiety and mental workload. Transportation Research Part C: Emerging Technologies. | Yurtsever, E., Lambert, J., Carballo, A., &amp; Takeda, K. (2020). A Survey of Autonomous Driving: Common Practices and Emerging Technologies. IEEE Access. | Chen, C., Seff, A., Kornhauser, A. L., &amp; Xiao, J. (2015). DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving. ICCV. | Toromanoff, M., Émilie Wirbel, &amp; Moutarde, F. (2020). End-to-End Model-Free Reinforcement Learning for Urban Driving Using Implicit Affordances. CVPR. | Pomerleau, D. (1988). ALVINN: An Autonomous Land Vehicle in a Neural Network. NIPS. | Bojarski, M., Testa, D. D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., Jackel, L. D., Monfort, M., Muller, U., Zhang, J., Zhang, X., Zhao, J., &amp; Zieba, K. (2016). End to End Learning for Self-Driving Cars. CoRR. | Dosovitskiy, A., Ros, G., Codevilla, F., López, A., &amp; Koltun, V. (2017). CARLA: An Open Urban Driving Simulator. CoRL. | Michon, J. A. (1984). A Critical View of Driver Behavior Models: What Do We Know, what Should We Do? Human behavior and traffic safety. |",
            "url": "https://valeoai.github.io/blog/2021/02/18/explainable-driving.html",
            "relUrl": "/2021/02/18/explainable-driving.html",
            "date": " • Feb 18, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving",
            "content": "This post describes our recent work on probabilistic trajectory prediction for autonomous driving presented at CORL 2020. PLOP is a trajectory prediction method that intent to control an autonomous vehicle (ego vehicle) in urban environment while considering and predicting the intents of other road users (neighbors). We focus here on predicting multiple feasible future trajectories for both ego vehicle and neighbors through a probabilistic framework and rely on a conditional imitation learning algorithm, conditioned by a navigation command for the ego vehicle (e.g., ``turn right’’). Our model processes only onboard sensor data (camera and lidars) along with detections of past and presents objects relaxing the necessity of an HDMap and is computationally efficient as it can run in real time (25 fps) on an embedded board in the real vehicle. We evaluate our method offline on the publicly available dataset nuScenes (Caesar et al., 2020), achieving state-of-the-art performance, investigate the impact of our architecture choices on online simulated experiments and show preliminary insights for real vehicle control. . . Figure 1. Qualitative example of trajectory predictions on a test sample from nuScenes dataset. The top image show a bird&#39;s eye view of PLOP&#39;s predictions for the ego and neighbor vehicles (to be compared with the ground truth in green). The bottom row present the input image (left) in which we added object correspondance with the bird&#39;s eye view and the auxiliary semantic segmentation of this image (right) Predicting the future positions of other agents of the road, or of the autonomous vehicle itself, is critical for autonomous driving. This trajectory prediction must not only respect the rules of the road, but capture the interactions of the agents over time. It is also important to allow multiple possible predictions, as there is usually not a single valid trajectory. . Some approaches such as ChauffeurNet (Bansal et al., 2018) use a high-levelscene representation (road map, traffic lights, speed limit, route, dynamic bounding boxes, etc.). More recently, MultiPath (Chai et al., 2019) uses trajectory anchors, used in one-step object detection, extracted from the training data for ego vehicle prediction. (Hong et al., 2019) use a high level representation which includes some dynamic context. In contrast, we choose to leverage also low level sensor data, here Lidar point clouds and camera image. In that domain, recent approaches address the variation in agent behaviors by predicting multiple trajectories, often in a stochastic way. Many works, e.g., PRECOG (Rhinehart et al., 2019), MFP (Tang &amp; Salakhutdinov, 2019), SocialGAN (Gupta et al., 2018) and others (Rhinehart et al., 2018), focus on this aspect through a probabilistic framework on the network output or latent representations, producing multiple trajectories for ego vehicle, nearby vehicles or both. (Phan-Minh et al., 2020) generate a trajectory set, then classify correct trajectories. (Marchetti et al., 2020) generate multiple futures from encodings of similar trajectories stored in a memory. (Ohn-Bar et al., 2020) learn a weighted mixture of expert policies trained to mimic agents with specific behaviors. In PRECOG, (Rhinehart et al., 2019) advance a probabilistic formulation that explicitly models interactions between agents, using latent variables to model their plausible reactions, with the possibility to precondition the trajectory of the ego vehicle by a goal. . PLOP method . Contributions . Our main goal is to produce a trajectory prediction which can be used to drive the ego vehicle relying on a conditional imitation learning algorithm, conditioned by a navigation command for the ego vehicle (e.g., “turn right”). To do so, we propose a single-shot, anchor-less trajectory prediction method, based on Mixture Desity Networks (MDNs) and polynomial trajectory constraints, relying only on on-board sensors which relaxes the HD map requirement and allow more flexibility for driving in the real world. The polynomial formulation ensures that the predicted trajectories are coherent and smooth, while providing more learning flexibility through the extra parameters. We find that this mitigates training instability and mode collapse that are common to MDNs (Cui et al., 2019). PLOP is trainable end-to-end from imitation learning, where data is relatively easier to obtain and it is computationally efficient during both training and inference as it predicts trajectory coefficients in a single step, without requiring a RNN-based decoder. The polynomial function trajectory coefficients eschew the need for anchors (Chai et al., 2019), whose quality can vary across datasets. . We propose an extensive evaluation of PLOP and show its effectiveness across datasets and settings. We conduct a comparison showing the improvement over state-of-the-art PRECOG (Rhinehart et al., 2019) on the public dataset nuScenes (Caesar et al., 2020); . Then for a better evaluation of the driving capacities of PLOP, we study closed loop performance for the ego vehicle, on simulation and with preliminary insights for real vehicle control. . Network architecture . PLOP takes as inputs: the ego and neighbor vehicles past positions represented as time sequences of x and y over the last 2 seconds, the frontal camera image of the ego vehicle, and 2 second history of bird’s eye views with a cell resolution of 1m square containing the lidar point cloud and the object detections information represented in Figure 2. The objects detections being the output of a state of the art perception algorithm. . . Figure 2. Image and Bird&#39;s eye view. The left image is an example of a front camera input image of PLOP and the diagram on the right is a representation of the bird&#39;eye view input. We pass these inputs through a multibranch neural network represented in Figure 3 to predict the ego vehicle future trajectory and two auxiliary tasks that are the future trajectory prediction for the neighbors vehicles and the semantic segmentation of the camera image. . . Figure 3. PLOP&#39;s Architecture. PLOP&#39;s architecture is reprented on the left while the polynomial multimodal gaussian trajectory representation is on the right The front camera image features, the bird’s eye view features and the ego vehicle past positions features are passed down to conditional fully connected architecture to output multiple future trajectories for the ego vehicle regarding the current navigation order. The trajectories are predicted using MDNs where gaussian means are generated using polynomial functions of degree 4 over x and y . To improve the learning stability of our training and inject awareness about the scene layouts into the camera features we pass them through a U-Net decoder to output semantic segmentation and then use an auxiliary cross entropy loss. To improve the encoding of interactions between the differents agents of the scene in the bird’s eye features, we predict the future possible trajectories for each neighbor feeding the bird’s eye views encoding and its past positions encoded through a LSTM layer to a small fully connected network. The weights of LSTMs and fully connected layers are shared between all neighbors. This output allows us to get useful information about the ego vehicle environment that can be used online to improve the ego vehicle driving with safety collision checks for example. . Offline evaluation . To evaluate PLOP, we use the nuScenes dataset to train the trajectory loss along with the Audi (Geyer et al., 2019) dataset to train the semantic segmentation loss. We choose to compare our method with the DESIRE (Lee et al., 2017) baseline and against two state of the art methods that are PRECOG and ESP (Rhinehart et al., 2019) using the minimum Mean Squared Deviation metric to avoid penalizing valid trajectories that are not matching the ground truth. For one agent, meaning ego vehicle only, PRECOG and ESP have access to the future desired target position and PRECOG return significantly better results than PLOP but PLOP still reaches similar results as ESP. For multiple agents PLOP outperforms other presented methods . We note that the comparison if fairer for neighbor trajectories and the performance is relevant since they are by definition open loop. . . Figure 4. Comparison with state-of-the-art: Against the DESIRE, ESP and PRECOG for predicting a trajectory of 4 seconds into the future But we argue that such evaluation is not totally relevant for controling the ego vehicle in real conditions. Such metrics does not value the situations in which the errors are made, failing to brake at a traffic light is a critical error for example but it is quick and represent a very small part of the test set so it will impact very poorly the overall metrics. However, making a small constant error such as driving 2kph too slow over the whole test set set might be an acceptable and non impacting error but will lead to a considerable overall error. Also, using only offline metrics where the method can’t control the vehicle does not allow us to evaluate its capacities to react to its own mistakes. . Online Evaluation through simulation . To simulate driving, we developped a data driven simulator that allows us to use real driving data to simulate applying the prediction to the ego vehicle. We can generate the input data that corresponds to the new vehicle position after following the trajectory using reprojections (for the image and the pointcloud), then use it to predict a new trajectory, and so on. This allows us the measure the performance in closed loop, and in particular to count failures which would have resulted in a takeover. We rely on 3 metrics: lateral (&gt;1m from expert), high speed (catching up to a vehicle 15% faster than the real vehicle up to 0.6s in the future) and low speed (&gt; 20kph under the expert speed) errors count. . . Figure 5. Evaluation using the simulator. Comparison with PLOP without semantic segmentation loss, Constant velocity baseline and Multi-Layer Perceptron baseline in the table on the left. Additionnal qualitative results about the errors positioning on the differents test tracks are on the right. We trained PLOP on an internal dataset combining both open road and urban test track and compared PLOP, PLOP without auxiliary semantic loss, the constant velocity baseline and a MLP baseline in our simulator using test data. We note that semantic segmentation improve the driving performance and that MLP has better offline metrics than constant velocity approach but still perform worse due to the simulated driving conditions. As expected, offline metrics are not discriminating enough for the online behavior since the best model checkpoints in simulation are not necessarily the ones with the better offline metrics. An additionnal ablation study where we remove mandatory information (such as the camera image input) shows that it may even be dangerous to trust them blindly. . Conclusion . In this work, we demonstrate the interest of our multi-input multimodal approach PLOP for vehicle trajectory prediction in an urban environment. Our architecture leverages frontal camera and Lidar inputs, to produce multiple trajectories using reparameterized Mixture Density Networks, with an auxiliary semantic segmentation task. We show that we can improve open loop state-of-the-art performance in a multi-agent system, by evaluating the vehicle trajectories from the nuScenes dataset. We also provide a simulated closed loop evaluation, to go towards real vehicle online application. Please check out our paper along with supplementary materials for greater details about our approach and experiments and feel free to contact us for any question. . References . Caesar, H., Bankiti, V., Lang, A. H., Vora, S., Liong, V. E., Xu, Q., Krishnan, A., Pan, Y., Baldan, G., &amp; Beijbom, O. (2020). nuScenes: A Multimodal Dataset for Autonomous Driving. Cvpr. | Bansal, M., Krizhevsky, A., &amp; Ogale, A. S. (2018). ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst. CoRR. | Chai, Y., Sapp, B., Bansal, M., &amp; Anguelov, D. (2019). MultiPath: Multiple Probabilistic Anchor Trajectory Hypotheses for Behavior Prediction. | Hong, J., Sapp, B., &amp; Philbin, J. (2019). Rules of the Road: Predicting Driving Behavior with a Convolutional Model of Semantic Interactions. CoRR. | Rhinehart, N., McAllister, R., Kitani, K., &amp; Levine, S. (2019). Precog: Prediction conditioned on goals in visual multi-agent settings. Iccv. | Tang, C., &amp; Salakhutdinov, R. R. (2019). Multiple futures prediction. Advances in Neural Information Processing Systems, 15424–15434. | Gupta, A., Johnson, J., Fei-Fei, L., Savarese, S., &amp; Alahi, A. (2018). Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks. CoRR. | Rhinehart, N., McAllister, R., &amp; Levine, S. (2018). Deep Imitative Models for Flexible Inference, Planning, and Control. CoRR. | Phan-Minh, T., Grigore, E. C., Boulton, F. A., Beijbom, O., &amp; Wolff, E. M. (2020). Covernet: Multimodal behavior prediction using trajectory sets. Cvpr. | Marchetti, F., Becattini, F., Seidenari, L., &amp; Bimbo, A. D. (2020). Mantra: Memory augmented networks for multiple trajectory prediction. Cvpr. | Ohn-Bar, E., Prakash, A., Behl, A., Chitta, K., &amp; Geiger, A. (2020). Learning Situational Driving. Cvpr. | Cui, H., Radosavljevic, V., Chou, F.-C., Lin, T.-H., Nguyen, T., Huang, T.-K., Schneider, J., &amp; Djuric, N. (2019). Multimodal trajectory predictions for autonomous driving using deep convolutional networks. Icra. | Geyer, J., Kassahun, Y., Mahmudi, M., Ricou, X., Durgesh, R., Chung, A. S., Hauswald, L., Pham, V. H., Mühlegg, M., Dorn, S., Fernandez, T., Jänicke, M., Mirashi, S., Savani, C., Sturm, M., Vorobiov, O., &amp; Schuberth, P. (2019). A2D2: AEV Autonomous Driving Dataset. http://www.a2d2.audi | Lee, N., Choi, W., Vernaza, P., Choy, C., Torr, P., &amp; Chandraker, M. (2017). DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents. 2165–2174. https://doi.org/10.1109/CVPR.2017.233 |",
            "url": "https://valeoai.github.io/blog/2020/11/26/plop-trajectory-prediction.html",
            "relUrl": "/2020/11/26/plop-trajectory-prediction.html",
            "date": " • Nov 26, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Is Deep Reinforcement Learning Really Superhuman on Atari?",
            "content": "This post describes our recent work on Deep Reinforcement Learning (DRL) on the Atari benchmark. DRL appears today as one of the closest paradigm to Artificial General Intelligence and large progress in this field has been enabled by the popular Atari benchmark. However, training and evaluation protocols on Atari vary across papers leading to biased comparisons, difficulty in reproducing results and in estimating the true contributions of the methods. Here we attempt to mitigate this problem with SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms. SABER allows us to compare multiple methods under the same conditions against a human baseline and to note that previous claims of superhuman performance on DRL do not hold. Finally, we propose a new state-of-the-art algorithm R-IQN combining Rainbow with Implicit Quantile Networks (IQN). Our code is available. . Deep Reinforcement Learning is a learning scheme based on trial-and-error in which an agent learns an optimal policy from its own experiments and a reward signal. The goal of the agent is to maximize the sum of future accumulated rewards and thus the agent needs to think about sequences of actions rather than instantaneous ones. The Atari benchmark is valuable for evaluating general AI algorithms as it includes more than 50 games displaying high variability in the task to solve, ranging from simple paddle control in the ball game Pong to complex labyrinth exploration in Montezuma’s Revenge which remains unsolved by general algorithms up to today. . We notice however that training and evaluation procedures on Atari can be different from paper to paper and thus leading to bias in comparison. Moreover this leads to difficulties to reproduce results of published works as some training or evaluation parameters are barely explained or sometimes not mentioned. In order to facilitate reproducible and comparable DRL, we introduce SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms. Furthermore, we introduce a human world record baseline and argue that previous claims of superhuman performance of DRL might not hold. Finally, we propose a new state-of-the-art algorithm R-IQN by combining the current state-of-the-art Rainbow (Hessel et al., 2018) along with Implicit Quantile Networks (IQN (Dabney et al., 2018)). We release an open-source implementation of distributed R-IQN following the ideas from Ape-X! . DQN’s human baseline vs human world record on Atari Games . A common way to evaluate AI for games is to let agents compete against the best humans. Recent examples for DRL include the victory of AlphaGo versus Lee Sedol for Go, OpenAI Five on Dota 2 or AlphaStar versus Mana for StarCraft 2. For this reason one of the most used metrics for evaluating RL agents on Atari is to compare them to the human baseline introduced in DQN. . Previous works use the normalized human score, i.e., 0% is the score of a random player and 100% is the score of the human baseline, which allows one to summarize the performance on the whole Atari set in one number, instead of individually comparing raw scores for each of the 61 games. However we argue that this human baseline is far from being representative of the best human player, which means that using it to claim superhuman performance is misleading. . The current world records are available online for 58 of the 61 evaluated Atari games. For example, on VideoPinball, the world record is 50,000 times higher than the human baseline of DQN. Evaluating these world records scores using the usual human normalized score has a median of 4,400% and a mean of 99,300% (see Figure below for details on each game), to be compared to 200% and 800% of the current state-of-the-art Rainbow! . . Figure 1: World record scores vs. the usual beginner human baseline (log scale) (Dabney et al., 2018) baseline can be up to 50k times lower than registered world records. Beating that baseline does not necessarily make the agent superhuman. We estimate that evaluating the algorithms under the world record baseline instead of the DQN human baseline will give a better view of the gap remaining between best human players and DRL agents. Results are confirming this, as Rainbow reaches only a median human-normalized score of 3% (see Figure 2 below) meaning that for half of Atari games, the agent doesn’t even reach 3% of the way from random to best human run. . In the video below we analyze agents previously claimed as above human-level but far from the world record. By taking a closer look at the AI playing, we discovered that on most of Atari games DRL agents fail to understand the goal of the game. Sometimes they just don’t explore other levels than the initial one, sometimes they are stuck in a loop giving small amount of reward, etc. A compelling example of this is the game Riverraid. In this game, the agent must shoot everything and take fuel to survive: the agent dies if there is a collision with an enemy or if out of fuel. But as shooting fuel actually gives points, the agent doesn’t understand that he could play way longer and win even more points by actually taking this fuel bonus and not shooting them! . . SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms . In our work we extend the recommendations proposed by (Machado et al., 2018) They also point out divergences in training and evaluating agents on Atari. Consequently they compile and propose a set of recommendations for more reproducible and comparable RL including sticky actions, ignoring life signal and using full action set. . There is however one major parameter that is left out in (Machado et al., 2018): the maximum number of frames allowed per episode. This parameter ends the episode after a fixed number of time steps even if the game is not over. In most of the recent works, this is set to 30 min of game play (i.e., 108k frames) and only to 5 min in some others (i.e.,18k frames). This means that the reported scores can not be compared fairly. For example, in easy games (e.g., Atlantis), the agent never dies and the score is more or less linear with the allowed time: the reported score will be 6 times higher if capped at 30 minutes instead of 5 minutes. . Another issue with this time cap comes from the fact that some games are designed for much longer gameplay than 5 or 30 minutes. On those games (e.g., Atlantis, Video Pinball, Enduro) the scores reported of Ape-X, Rainbow and IQN are almost the same. This is due to all agents reaching the time limit and getting the maximum possible score in 30 minutes: the difference in scores is due to minor variations, not algorithmic difference and thus the comparison is not significant. As a consequence, the more successful agents are, the more games are incomparable because they reach the maximum possible score in the time cap while still being far behind human world record. . This parameter can also be a source of ambiguity and error. The best score on Atlantis (2,311,815) is reported by Proximal Policy Optimization by (Schulman et al., 2017), however this score is likely wrong: it seems impossible to reach in 30 minutes only! The first distributional paper by (Bellemare et al.) also did this mistake and reported wrong results before adding an erratum in a later version on ArXiv (Bellemare et al., 2017). . . Table 1: Game parameters of SABER. We first re-implemented the current state-of-the-art Rainbow and evaluated it on SABER. We noticed that the same algorithm under different evaluation settings can lead to significantly different results. This showed again the necessity of a common and standardized benchmark, more details can be found in the paper. . Then we implemented a new algorithm, R-IQN, by replacing the C51 algorithm (which is one of the 6 components of Rainbow) by Implicit Quantile Network (IQN). Both C51 and IQN belong to the field of Distributional RL which aims to predict the full distribution of the Q-value function instead of just predicting the mean of it. The fundamental difference between these two algorithms is how they parametrize the Q-value distribution. C51, which is the first algorithm of Distributional RL, approximates the Q-value as a categorical distribution with fixed support and just learns the mass to attribute to each category. On the other hand, IQN approximates the Q-value with quantile regression and both the support and the mass arelearned resulting in a major improvement in performance and data-efficiency over C51. As IQN arises much better performance than C51 while still designed for the same goal (predict the full distribution of the Q-function), combining Rainbow with IQN is relevant and natural. . As shown in the graph below, R-IQN outperforms Rainbow and thus becomes the new state-of-the-art on Atari. However, we acknowledge that in order to make a more confident state-of-the-art claim we should run multiple times with different seeds. Testing an increased number of random seeds across the 60 Atari games is a computationally costly endeavor and is beyond the scope of this study. We test the stability of the performances of R-IQN across 5 random seeds on a subset of 14 games. We compare against Rainbow and report similar results in Figure 3. . . Figure 2: Comparison of Rainbow and Rainbow-IQN on SABER. We report median normalized scores w.r.t training steps. . Figure 3: Comparison of Rainbow and Rainbow-IQN on a subset of 14 games using 5 seeds. We report median normalized scores w.r.t training steps. Open-source implementation of distributed Rainbow-IQN: R-IQN Ape-X . We release our code of a distributed version of Rainbow-IQN following ideas from Ape-X (Horgan et al., 2018). The distributed part is our main practical advantage over some existing DRL repositories (particularly Dopamine a popular open-source implementation of DQN, C51, IQN and a small-Rainbow but in which all algorithms are single worker). Indeed, when using DRL algorithms for other tasks (other than Atari and MuJoCO) a major bottleneck is the speed of the environment. DRL algorithms often need a huge amount of data before reaching reasonable performance. This amount may be practically impossible to reach if the environment is real-time and if collecting data from multiple environments at the same time is not possible. . Building upon ideas from Ape-X, we use REDIS as a side server to store our replay memory. Multiple actors will act in their own instances of the environment to fill as fast as they can the replay memory. Finally, the learner will sample from this replay memory (the learner is actually completely independent of the environment) for backprop. The learner will also periodically update the weight of each actor as shown in the schema below. . . Figure 4: Ape-X architecture. image taken from (Horgan et al., 2018) This scheme allowed us to use R-IQN Ape-X for the task of autonomous driving using CARLA as environment. This enabled us to win the CARLA Challenge on Track 2 Cameras Only showing the strength of R-IQN Ape-X as a general algorithm. . Conclusion . In this work, we confirm the impact of standardized guidelines for DRL evaluation, and build a consolidated benchmark, SABER. In order to provide a more significant comparison, we build a new baseline based on human world records and show that the state-of-the-art Rainbow agent is in fact far from human world record performance. In the paper we share possible reasons for this failure. We hope that SABER will facilitate better comparisons and enable new exciting methods to prove their effectiveness without ambiguity. . Check out our paper to find out more about intuitions, experiments and interpretations. . References . Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., &amp; Silver, D. (2018). Rainbow: Combining improvements in deep reinforcement learning. AAAI. | Dabney, W., Ostrovski, G., Silver, D., &amp; Munos, R. (2018). Implicit quantile networks for distributional reinforcement learning. ICML. | Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., &amp; Bowling, M. (2018). Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61, 523–562. | Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017). Proximal policy optimization algorithms. ArXiv Preprint ArXiv:1707.06347. | Bellemare, M. G., Dabney, W., &amp; Munos, R. (2017). A distributional perspective on reinforcement learning. ArXiv Preprint ArXiv:1707.06887. | Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., Van Hasselt, H., &amp; Silver, D. (2018). Distributed prioritized experience replay. ArXiv Preprint ArXiv:1803.00933. |",
            "url": "https://valeoai.github.io/blog/2020/10/19/saber.html",
            "relUrl": "/2020/10/19/saber.html",
            "date": " • Oct 19, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "ADVENT - Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation",
            "content": "This post describes our recent work on unsupervised domain adaptation for semantic segmentation presented at CVPR 2019. ADVENT is a flexible technique for bridging the gap between two different domains through entropy minimization. Our work builds upon a simple observation: models trained only on source domain tend to produce over-confident, i.e., low-entropy, predictions on source-like images and under-confident, i.e., high-entropy, predictions on target-like ones. Consequently by minimizing the entropy on the target domain, we make the feature distributions from the two domains more similar. We show that our approach achieves competitive performances on standard semantic segmentation benchmarks and that it can be successfully extended to other tasks such as object detection. . Visual perception is a remarkable ability that human drivers leverage for understanding their surroundings and for supporting the multiple micro-decisions needed in traffic. Since many years, researchers have been working on mimicking this human capability by means of computer algorithms. This research field is known as computer vision and it has seen impressive progress and wide adoption. Most of the modern computer vision systems rely on Deep Neural Networks (DNNs) which are powerful and widely employed tools able to learn from large amounts of data and make accurate predictions. In autonomous driving, DNN-based visual perception is also at the heart of the complex architectures under intelligent cars, and supports downstream decisions of the vehicle, e.g., steering, braking, signaling, etc. . The diversity and complexity of the situations encountered in real-world driving is tremendous. Unlike humans who can extrapolate effortlessly from previous experience in order to adapt to new environments and conditions, the scope of DNNs beyond the types of conditions and scenes seen during training is limited. For instance a model trained on data from a sunny country, would have a hard time delivering the same performance on streets with mixed weather conditions in a different country (with different urban architecture, furniture, vegetation, types of cars and pedestrian appearance and clothing). Similarly a model trained on a particular type of camera, is expected to see a drop in performance with images coming from a camera with different specifications. This difference between environments that leads to performance drops is referred to as domain gap. . Bridging domains . We can resort to two options for narrowing the domain gap: (i) annotate more data; (ii) leverage the experience acquired on an initial environment and transfer it to the new environment. More annotated data has been shown to always improve performance of DNNs (Sun et al., 2017). However the labeling process brings a significant financial and temporal burden. The time required for a high-quality annotation, such as the ones from the popular Cityscapes dataset is ∼90 minutes per image (Cordts et al., 2016). The amount of images required to train high performance DNNs typically counts in hundreds of thousands. The acquisition of diverse data across seasons and weather conditions adds up even more time. It makes then sense to look for a solution elsewhere and the second option seems now more appealing, though achieving it remains technically challenging. This is actually the area of research of domain adaptation (DA) which addresses the domain-gap problem by transferring knowledge from a source domain (with full annotations) to a target domain (with fewer annotations if any), aiming to reach good performances on target samples. DA has consistently attracted interest from different communities across years (Csurka, 2017). . Here we are working on Unsupervised DA (UDA), which is a more challenging task where we have access to labeled source samples and only unlabeled target samples. We use as source, data generated by a simulator or video game engine, while for target we consider real-data from car-mounted cameras. In Figure 1 we illustrate the difficulty of this task and the impact of our UDA technique, ADVENT. . . Figure 1. Proposed entropy-based unsupervised domain adaptation for semantic segmentation. The top two rows show results on source and target domain scenes of the model trained without adaptation. The bottom row shows the result on the same target domain scene of the model trained with entropy-based adaptation. The left and right columns visualize respectively the semantic segmentation outputs and the corresponding prediction entropy maps. The main approaches for UDA include discrepancy minimization between source and target feature distributions usually achieved via adversarial training (Ganin &amp; Lempitsky, 2015), (Tzeng et al., 2017), self-training with pseudo-labels (Zou et al., 2018) and generative approaches (Hoffman et al., 2018), (Wu et al., 2018). . Entropy minimization has been shown to be useful for semi-supervised learning (Grandvalet &amp; Bengio, 2005), clustering (Jain et al., 2018) and more recently to domain adaptation for classification (Long et al., 2016). We chose to explore entropy based UDA training to obtain competitive performance on semantic segmentation. . Approach . We present our two proposed approaches for entropy minimization using (i) an unsupervised entropy loss and (ii) adversarial training. To build our models, we start from existing semantic segmentation frameworks and add an additional network branch used for domain adaptation. Figure 2 illustrates our architectures. . . Figure 2. Approach overview. First, direct entropy minimization decreases the entropy of the target $P_{x_t}$, which is equivalent to minimizing the sum of weighted self-information maps $I_{x_t}$. In the second approach, we use adversarial training to enforce the consistency in $P_x$ across domains. Red arrows are used for target domain, blue arrows for source. Direct entropy minimization . On the source domain we train our model, denoted as $F$, as usual using a supervised loss. For the target domain, we do not have annotations and we can no longer use the segmentation loss to train $F$. We notice that models trained only on source domain tend to produce over-confident predictions on source-like images and under-confident predictions on target-like ones. Motivated by this observation, we propose a supervision signal that could leverage visual information from the target samples, in spite of the lack of annotations. The objective is to constrain $F$ to produce high-confident predictions on target samples similarly to source samples. To this effect, we introduce the entropy loss $ mathcal{L}_{ent}$​ to maximize directly the prediction confidence in the target domain. Here we consider the Shannon Entropy (Shannon). During training, we jointly optimize the supervised segmentation loss $ mathcal{L}_{seg}$ on source samples and the unsupervised entropy loss $ mathcal{L}_{ent}$​​ on target samples. . Entropy minimization by adverarial learning . A limitation of the entropy loss is related to the absence of structural dependencies between local semantics. This is caused by the aggregation of the pixel-wise prediction entropies by summation. We address this through a unified adversarial training framework which minimizes indirectly the entropy of target data, by encouraging it to become similar to the source one. This allows the exploitation of the structural consistency between the domains. To this end, we formulate the UDA task as minimizing distribution distance between source and target on the weighted self-information space. Since the trained model produces naturally low-entropy predictions on source-like images, by aligning weighted self-information distributions of target and source domains, we reach the same behavior on target-like data. . We perform the adversarial adaptation on weighted self-information maps using a fully-convolutional discriminator network $D$. The discriminator produces domain classification outputs, i.e., class label $1$ (resp. $0$) for the source (resp. target) domain. We train $D$ to discriminate outputs coming from source and target images, and at the same time, train the segmentation network to fool the discriminator. . Experiments . We evaluate our approaches on the challenging synthetic-2-real unsupervised domain adaptation set-ups. Models are trained on fully-annotated synthetic data and are validated on real-world data. In such set-ups, the models have access to some unlabeled real images during training. . Semantic Segmentation . To train our models, we use either GTA5 (Richter et al., 2016) or SYNTHIA (Ros et al., 2016) as source synthetic data, along with the training split of Cityscapes dataset (Cordts et al., 2016) as target domain data. . In Table 1 we report our results on semantic segmentation from models trained on GTA5 $ rightarrow$ Cityscapes and from SYNTHIA $ rightarrow$ Cityscapes. We compare here only with the top performing method Adapt-SegMap (Tsai et al., 2018), while additional baselines and related methods are covered in the paper. . . Table 1. Segmentation performance in mIoU with ResNet-101 based model and Deeplab-V2 as the segmentation network.We show results of our approaches using the direct entropy loss (MinEnt) and using adversarial training (AdvEnt). Our first approach of direct entropy minimization (MinEnt) achieves comparable performance to state-of-the-art baselines. The light overhead of the entropy loss makes training time shorter for the MinEnt model, while being easier train compared to adversarial networks. Our second approach using adversarial training on the weighted self-information space, noted as AdvEnt, shows consistent improvement to the baselines. In general, AdvEnt works better than MinEnt, confirming the importance of structural adaptation. The two approaches are complementary as their combination boosts performance further. . In Figure 3, we illustrate a few qualitative results of our models. Without domain adaptation, the model trained only on source supervision produces noisy segmentation predictions as well as high entropy activations, with a few exceptions on some classes like “building” and “car”. However, there are many confident predictions (low entropy) which are completely wrong. Our models, on the other hand, manage to produce correct predictions at high level of confidence. . . Figure 3. Segmentation and detection qualitative results. Segmentation on Cityscapes validation set with ResNet-101 + DeepLab-V2; Detection on Cityscapes-foggy with VGG-16 as the backbone and SSD. UDA for object detection . The proposed entropy-based approaches are not limited to semantic segmentation and can be applied to UDA for other recognition tasks, e.g. object detection. We conducted experiments in the UDA object detection set-up Cityscapes $ rightarrow$ Cityscapes-Foggy, similar to the one in (Chen et al., 2018). We report quantitative results in Table 2 and qualitative ones in Figure 3. In spite of the unfavorable factors, our improvement over the baseline ($+11.5 %$ mAP using AdvEnt) is larger than the one reported in (Chen et al., 2018) ($+8.8 %$). Additional experiments and implementation details can be found in the paper. These encouraging preliminary results suggest the feasibility of applying entropy-based approached on UDA for detection. . . Table 2. Object detection performance on Cityscapes Foggy. Conclusion . In this work, we propose two approaches for unsupervised domain adaptation reaching state-of-the-art performances on standard synthetic-2-real benchmarks. Interestingly the method can be easily extended to UDA for object detection with promising preliminary results. Check out our paper to find out more about intuitions, experiments and implementation details for AdvEnt and try out our code. . References . Sun, C., Shrivastava, A., Singh, S., &amp; Gupta, A. (2017). Revisiting unreasonable effectiveness of data in deep learning era. Proceedings of the IEEE International Conference on Computer Vision, 843–852. | Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., &amp; Schiele, B. (2016). The cityscapes dataset for semantic urban scene understanding. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3213–3223. | Csurka, G. (2017). Domain adaptation in computer vision applications. 8. | Ganin, Y., &amp; Lempitsky, V. (2015). Unsupervised domain adaptation by backpropagation. International Conference on Machine Learning, 1180–1189. | Tzeng, E., Hoffman, J., Saenko, K., &amp; Darrell, T. (2017). Adversarial discriminative domain adaptation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 7167–7176. | Zou, Y., Yu, Z., Vijaya Kumar, B. V. K., &amp; Wang, J. (2018). Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. Proceedings of the European Conference on Computer Vision (ECCV), 289–305. | Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y., Isola, P., Saenko, K., Efros, A., &amp; Darrell, T. (2018). Cycada: Cycle-consistent adversarial domain adaptation. International Conference on Machine Learning, 1989–1998. | Wu, Z., Han, X., Lin, Y.-L., Gokhan Uzunbas, M., Goldstein, T., Nam Lim, S., &amp; Davis, L. S. (2018). Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation. Proceedings of the European Conference on Computer Vision (ECCV), 518–534. | Grandvalet, Y., &amp; Bengio, Y. (2005). Semi-supervised learning by entropy minimization. Advances in Neural Information Processing Systems, 529–536. | Jain, H., Zepeda, J., Pérez, P., &amp; Gribonval, R. (2018). Learning a complete image indexing pipeline. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 4933–4941. | Long, M., Zhu, H., Wang, J., &amp; Jordan, M. I. (2016). Unsupervised domain adaptation with residual transfer networks. Advances in Neural Information Processing Systems, 136–144. | Richter, S. R., Vineet, V., Roth, S., &amp; Koltun, V. (2016). Playing for data: Ground truth from computer games. European Conference on Computer Vision, 102–118. | Ros, G., Sellart, L., Materzynska, J., Vazquez, D., &amp; Lopez, A. M. (2016). The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3234–3243. | Tsai, Y.-H., Hung, W.-C., Schulter, S., Sohn, K., Yang, M.-H., &amp; Chandraker, M. (2018). Learning to adapt structured output space for semantic segmentation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 7472–7481. | Chen, Y., Li, W., Sakaridis, C., Dai, D., &amp; Van Gool, L. (2018). Domain adaptive faster r-cnn for object detection in the wild. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3339–3348. |",
            "url": "https://valeoai.github.io/blog/2020/07/07/advent-domain-adaptation.html",
            "relUrl": "/2020/07/07/advent-domain-adaptation.html",
            "date": " • Jul 7, 2020"
        }
        
    
  

  
  
      ,"page0": {
          "title": "Posts",
          "content": "Posts . . . . valeo.ai at NeurIPS 2023 . Victor Letzelter, Antonin Vobecky, Mickael Chen, Cedric Rommel, Matthieu Cord, Andrei Bursuc . Dec 8, 2023 . | valeo.ai at ICCV 2023 . Gilles Puy, Tuan-Hung Vu, Oriane Siméoni, Matthieu Cord, Cédric Rommel, Andrei Bursuc . Sep 26, 2023 . | valeo.ai at CVPR 2023 . Alexandre Boulch, Oriane Siméoni, Gilles Puy, Éloi Zablocki, Spyros Gidaris, Andrei Bursuc . Jun 14, 2023 . | valeo.ai at CVPR 2022 . Corentin Sautier, Alexandre Boulch, Patrick Pérez, Éloi Zablocki, Tuan-Hung Vu, Matthieu Cord, Andrei Bursuc . Jun 14, 2022 . | valeo.ai at ICCV 2021 . Andrei Bursuc, Gilles Puy, Arthur Ouaknine, Antoine Saporta . Oct 8, 2021 . | How can we make driving systems explainable? . Hedi Ben younes*, Eloi Zablocki*, Matthieu Cord and Patrick Pérez . Feb 18, 2021 . | PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving . Thibault Buhet, Emilie Wirbel, Andrei Bursuc and Xavier Perrotton . Nov 26, 2020 . | Is Deep Reinforcement Learning Really Superhuman on Atari? . Marin Toromanoff, Emilie Wirbel . Oct 19, 2020 . | ADVENT - Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation . Tuan-Hung Vu, Andrei Bursuc . Jul 7, 2020 . | .",
          "url": "https://valeoai.github.io/blog/posts/",
          "relUrl": "/posts/",
          "date": ""
      }
      
  

  
      ,"page1": {
          "title": "Projects",
          "content": "Projects . Multi-sensor perception . Automated driving relies first on a diverse range of sensors, like Valeo’s fish-eye cameras, LiDARs, radars and ultrasonics. Exploiting at best the outputs of each of these sensors at any instant is fundamental to understand the complex environment of the vehicle and gain robustness. To this end, we explore various machine learning approaches where sensors are considered either in isolation (as radar in Carrada at ICPR’20) or collectively (as in xMUDA at CVPR’20). . . 3D perception . Each sensor delivers information about the 3D world around the vehicle. Making sense of this information in terms of drivable space and important objects (road users, curb, obstacles, street furnitures) in 3D is required for the driving system to plan and act in the safest and most confortable way. This encompasses several challenging tasks, in particular detection and segmentation of objects in point clouds as in FKAConv at ACCV’20. . . Frugal learning . Collecting diverse enough data, and annotating it precisely, is complex, costly and time-consuming. To reduce dramatically these needs, we explore various alternatives to fully-supervised learning, e.g, training that is unsupervised (as rOSD at ECCCV’20), self-supervised (as BoWNet at CVPR’20), semi-supervised, active, zero-shot (as ZS3 at NeurIPS’19) or few-shot. We also investigate training with fully-synthetic data (in combination with unsupervised domain adaptation) and with GAN-augmented data (as Semantic Palette at CVPR’21). . . Domain adaptation . Deep learning and reinforcement learning are key technologies for autonomous driving. One of the challenges they face is to adapt to conditions which differ from those met during training. To improve systems’ performance in such situations, we explore so-called “domain adaptation” techniques, as in AdvEnt at CVPR’19 and DADA its extension at ICCV’19. We propose new solutions to more practical DA scenarios in MTAF (ICCV&#39;21) to handle multiple target domains and in BUDA (CVIU&#39;21) to handle new target classes. In xMUDA (CVPR&#39;20), we introduce a new framework to tackle the challenging adaptation problem on both 2D image and 3D point-cloud spaces. . . Reliability . When the unexpected happens, when the weather badly degrades, when a sensor gets blocked, the embarked perception system should diagnose the situation and react accordingly, e.g., by calling an alternative system or the human driver. With this in mind, we investigate ways to improve the robustness of neural nets to input variations, including to adversarial attacks, and to predict automatically the performance and the confidence of their predictions as in ConfidNet at NeurIPS’19. . . Driving in action . Getting from sensory inputs to car control goes either through a modular stack (perception &gt; localization &gt; forecast &gt; planning &gt; actuation) or, more radically, through a single end-to-end model. We work on both strategies, more specificaly on action forecasting, automatic interpretation of decisions taken by a driving system, and reinforcement / imitation learning for end-to-end systems (as in RL work at CVPR’20). . . Core Deep Learning . Deep learning being now a key component of AD systems, it is important to get a better understanding of its inner workings, in particular the link between the specifics of the learning optimization and the key properities (performance, regularity, robustness, generalization) of the trained models. Among other things, we investigate the impact of popular batch normalization on standard learning procedures and the ability to learn through unsupervised distillation. . . Interpretability and Explainability of Deep Models . The concept of explainability has several facets and the need for explainability is strong in safety-critical applications such as autonomous driving where deep learning models are now widely used. As the underlying mechanisms of these models remain opaque, explainability and trustworthiness have become major concerns. Among other things, we investigate methods providing explanations to a black-box visual-based systems in a post-hoc fashion, as well as approaches that aim at building more interpretable self-driving systems by design. .",
          "url": "https://valeoai.github.io/blog/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Publications",
          "content": "All publications . For the comprehensive collection of our research papers, please refer to our team’s Google Scholar page. By following our account, you will be updated with notifications about new papers published by our team. . Selected publications . 2024 . BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds . Corentin Sautier, Gilles Puy, Alexandre Boulch, Renaud Marlet, Vincent Lepetit International Conference on 3D Vision (3DV), 2024 . . SALUDA: Surface-based Automotive Lidar Unsupervised Domain Adaptation . Bjoern Michele, Alexandre Boulch, Gilles Puy, Tuan-Hung Vu, Renaud Marlet and Nicolas Courty International Conference on 3D Vision (3DV), 2024 . . CLIP-DIY: CLIP Dense Inference Yields Open-Vocabulary Semantic Segmentation For-Free . Monika Wysoczańska, Michaël Ramamonjisoa, Tomasz Trzciński, Oriane Siméoni Winter Conference on Applications of Computer Vision (WACV), 2024 . 2023 . Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis . Victor Letzelter, Mathieu Fontaine, Mickaël Chen, Patrick Pérez, Slim Essid, and Gaël Richard Advances in Neural Information Processing Systems (NeurIPS), 2023 . . PØDA: Prompt-driven Zero-shot Domain Adaptation . Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette International Conference on Computer Vision (ICCV), 2023 . . DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion . Cédric Rommel, Eduardo Valle, Mickaël Chen, Souhaiel Khalfaoui, Renaud Marlet, Matthieu Cord, Patrick Pérez International Conference on Computer Vision (ICCV) Workshop, 2023 . . Towards Motion Forecasting with Real-World Perception Inputs: Are End-to-End Approaches Competitive? . Yihong Xu, Loïck Chambon, Éloi Zablocki, Mickaël Chen, Alexandre Alahi, Matthieu Cord, Patrick Pérez ICCV Workshop &amp; Challenge on Event Detection for Situation Awareness in Autonomous Driving (ROAD++), 2023 . . You Never Get a Second Chance To Make a Good First Impression: Seeding Active Learning for 3D Semantic Segmentation . Nermin Samet, Oriane Siméoni, Gilles Puy, Georgy Ponimatkin, Renaud Marlet, and Vincent Lepetit International Conference on Computer Vision (ICCV), 2023 . . Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation . Gilles Puy, Alexandre Boulch, and Renaud Marlet International Conference on Computer Vision (ICCV), 2023 . . OCTET: Object-aware Counterfactual Explanations . Mehdi Zemni, Mickaël Chen, Éloi Zablocki, Hédi Ben-Younes, Patrick Pérez, Matthieu Cord Computer Vision and Pattern Recognition (CVPR), 2023 . . ALSO: Automotive Lidar Self-supervision by Occupancy estimation . Alexandre Boulch, Corentin Sautier, Björn Michele, Gilles Puy, Renaud Marlet Computer Vision and Pattern Recognition (CVPR), 2023 . . Unsupervised Object Localization: Observing the Background to Discover Objects . Oriane Siméoni, Chloé Sekkat, Gilles Puy, Antonin Vobecky, Éloi Zablocki, Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2023 . . RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving . Angelika Ando, Spyros Gidaris, Andrei Bursuc, Gilles Puy, Alexandre Boulch, and Renaud Marlet Computer Vision and Pattern Recognition (CVPR), 2023 . . Packed Ensembles for efficient uncertainty estimation . Olivier Laurent, Adrien Lafage, Enzo Tartaglione, Geoffrey Daniel, Jean-Marc Martinez, Andrei Bursuc, and Gianni Franchi International Conference on Learning Representations (ICLR), 2023 . . Self-supervised learning with rotation-invariant kernels . Léon Zheng, Gilles Puy, Elisa Riccietti, Patrick Pérez, Rémi Gribonval International Conference on Learning Representations (ICLR), 2023 . . Cross-task Attention Mechanism for Dense Multi-task Learning . Ivan Lopes , Tuan-Hung Vu and Raoul de Charette IEEE/CVF Winter Conference on Applications of Computer Vision, 2023 . 2022 . LaRa: Latents and Rays for Multi-Camera Bird&#39;s-Eye-View Semantic Segmentation . Florent Bartoccioni, Éloi Zablocki, Andrei Bursuc, Patrick Pérez, Matthieu Cord, Karteek Alahari Conference on Robot Learning, 2022 . . LiDARTouch: Monocular metric depth estimation with a few-beam LiDAR . Florent Bartoccioni, Éloi Zablocki, Patrick Pérez, Matthieu Cord, Karteek Alahari Computer Vision and Image Understanding (CVIU), 2022 . . What to Hide from Your Students: Attention-Guided Masked Image Modeling . Ioannis Kakogeorgiou, Spyros Gidaris, Bill Psomas, Yannis Avrithis, Andrei Bursuc, Konstantinos Karantzalos, and Nikos Komodakis European Conference on Computer Vision (ECCV), 2022 . . STEEX: Steering Counterfactual Explanations with Semantics . Paul Jacob, Éloi Zablocki, Hédi Ben-Younes, Mickaël Chen, Patrick Pérez, Matthieu Cord European Conference on Computer Vision, 2022 . . Latent Discriminant deterministic Uncertainty . Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson, and David Filliat European Conference on Computer Vision (ECCV), 2022 . . Active Learning Strategies for Weakly-Supervised Object Detection . Huy V. Vo, Oriane Siméoni, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, and Jean Ponce European Conference on Computer Vision (ECCV), 2022 . . Explainability of deep vision-based autonomous driving systems: Review and challenges . Éloi Zablocki*, Hédi Ben-Younes*, Patrick Pérez, Matthieu Cord International Journal of Computer Vision, 2022 . . Raising context awareness in motion forecasting . Hédi Ben-Younes*, Éloi Zablocki*, Mickaël Chen, Patrick Pérez, Matthieu Cord Computer Vision and Pattern Recognition (CVPR) Workshop, 2022 . . CSG0: Continual Urban Scene Generation with Zero Forgetting . Himalaya Jain (*), Tuan-Hung Vu (*), Patrick Pérez and Matthieu Cord (* equal contrib.) Computer Vision and Pattern Recognition (CVPR) Workshop, 2022 . . Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data . Corentin Sautier, Gilles Puy, Spyros Gidaris, Alexandre Boulch, Andrei Bursuc, and Renaud Marlet Computer Vision and Pattern Recognition (CVPR), 2022 . . Multi-Head Distillation for Continual Unsupervised Domain Adaptation in Semantic Segmentation . Antoine Saporta, Arthur Douillard, Tuan-Hung Vu, Patrick Pérez and Matthieu Cord Computer Vision and Pattern Recognition (CVPR) Workshop, 2022 . . Diverse Probabilistic Trajectory Forecasting with Admissibility Constraints . Laura Calem, Hedi Ben-Younes, Patrick Pérez, Nicolas Thome International Conference on Pattern Recognition, 2022 . . Raw High-Definition Radar for Multi-Task Learning . Julien Rebut, Arthur Ouaknine, Waqas Malik, and Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2022 . . POCO: Point Convolution for Surface Reconstruction . Alexandre Boulch and Renaud Marlet Computer Vision and Pattern Recognition (CVPR), 2022 . . Spherical perspective on learning with normalization layers . Simon Roburin, Yann de Mont-Marin, Andrei Bursuc, Renaud Marlet, Patrick Pérez, Mathieu Aubry Neurocomputing, 2022 . . Cross-modal Learning for Domain Adaptation in 3D Semantic Segmentation . Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Émilie Wirbel, and Patrick Pérez IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022 . . Solving Disjunctive Temporal Networks with Uncertainty under Restricted Time-Based Controllability using Tree Search and Graph Neural Networks . Kevin Osanlou, Jeremy Frank, J. Benton, Andrei Bursuc, Christophe Guettier, Tristan Cazenave and Eric Jacopin AAAI Conference on Artificial Intelligence (AAAI), 2022 . . Driving behavior explanation with multi-level fusion . Hédi Ben-Younes*, Éloi Zablocki*, Patrick Pérez, Matthieu Cord Pattern Recognition (PR), 2022 . 2021 . Localizing Objects with Self-Supervised Transformers and no Labels . Oriane Siméoni, Gilles Puy, Huy V. Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Renaud Marlet, Jean Ponce British Machine Vision Conference (BMVC), 2021 . . Large-Scale Unsupervised Object Discovery . Huy V. Vo, Elena Sizikova, Cordelia Schmid, Patrick Pérez and Jean Ponce Advances in Neural Information Processing Systems (NeurIPS), 2021 . . Handling new target classes in semantic segmentation with domain adaptation . Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick Pérez Computer Vision and Image Understanding (CVIU), 2021 . . Robust Semantic Segmentation with Superpixel-Mix . Gianni Franchi, Nacim Belkhir, Mai Lan Ha, Yufei Hu, Andrei Bursuc, Volker Blanz, Angela Yao British Machine Vision Conference (BMVC), 2021 . . Generative Zero-Shot Learning for Semantic Segmentation of 3D Point Clouds . Bjoern Michele, Alexandre Boulch, Gilles Puy, Maxime Bucher, and Renaud Marlet International Conference on 3D Vision (3DV), 2021 . . NeeDrop: Unsupervised Shape Representation from Sparse Point Clouds using Needle Dropping . Alexandre Boulch, Pierre-Alain Langlois, Gilles Puy, and Renaud Marlet International Conference on 3D Vision (3DV), 2021 . . PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds . Anh-Quan Cao, Gilles Puy, Alexandre Boulch, and Renaud Marlet International Conference on Computer Vision (ICCV), 2021 . . Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation . Victor Besnier, Andrei Bursuc, Alexandre Briot, and David Picard International Conference on Computer Vision (ICCV), 2021 . . Multi-View Radar Semantic Segmentation . Arthur Ouaknine, Alasdair Newson, Patrick Pérez, Florence Tupin and Julien Rebut International Conference on Computer Vision (ICCV), 2021 . . Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation . Antoine Saporta, Tuan-Hung Vu, Matthieu Cord and Patrick Pérez International Conference on Computer Vision (ICCV), 2021 . . StyleLess layer: Improving robustness for real-world driving . Julien Rebut, Andrei Bursuc, and Patrick Pérez IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021 . . Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning . Spyros Gidaris, Andrei Bursuc, Gilles Puy, Nikos Komodakis, Patrick Pérez, and Matthieu Cord Computer Vision and Pattern Recognition (CVPR), 2021 . . Confidence Estimation via Auxiliary Models . Charles Corbière, Nicolas Thome, Antoine Saporta, Tuan-Hung Vu, Matthieu Cord, and Patrick Pérez IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021 . . Semantic Palette: Guiding Scene Generation with Class Proportions . Guillaume Le Moing, Tuan-Hung Vu, Himalaya Jain, Patrick Pérez and Matthieu Cord Computer Vision and Pattern Recognition (CVPR), 2021 . . Artificial Dummies for Urban Dataset Augmentation . Antonín Vobecký, David Hurych, Michal Uřičář, Patrick Pérez, and Josef Šivic AAAI Conference on Artificial Intelligence (AAAI), 2021 . 2020 . STaRFlow: A SpatioTemporal Recurrent Cell for Lightweight Multi-Frame Optical Flow Estimation . Pierre Godet, Alexandre Boulch, Aurelien Plyer and Guy Le Besnerais International Conference on Pattern Recognition (ICPR), 2020 . . FKAConv: Feature-Kernel Alignment for Point Cloud Convolutiont . Alexandre Boulch, Gilles Puy, and Renaud Marlet Asian Conference on Computer Vision (ACCV), 2020 . . CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations . Arthur Ouaknine, Alasdair Newson, Julien Rebut, Florence Tupin and Patrick Pérez International Conference on Pattern Recognition (ICPR), 2020 . . PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving . Thibault Buhet, Emilie Wirbel, Andrei Bursuc and Xavier Perrotton Conference on Robot Learning (CoRL), 2020 . . Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems . Isabelle Leang, Ganesh Sistu, Fabian Burger, Andrei Bursuc, and Senthil Yogamani IEEE International Conference on Intelligent Transportation Systems (ITSC), 2020 . . TRADI: Tracking deep neural network weight distributions for uncertainty estimation . Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson, and Isabelle Bloch European Conference on Computer Vision (ECCV), 2020 . . Toward Unsupervised, Multi-Object Discovery in Large-Scale Image Collections . Huy V. Vo, Patrick Pérez and Jean Ponce European Conference on Computer Vision (ECCV), 2020 . . FLOT: Scene Flow on Point Clouds guided by Optimal Transport . Gilles Puy, Alexandre Boulch, and Renaud Marlet European Conference on Computer Vision (ECCV), 2020 . . QuEST: Quantized Embedding Space for Transferring Knowledge . Himalaya Jain, Spyros Gidaris, Nikos Komodakis, Patrick Pérez, and Matthieu Cord European Conference on Computer Vision (ECCV), 2020 . . xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation . Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Émilie Wirbel, and Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2020 . . ESL: Entropy-guided Self-supervised Learning for Domain Adaptation in Semantic Segmentation . Antoine Saporta, Tuan-Hung Vu, Matthieu Cord and Patrick Pérez Computer Vision and Pattern Recognition Workshop on Scalability in Autonomous Driving, 2020 . . End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances . Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde Computer Vision and Pattern Recognition (CVPR), 2020 . . Learning Representations by Predicting Bags of Visual Words . Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pérez, and Matthieu Cord Computer Vision and Pattern Recognition (CVPR), 2020 . . This dataset does not exist: training models from generated images . Victor Besnier, Himalaya Jain, Andrei Bursuc, Matthieu Cord, and Patrick Pérez International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2020 . . VRUNet: Multi-Task Learning Model for Intent Prediction of Vulnerable Road Users . Adithya Ranga, Filippo Giruzzi, Jagdish Bhanushali, Emilie Wirbel, Patrick Pérez, Tuan-Hung Vu, Xavier Perotton Electronic Imaging, 2020 . . ConvPoint: Continuous Convolutions for Point Cloud Processing . Alexandre Boulch Computers &amp; Graphics Journal, 2020 . 2019 . Zero-Shot Semantic Segmentation . Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick Pérez Neural Information Processing Systems (NeurIPS), 2019 . . Addressing Failure Prediction by Learning Model Confidence . Charles Corbière, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez Neural Information Processing Systems (NeurIPS), 2019 . . DADA: Depth-aware Domain Adaptation in Semantic Segmentation . Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Pérez International Conference on Computer Vision (ICCV), 2019 . . Optimal Solving of Constrained Path-Planning Problems with Graph Convolutional Networks and Optimized Tree Search . Kevin Osanlou, Andrei Bursuc, Christophe Guettier, Tristan Cazenave and Eric Jacopin International Conference on Intelligent Robots and Systems (IROS), 2019 . . Boosting Few-Shot Visual Learning With Self-Supervision . Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pérez, and Matthieu Cord International Conference on Computer Vision (ICCV), 2019 . . Unsupervised Image Matching and Object Discovery as Optimization . Huy V. Vo, Francis Bach, Minsu Cho, Kai Han, Yann Lecun, Patrick Pérez and Jean Ponce Computer Vision and Pattern Recognition (CVPR), 2019 . . AdvEnt: Adversarial Entropy minimization for domain adaptation in semantic segmentation . Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2019 .",
          "url": "https://valeoai.github.io/blog/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "About",
          "content": "Overview . valeo.ai is an international team based in Paris, conducting AI research for Valeo automotive applications, in collaboration with world-class academics. Our main research is towards better, clearer &amp; safer automotive AI. . You can find out more about our research through our papers, released code, tweets, and this blog. . Blog purpose . The aim of this blog is to provide an accessible, general-audience medium for valeo.ai researchers to communicate research publications and findings, as well as perspectives on the field. Posts are written by students and research scientists at valeo.ai. They are intended to provide insights and walk-throughs for our findings and results, both to experts and the general audience. . Translating Posts . If you wish to translate our blog posts, please contact the authors of the posts, as they own the copyright, and copy the editor, andrei.bursuc@valeo.com in your email. . Acknowledgments . This site is built with fastpages and hosted on Github. The design is based upon the Jekyll theme Minima. .",
          "url": "https://valeoai.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  

  

  
  

  
  

  
  

  
  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://valeoai.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  


  
  
      ,"page0": {
          "title": "Multi-sensor perception",
          "content": "Multi-sensor perception . Automated driving relies first on a diverse range of sensors, like Valeo’s fish-eye cameras, LiDARs, radars and ultrasonics. Exploiting at best the outputs of each of these sensors at any instant is fundamental to understand the complex environment of the vehicle and gain robustness. To this end, we explore various machine learning approaches where sensors are considered either in isolation (as radar in Carrada at ICPR’20) or collectively (as in xMUDA at CVPR’20). . Publications . LaRa: Latents and Rays for Multi-Camera Bird&#39;s-Eye-View Semantic Segmentation . Florent Bartoccioni, Éloi Zablocki, Andrei Bursuc, Patrick Pérez, Matthieu Cord, Karteek Alahari Conference on Robot Learning, 2022 . . LiDARTouch: Monocular metric depth estimation with a few-beam LiDAR . Florent Bartoccioni, Éloi Zablocki, Patrick Pérez, Matthieu Cord, Karteek Alahari Computer Vision and Image Understanding (CVIU), 2022 . . Raw High-Definition Radar for Multi-Task Learning . Julien Rebut, Arthur Ouaknine, Waqas Malik, and Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2022 . . Cross-modal Learning for Domain Adaptation in 3D Semantic Segmentation . Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Émilie Wirbel, and Patrick Pérez IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022 . . Multi-View Radar Semantic Segmentation . Arthur Ouaknine, Alasdair Newson, Patrick Pérez, Florence Tupin and Julien Rebut International Conference on Computer Vision (ICCV), 2021 . . CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations . Arthur Ouaknine, Alasdair Newson, Julien Rebut, Florence Tupin and Patrick Pérez International Conference on Pattern Recognition (ICPR), 2020 . . PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving . Thibault Buhet, Emilie Wirbel, Andrei Bursuc and Xavier Perrotton Conference on Robot Learning (CoRL), 2020 . . Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems . Isabelle Leang, Ganesh Sistu, Fabian Burger, Andrei Bursuc, and Senthil Yogamani IEEE International Conference on Intelligent Transportation Systems (ITSC), 2020 . . xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation . Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Émilie Wirbel, and Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2020 .",
          "url": "https://valeoai.github.io/blog/projects/multi-sensor",
          "relUrl": "/projects/multi-sensor",
          "date": ""
      }
      
  

  
      ,"page1": {
          "title": "3D perception",
          "content": "3D perception . Each sensor delivers information about the 3D world around the vehicle. Making sense of this information in terms of drivable space and important objects (road users, curb, obstacles, street furnitures) in 3D is required for the driving system to plan and act in the safest and most confortable way. This encompasses several challenging tasks, in particular detection and segmentation of objects in point clouds as in FKAConv at ACCV’20. . Publications . SALUDA: Surface-based Automotive Lidar Unsupervised Domain Adaptation . Bjoern Michele, Alexandre Boulch, Gilles Puy, Tuan-Hung Vu, Renaud Marlet and Nicolas Courty International Conference on 3D Vision (3DV), 2024 . . BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds . Corentin Sautier, Gilles Puy, Alexandre Boulch, Renaud Marlet, Vincent Lepetit International Conference on 3D Vision (3DV), 2024 . . Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation . Gilles Puy, Alexandre Boulch, and Renaud Marlet International Conference on Computer Vision (ICCV), 2023 . . DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion . Cédric Rommel, Eduardo Valle, Mickaël Chen, Souhaiel Khalfaoui, Renaud Marlet, Matthieu Cord, Patrick Pérez International Conference on Computer Vision (ICCV) Workshop, 2023 . . You Never Get a Second Chance To Make a Good First Impression: Seeding Active Learning for 3D Semantic Segmentation . Nermin Samet, Oriane Siméoni, Gilles Puy, Georgy Ponimatkin, Renaud Marlet, and Vincent Lepetit International Conference on Computer Vision (ICCV), 2023 . . RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving . Angelika Ando, Spyros Gidaris, Andrei Bursuc, Gilles Puy, Alexandre Boulch, and Renaud Marlet Computer Vision and Pattern Recognition (CVPR), 2023 . . ALSO: Automotive Lidar Self-supervision by Occupancy estimation . Alexandre Boulch, Corentin Sautier, Björn Michele, Gilles Puy, Renaud Marlet Computer Vision and Pattern Recognition (CVPR), 2023 . . LaRa: Latents and Rays for Multi-Camera Bird&#39;s-Eye-View Semantic Segmentation . Florent Bartoccioni, Éloi Zablocki, Andrei Bursuc, Patrick Pérez, Matthieu Cord, Karteek Alahari Conference on Robot Learning, 2022 . . LiDARTouch: Monocular metric depth estimation with a few-beam LiDAR . Florent Bartoccioni, Éloi Zablocki, Patrick Pérez, Matthieu Cord, Karteek Alahari Computer Vision and Image Understanding (CVIU), 2022 . . POCO: Point Convolution for Surface Reconstruction . Alexandre Boulch and Renaud Marlet Computer Vision and Pattern Recognition (CVPR), 2022 . . Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data . Corentin Sautier, Gilles Puy, Spyros Gidaris, Alexandre Boulch, Andrei Bursuc, and Renaud Marlet Computer Vision and Pattern Recognition (CVPR), 2022 . . Generative Zero-Shot Learning for Semantic Segmentation of 3D Point Clouds . Bjoern Michele, Alexandre Boulch, Gilles Puy, Maxime Bucher, and Renaud Marlet International Conference on 3D Vision (3DV), 2021 . . NeeDrop: Unsupervised Shape Representation from Sparse Point Clouds using Needle Dropping . Alexandre Boulch, Pierre-Alain Langlois, Gilles Puy, and Renaud Marlet International Conference on 3D Vision (3DV), 2021 . . PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds . Anh-Quan Cao, Gilles Puy, Alexandre Boulch, and Renaud Marlet International Conference on Computer Vision (ICCV), 2021 . . FKAConv: Feature-Kernel Alignment for Point Cloud Convolutiont . Alexandre Boulch, Gilles Puy, and Renaud Marlet Asian Conference on Computer Vision (ACCV), 2020 . . STaRFlow: A SpatioTemporal Recurrent Cell for Lightweight Multi-Frame Optical Flow Estimation . Pierre Godet, Alexandre Boulch, Aurelien Plyer and Guy Le Besnerais International Conference on Pattern Recognition (ICPR), 2020 . . FLOT: Scene Flow on Point Clouds guided by Optimal Transport . Gilles Puy, Alexandre Boulch, and Renaud Marlet European Conference on Computer Vision (ECCV), 2020 . . ConvPoint: Continuous Convolutions for Point Cloud Processing . Alexandre Boulch Computers &amp; Graphics Journal, 2020 .",
          "url": "https://valeoai.github.io/blog/projects/3d-perception",
          "relUrl": "/projects/3d-perception",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Frugal learning",
          "content": "Frugal learning . Collecting diverse enough data, and annotating it precisely, is complex, costly and time-consuming. To reduce dramatically these needs, we explore various alternatives to fully-supervised learning, e.g, training that is unsupervised (as rOSD at ECCCV’20), self-supervised (as BoWNet at CVPR’20), semi-supervised, active, zero-shot (as ZS3 at NeurIPS’19) or few-shot. We also investigate training with fully-synthetic data (in combination with unsupervised domain adaptation) and with GAN-augmented data (as Semantic Palette at CVPR’21). . Publications . BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds . Corentin Sautier, Gilles Puy, Alexandre Boulch, Renaud Marlet, Vincent Lepetit International Conference on 3D Vision (3DV), 2024 . . SALUDA: Surface-based Automotive Lidar Unsupervised Domain Adaptation . Bjoern Michele, Alexandre Boulch, Gilles Puy, Tuan-Hung Vu, Renaud Marlet and Nicolas Courty International Conference on 3D Vision (3DV), 2024 . . CLIP-DIY: CLIP Dense Inference Yields Open-Vocabulary Semantic Segmentation For-Free . Monika Wysoczańska, Michaël Ramamonjisoa, Tomasz Trzciński, Oriane Siméoni Winter Conference on Applications of Computer Vision (WACV), 2024 . . You Never Get a Second Chance To Make a Good First Impression: Seeding Active Learning for 3D Semantic Segmentation . Nermin Samet, Oriane Siméoni, Gilles Puy, Georgy Ponimatkin, Renaud Marlet, and Vincent Lepetit International Conference on Computer Vision (ICCV), 2023 . . PØDA: Prompt-driven Zero-shot Domain Adaptation . Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette International Conference on Computer Vision (ICCV), 2023 . . RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving . Angelika Ando, Spyros Gidaris, Andrei Bursuc, Gilles Puy, Alexandre Boulch, and Renaud Marlet Computer Vision and Pattern Recognition (CVPR), 2023 . . ALSO: Automotive Lidar Self-supervision by Occupancy estimation . Alexandre Boulch, Corentin Sautier, Björn Michele, Gilles Puy, Renaud Marlet Computer Vision and Pattern Recognition (CVPR), 2023 . . Unsupervised Object Localization: Observing the Background to Discover Objects . Oriane Siméoni, Chloé Sekkat, Gilles Puy, Antonin Vobecky, Éloi Zablocki, Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2023 . . Active Learning Strategies for Weakly-Supervised Object Detection . Huy V. Vo, Oriane Siméoni, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, and Jean Ponce European Conference on Computer Vision (ECCV), 2022 . . What to Hide from Your Students: Attention-Guided Masked Image Modeling . Ioannis Kakogeorgiou, Spyros Gidaris, Bill Psomas, Yannis Avrithis, Andrei Bursuc, Konstantinos Karantzalos, and Nikos Komodakis European Conference on Computer Vision (ECCV), 2022 . . Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data . Corentin Sautier, Gilles Puy, Spyros Gidaris, Alexandre Boulch, Andrei Bursuc, and Renaud Marlet Computer Vision and Pattern Recognition (CVPR), 2022 . . CSG0: Continual Urban Scene Generation with Zero Forgetting . Himalaya Jain (*), Tuan-Hung Vu (*), Patrick Pérez and Matthieu Cord (* equal contrib.) Computer Vision and Pattern Recognition (CVPR) Workshop, 2022 . . Localizing Objects with Self-Supervised Transformers and no Labels . Oriane Siméoni, Gilles Puy, Huy V. Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Renaud Marlet, Jean Ponce British Machine Vision Conference (BMVC), 2021 . . Large-Scale Unsupervised Object Discovery . Huy V. Vo, Elena Sizikova, Cordelia Schmid, Patrick Pérez and Jean Ponce Advances in Neural Information Processing Systems (NeurIPS), 2021 . . Handling new target classes in semantic segmentation with domain adaptation . Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick Pérez Computer Vision and Image Understanding (CVIU), 2021 . . Generative Zero-Shot Learning for Semantic Segmentation of 3D Point Clouds . Bjoern Michele, Alexandre Boulch, Gilles Puy, Maxime Bucher, and Renaud Marlet International Conference on 3D Vision (3DV), 2021 . . Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning . Spyros Gidaris, Andrei Bursuc, Gilles Puy, Nikos Komodakis, Patrick Pérez, and Matthieu Cord Computer Vision and Pattern Recognition (CVPR), 2021 . . Semantic Palette: Guiding Scene Generation with Class Proportions . Guillaume Le Moing, Tuan-Hung Vu, Himalaya Jain, Patrick Pérez and Matthieu Cord Computer Vision and Pattern Recognition (CVPR), 2021 . . Artificial Dummies for Urban Dataset Augmentation . Antonín Vobecký, David Hurych, Michal Uřičář, Patrick Pérez, and Josef Šivic AAAI Conference on Artificial Intelligence (AAAI), 2021 . . Toward Unsupervised, Multi-Object Discovery in Large-Scale Image Collections . Huy V. Vo, Patrick Pérez and Jean Ponce European Conference on Computer Vision (ECCV), 2020 . . Learning Representations by Predicting Bags of Visual Words . Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pérez, and Matthieu Cord Computer Vision and Pattern Recognition (CVPR), 2020 . . This dataset does not exist: training models from generated images . Victor Besnier, Himalaya Jain, Andrei Bursuc, Matthieu Cord, and Patrick Pérez International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2020 . . Zero-Shot Semantic Segmentation . Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick Pérez Neural Information Processing Systems (NeurIPS), 2019 . . Boosting Few-Shot Visual Learning With Self-Supervision . Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pérez, and Matthieu Cord International Conference on Computer Vision (ICCV), 2019 . . Unsupervised Image Matching and Object Discovery as Optimization . Huy V. Vo, Francis Bach, Minsu Cho, Kai Han, Yann Lecun, Patrick Pérez and Jean Ponce Computer Vision and Pattern Recognition (CVPR), 2019 .",
          "url": "https://valeoai.github.io/blog/projects/limited-supervision",
          "relUrl": "/projects/limited-supervision",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "Domain adaptation",
          "content": "Domain adaptation . Deep learning and reinforcement learning are key technologies for autonomous driving. One of the challenges they face is to adapt to conditions which differ from those met during training. To improve systems’ performance in such situations, we explore so-called “domain adaptation” techniques, as in AdvEnt at CVPR’19 and DADA its extension at ICCV’19. We propose new solutions to more practical DA scenarios in MTAF (ICCV&#39;21) to handle multiple target domains and in BUDA (CVIU&#39;21) to handle new target classes. In xMUDA (CVPR&#39;20), we introduce a new framework to tackle the challenging adaptation problem on both 2D image and 3D point-cloud spaces. . Publications . SALUDA: Surface-based Automotive Lidar Unsupervised Domain Adaptation . Bjoern Michele, Alexandre Boulch, Gilles Puy, Tuan-Hung Vu, Renaud Marlet and Nicolas Courty International Conference on 3D Vision (3DV), 2024 . . PØDA: Prompt-driven Zero-shot Domain Adaptation . Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette International Conference on Computer Vision (ICCV), 2023 . . Multi-Head Distillation for Continual Unsupervised Domain Adaptation in Semantic Segmentation . Antoine Saporta, Arthur Douillard, Tuan-Hung Vu, Patrick Pérez and Matthieu Cord Computer Vision and Pattern Recognition (CVPR) Workshop, 2022 . . Cross-modal Learning for Domain Adaptation in 3D Semantic Segmentation . Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Émilie Wirbel, and Patrick Pérez IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022 . . Handling new target classes in semantic segmentation with domain adaptation . Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick Pérez Computer Vision and Image Understanding (CVIU), 2021 . . Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation . Antoine Saporta, Tuan-Hung Vu, Matthieu Cord and Patrick Pérez International Conference on Computer Vision (ICCV), 2021 . . Semantic Palette: Guiding Scene Generation with Class Proportions . Guillaume Le Moing, Tuan-Hung Vu, Himalaya Jain, Patrick Pérez and Matthieu Cord Computer Vision and Pattern Recognition (CVPR), 2021 . . Confidence Estimation via Auxiliary Models . Charles Corbière, Nicolas Thome, Antoine Saporta, Tuan-Hung Vu, Matthieu Cord, and Patrick Pérez IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021 . . ESL: Entropy-guided Self-supervised Learning for Domain Adaptation in Semantic Segmentation . Antoine Saporta, Tuan-Hung Vu, Matthieu Cord and Patrick Pérez Computer Vision and Pattern Recognition Workshop on Scalability in Autonomous Driving, 2020 . . xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation . Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Émilie Wirbel, and Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2020 . . DADA: Depth-aware Domain Adaptation in Semantic Segmentation . Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Pérez International Conference on Computer Vision (ICCV), 2019 . . AdvEnt: Adversarial Entropy minimization for domain adaptation in semantic segmentation . Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2019 .",
          "url": "https://valeoai.github.io/blog/projects/domain-adaptation",
          "relUrl": "/projects/domain-adaptation",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Reliability",
          "content": "Reliability . When the unexpected happens, when the weather badly degrades, when a sensor gets blocked, the embarked perception system should diagnose the situation and react accordingly, e.g., by calling an alternative system or the human driver. With this in mind, we investigate ways to improve the robustness of neural nets to input variations, including to adversarial attacks, and to predict automatically the performance and the confidence of their predictions as in ConfidNet at NeurIPS’19. . Publications . Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis . Victor Letzelter, Mathieu Fontaine, Mickaël Chen, Patrick Pérez, Slim Essid, and Gaël Richard Advances in Neural Information Processing Systems (NeurIPS), 2023 . . Packed Ensembles for efficient uncertainty estimation . Olivier Laurent, Adrien Lafage, Enzo Tartaglione, Geoffrey Daniel, Jean-Marc Martinez, Andrei Bursuc, and Gianni Franchi International Conference on Learning Representations (ICLR), 2023 . . Latent Discriminant deterministic Uncertainty . Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson, and David Filliat European Conference on Computer Vision (ECCV), 2022 . . Solving Disjunctive Temporal Networks with Uncertainty under Restricted Time-Based Controllability using Tree Search and Graph Neural Networks . Kevin Osanlou, Jeremy Frank, J. Benton, Andrei Bursuc, Christophe Guettier, Tristan Cazenave and Eric Jacopin AAAI Conference on Artificial Intelligence (AAAI), 2022 . . Robust Semantic Segmentation with Superpixel-Mix . Gianni Franchi, Nacim Belkhir, Mai Lan Ha, Yufei Hu, Andrei Bursuc, Volker Blanz, Angela Yao British Machine Vision Conference (BMVC), 2021 . . Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation . Victor Besnier, Andrei Bursuc, Alexandre Briot, and David Picard International Conference on Computer Vision (ICCV), 2021 . . StyleLess layer: Improving robustness for real-world driving . Julien Rebut, Andrei Bursuc, and Patrick Pérez IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021 . . Confidence Estimation via Auxiliary Models . Charles Corbière, Nicolas Thome, Antoine Saporta, Tuan-Hung Vu, Matthieu Cord, and Patrick Pérez IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021 . . TRADI: Tracking deep neural network weight distributions for uncertainty estimation . Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson, and Isabelle Bloch European Conference on Computer Vision (ECCV), 2020 . . Addressing Failure Prediction by Learning Model Confidence . Charles Corbière, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez Neural Information Processing Systems (NeurIPS), 2019 . . Optimal Solving of Constrained Path-Planning Problems with Graph Convolutional Networks and Optimized Tree Search . Kevin Osanlou, Andrei Bursuc, Christophe Guettier, Tristan Cazenave and Eric Jacopin International Conference on Intelligent Robots and Systems (IROS), 2019 .",
          "url": "https://valeoai.github.io/blog/projects/reliability",
          "relUrl": "/projects/reliability",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "Driving in action",
          "content": "Driving in action . Getting from sensory inputs to car control goes either through a modular stack (perception &gt; localization &gt; forecast &gt; planning &gt; actuation) or, more radically, through a single end-to-end model. We work on both strategies, more specificaly on action forecasting, automatic interpretation of decisions taken by a driving system, and reinforcement / imitation learning for end-to-end systems (as in RL work at CVPR’20). . Publications . Towards Motion Forecasting with Real-World Perception Inputs: Are End-to-End Approaches Competitive? . Yihong Xu, Loïck Chambon, Éloi Zablocki, Mickaël Chen, Alexandre Alahi, Matthieu Cord, Patrick Pérez ICCV Workshop &amp; Challenge on Event Detection for Situation Awareness in Autonomous Driving (ROAD++), 2023 . . OCTET: Object-aware Counterfactual Explanations . Mehdi Zemni, Mickaël Chen, Éloi Zablocki, Hédi Ben-Younes, Patrick Pérez, Matthieu Cord Computer Vision and Pattern Recognition (CVPR), 2023 . . LiDARTouch: Monocular metric depth estimation with a few-beam LiDAR . Florent Bartoccioni, Éloi Zablocki, Patrick Pérez, Matthieu Cord, Karteek Alahari Computer Vision and Image Understanding (CVIU), 2022 . . LaRa: Latents and Rays for Multi-Camera Bird&#39;s-Eye-View Semantic Segmentation . Florent Bartoccioni, Éloi Zablocki, Andrei Bursuc, Patrick Pérez, Matthieu Cord, Karteek Alahari Conference on Robot Learning, 2022 . . STEEX: Steering Counterfactual Explanations with Semantics . Paul Jacob, Éloi Zablocki, Hédi Ben-Younes, Mickaël Chen, Patrick Pérez, Matthieu Cord European Conference on Computer Vision, 2022 . . Explainability of deep vision-based autonomous driving systems: Review and challenges . Éloi Zablocki*, Hédi Ben-Younes*, Patrick Pérez, Matthieu Cord International Journal of Computer Vision, 2022 . . Raising context awareness in motion forecasting . Hédi Ben-Younes*, Éloi Zablocki*, Mickaël Chen, Patrick Pérez, Matthieu Cord Computer Vision and Pattern Recognition (CVPR) Workshop, 2022 . . Driving behavior explanation with multi-level fusion . Hédi Ben-Younes*, Éloi Zablocki*, Patrick Pérez, Matthieu Cord Pattern Recognition (PR), 2022 . . PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving . Thibault Buhet, Emilie Wirbel, Andrei Bursuc and Xavier Perrotton Conference on Robot Learning (CoRL), 2020 . . End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances . Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde Computer Vision and Pattern Recognition (CVPR), 2020 . . VRUNet: Multi-Task Learning Model for Intent Prediction of Vulnerable Road Users . Adithya Ranga, Filippo Giruzzi, Jagdish Bhanushali, Emilie Wirbel, Patrick Pérez, Tuan-Hung Vu, Xavier Perotton Electronic Imaging, 2020 .",
          "url": "https://valeoai.github.io/blog/projects/driving",
          "relUrl": "/projects/driving",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "Core Deep Learning",
          "content": "Core Deep Learning . Deep learning being now a key component of AD systems, it is important to get a better understanding of its inner workings, in particular the link between the specifics of the learning optimization and the key properities (performance, regularity, robustness, generalization) of the trained models. Among other things, we investigate the impact of popular batch normalization on standard learning procedures and the ability to learn through unsupervised distillation. . Publications . Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis . Victor Letzelter, Mathieu Fontaine, Mickaël Chen, Patrick Pérez, Slim Essid, and Gaël Richard Advances in Neural Information Processing Systems (NeurIPS), 2023 . . Spherical perspective on learning with normalization layers . Simon Roburin, Yann de Mont-Marin, Andrei Bursuc, Renaud Marlet, Patrick Pérez, Mathieu Aubry Neurocomputing, 2022 . . QuEST: Quantized Embedding Space for Transferring Knowledge . Himalaya Jain, Spyros Gidaris, Nikos Komodakis, Patrick Pérez, and Matthieu Cord European Conference on Computer Vision (ECCV), 2020 .",
          "url": "https://valeoai.github.io/blog/projects/deep-learning",
          "relUrl": "/projects/deep-learning",
          "date": ""
      }
      
  

  
      ,"page7": {
          "title": "Interpretability and Explainability of Deep Models",
          "content": "Interpretability and Explainability of Deep Models . The concept of explainability has several facets and the need for explainability is strong in safety-critical applications such as autonomous driving where deep learning models are now widely used. As the underlying mechanisms of these models remain opaque, explainability and trustworthiness have become major concerns. Among other things, we investigate methods providing explanations to a black-box visual-based systems in a post-hoc fashion, as well as approaches that aim at building more interpretable self-driving systems by design. . Publications . OCTET: Object-aware Counterfactual Explanations . Mehdi Zemni, Mickaël Chen, Éloi Zablocki, Hédi Ben-Younes, Patrick Pérez, Matthieu Cord Computer Vision and Pattern Recognition (CVPR), 2023 . . STEEX: Steering Counterfactual Explanations with Semantics . Paul Jacob, Éloi Zablocki, Hédi Ben-Younes, Mickaël Chen, Patrick Pérez, Matthieu Cord European Conference on Computer Vision, 2022 . . Explainability of deep vision-based autonomous driving systems: Review and challenges . Éloi Zablocki*, Hédi Ben-Younes*, Patrick Pérez, Matthieu Cord International Journal of Computer Vision, 2022 . . Driving behavior explanation with multi-level fusion . Hédi Ben-Younes*, Éloi Zablocki*, Patrick Pérez, Matthieu Cord Pattern Recognition (PR), 2022 .",
          "url": "https://valeoai.github.io/blog/projects/explainability",
          "relUrl": "/projects/explainability",
          "date": ""
      }
      
  



  
      ,"page0": {
          "title": "Optimal Solving of Constrained Path-Planning Problems with Graph Convolutional Networks and Optimized Tree Search",
          "content": "Optimal Solving of Constrained Path-Planning Problems with Graph Convolutional Networks and Optimized Tree Search . Kevin Osanlou&nbsp;&nbsp; Christophe Guettier&nbsp;&nbsp; Tristan Cazenave&nbsp;&nbsp; Eric Jacopin . IROS 2019 . Paper&nbsp;&nbsp; . . Abstract . Deep learning-based methods are growing prominence for planning purposes. In this paper, we present a hybrid planner that combines a graph machine learning model and an optimal solver based on branch and bound tree search for path-planning tasks. More specifically, a graph neural network is used to assist the branch and bound algorithm in handling constraints associated with a desired solution path. There are multiple downstream practical applications, such as Autonomous Unmanned Ground Vehicles (AUGV), typically deployed in disaster relief or search and rescue operations. In off-road environments, AUGVs must dynamically optimize a source-destination path under various operational constraints, out of which several are difficult to predict in advance and need to be addressed online. We conduct experiments on realistic scenarios and show that graph neural network support enables substantial speedup and smoother scaling to harder path-planning problems. Additionally, information provided by the graph neural network enables the approach to outperform problem-specific handcrafted heuristics, highlighting the potential graph neural networks hold for path-planning tasks. . . BibTeX . @inproceedings{osanlou2019optimal, title={Optimal Solving of Constrained Path-Planning Problems with Graph Convolutional Networks and Optimized Tree Search}, author={Osanlou, Kevin and Bursuc, Andrei and Guettier, Christophe and Cazenave, Tristan and Jacopin, Eric}, booktitle={IROS}, year={2019}, } . .",
          "url": "https://valeoai.github.io/blog/publications/hybrid-planner/",
          "relUrl": "/publications/hybrid-planner/",
          "date": ""
      }
      
  

  
      ,"page1": {
          "title": "ConvPoint: Continuous Convolutions for Point Cloud Processing",
          "content": "ConvPoint: Continuous Convolutions for Point Cloud Processing . Alexandre Boulch &nbsp;&nbsp; . CaG 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; Slides&nbsp;&nbsp; . . Abstract . Point clouds are unstructured and unordered data, as opposed to images. Thus, most machine learning approach developed for image cannot be directly transferred to point clouds. In this paper, we propose a generalization of discrete convolutional neural networks (CNNs) in order to deal with point clouds by replacing discrete kernels by continuous ones. This formulation is simple, allows arbitrary point cloud sizes and can easily be used for designing neural networks similarly to 2D CNNs. We present experimental results with various architectures, highlighting the flexibility of the proposed approach. We obtain competitive results compared to the state-of-the-art on shape classification, part segmentation and semantic segmentation for large-scale point clouds. . . BibTeX . @article{boulch2020convpoint, title={ConvPoint: Continuous convolutions for point cloud processing}, author={Boulch, Alexandre}, journal={Computers &amp; Graphics}, year={2020}, publisher={Elsevier} } . .",
          "url": "https://valeoai.github.io/blog/publications/2020_convpoint/",
          "relUrl": "/publications/2020_convpoint/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "STaRFlow: A SpatioTemporal Recurrent Cell for Lightweight Multi-Frame Optical Flow Estimation",
          "content": "STaRFlow: A SpatioTemporal Recurrent Cell for Lightweight Multi-Frame Optical Flow Estimation . Pierre Godet &nbsp;&nbsp; Alexandre Boulch &nbsp;&nbsp; Aurelien Plyer &nbsp;&nbsp; Guy Le Besnerais&nbsp;&nbsp; . ICPR 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . We present a new lightweight CNN-based algorithm for multi-frame optical flow estimation. Our solution introduces a double recurrence over spatial scale and time through repeated use of a generic “STaR” (SpatioTemporal Recurrent) cell. It includes (i) a temporal recurrence based on conveying learned features rather than optical flow estimates; (ii) an occlusion detection process which is coupled with optical flow estimation and therefore uses a very limited number of extra parameters. The resulting STaRFlow algorithm gives state-of-the-art performances on MPI Sintel and Kitti2015 and involves significantly less parameters than all other methods with comparable results. . . BibTeX . @inproceedings{godet2021starflow, title={STaRFlow: A SpatioTemporal Recurrent Cell for Lightweight Multi-Frame Optical Flow Estimation}, author={Godet, Pierre and Boulch, Alexandre and Plyer, Aur{ &#39;e}lien and Le Besnerais, Guy}, booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, pages={2462--2469}, year={2021}, organization={IEEE} } . .",
          "url": "https://valeoai.github.io/blog/publications/2020_starflow/",
          "relUrl": "/publications/2020_starflow/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "Generative Zero-Shot Learning for Semantic Segmentation of 3D Point Clouds",
          "content": "Generative Zero-Shot Learning for Semantic Segmentation of 3D Point Clouds . Bjoern Michele&nbsp;&nbsp; Alexandre Boulch&nbsp;&nbsp; Gilles Puy &nbsp;&nbsp; Maxime Bucher&nbsp;&nbsp; Renaud Marlet&nbsp;&nbsp; . 3DV 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; Slides&nbsp;&nbsp; . . Abstract . While there has been a number of studies on Zero-Shot Learning (ZSL) for 2D images, its application to 3D data is still recent and scarce, with just a few methods limited to classification. We present the first generative approach for both ZSL and Generalized ZSL (GZSL) on 3D data, that can handle both classification and, for the first time, semantic segmentation. We show that it reaches or outperforms the state of the art on ModelNet40 classification for both inductive ZSL and inductive GZSL. For semantic segmentation, we created three benchmarks for evaluating this new ZSL task, using S3DIS, ScanNet and SemanticKITTI. Our experiments show that our method outperforms strong baselines, which we additionally propose for this task. . . BibTeX . @inproceedings{michele2021generative, title={Generative Zero-Shot Learning for Semantic Segmentation of {3D} Point Cloud}, author={Michele, Bj{ &quot;o}rn and Boulch, Alexandre and Puy, Gilles and Bucher, Maxime and Marlet, Renaud}, booktitle={International Conference on 3D Vision (3DV)}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/3dgenz/",
          "relUrl": "/publications/3dgenz/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Driving behavior explanation with multi-level fusion",
          "content": "Driving behavior explanation with multi-level fusion . Hédi Ben-Younes &nbsp;&nbsp; Éloi Zablocki &nbsp;&nbsp; Patrick Pérez &nbsp;&nbsp; Matthieu Cord . Pattern Recognition 2022 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . In this era of active development of autonomous vehicles, it becomes crucial to provide driving systems with the capacity to explain their decisions. In this work, we focus on generating high-level driving explanations as the vehicle drives. We present BEEF, for BEhavior Explanation with Fusion, a deep architecture which explains the behavior of a trajectory prediction model. Supervised by annotations of human driving decisions justifications, BEEF learns to fuse features from multiple levels. Leveraging recent advances in the multi-modal fusion literature, BEEF is carefully designed to model the correlations between high-level decisions features and mid-level perceptual features. The flexibility and efficiency of our approach are validated with extensive experiments on the HDD and BDD-X datasets. . . BibTeX . @article{beef2021, author = {Hedi Ben{-}Younes and { &#39;{E}}loi Zablocki and Patrick P{ &#39;{e}}rez and Matthieu Cord}, title = {Driving Behavior Explanation with Multi-level Fusion}, journal = {Pattern Recognition (PR)}, year = {2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/beef/",
          "relUrl": "/publications/beef/",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "NeeDrop: Unsupervised Shape Representation from Sparse Point Clouds using Needle Dropping",
          "content": "NeeDrop: Unsupervised Shape Representation from Sparse Point Clouds using Needle Dropping . Alexandre Boulch   Pierre-Alain Langlois    Gilles Puy    Renaud Marlet   &lt;/h3&gt; . 3DV 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . There has been recently a growing interest for implicit shape representations. Contrary to explicit representations, they have no resolution limitations and they easily deal with a wide variety of surface topologies. To learn these implicit representations, current approaches rely on a certain level of shape supervision (e.g., inside/outside information or distance-to-shape knowledge), or at least require a dense point cloud (to approximate well enough the distance-to-shape). In contrast, we introduce NeeDrop, a self-supervised method for learning shape representations from possibly extremely sparse point clouds. Like in Buffon’s needle problem, we “drop” (sample) needles on the point cloud and consider that, statistically, close to the surface, the needle end points lie on opposite sides of the surface. No shape knowledge is required and the point cloud can be highly sparse, e.g., as lidar point clouds acquired by vehicles. Previous self-supervised shape representation approaches fail to produce good-quality results on this kind of data. We obtain quantitative results on par with existing supervised approaches on shape reconstruction datasets and show promising qualitative results on hard autonomous driving datasets such as KITTI. . . BibTeX . @inproceedings{boulch2021needrop, title={NeeDrop: Self-supervised Shape Representation from Sparse Point Clouds using Needle Dropping}, author={Boulch, Alexandre and Langlois, Pierre-Alain and Puy, Gilles and Marlet, Renaud}, booktitle={International Conference on 3D Vision (3DV)}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/needrop/",
          "relUrl": "/publications/needrop/",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation",
          "content": "Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation . Victor Besnier&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Alexandre Briot&nbsp;&nbsp; David Picard . ICCV 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . In this paper, we tackle the detection of out-of-distribution (OOD) objects in semantic segmentation. By analyzing the literature, we found that current methods are either accurate or fast but not both which limits their usability in real world applications. To get the best of both aspects, we propose to mitigate the common shortcomings by following four design principles: decoupling the OOD detection from the segmentation task, observing the entire segmentation network instead of just its output, generating training data for the OOD detector by leveraging blind spots in the segmentation network and focusing the generated data on localized regions in the image to simulate OOD objects. Our main contribution is a new OOD detection architecture called ObsNet associated with a dedicated training scheme based on Local Adversarial Attacks (LAA). We validate the soundness of our approach across numerous ablation studies. We also show it obtains top performances both in speed and accuracy when compared to ten recent methods of the literature on three different datasets. . . . BibTeX . @inproceedings{besnier2021triggering, title = {Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation}, author = {Besnier, Victor and Bursuc, Andrei and Picard, David and Briot Alexandre}, booktitle={Proceedings of the IEEE International Conference on Computer Vision}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/obsnet/",
          "relUrl": "/publications/obsnet/",
          "date": ""
      }
      
  

  
      ,"page7": {
          "title": "StyleLess layer: Improving robustness for real-world driving",
          "content": "StyleLess layer: Improving robustness for real-world driving . Julien Rebut &nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Patrick Pérez . IROS 2021 . Paper&nbsp;&nbsp; . . Abstract . Deep Neural Networks (DNNs) are a critical component for self-driving vehicles. They achieve impressive performance by reaping information from high amounts of labeled data. Yet, the full complexity of the real world cannot be encapsulated in the training data, no matter how big the dataset, and DNNs can hardly generalize to unseen conditions. Robustness to various image corruptions, caused by changing weather conditions or sensor degradation and aging, is crucial for safety when such vehicles are deployed in the real world. We address this problem through a novel type of layer, dubbed StyleLess, which enables DNNs to learn robust and informative features that can cope with varying external conditions. We propose multiple variations of this layer that can be integrated in most of the architectures and trained jointly with the main task. We validate our contribution on typical autonomous-driving tasks (detection, semantic segmentation), showing that in most cases, this approach improves predictive performance on unseen conditions (fog, rain), while preserving performance on seen conditions and objects. . . BibTeX . @inproceedings{rebut2021styleless, title={StyleLess layer: Improving robustness for real-world driving}, author={Rebut, Julien and Bursuc, Andrei and P{ &#39;e}rez, Patrick}, booktitle={IROS}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/styleless/",
          "relUrl": "/publications/styleless/",
          "date": ""
      }
      
  

  
      ,"page8": {
          "title": "Robust Semantic Segmentation with Superpixel-Mix",
          "content": "Robust Semantic Segmentation with Superpixel-Mix . Gianni Franchi&nbsp;&nbsp; Nacim Belkhir&nbsp;&nbsp; Mai Lan Ha&nbsp;&nbsp; Yufei Hu &nbsp;&nbsp;Andrei Bursuc&nbsp;&nbsp; Volker Blanz&nbsp;&nbsp; Angela Yao . BMVC 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Along with predictive performance and runtime speed, reliability is a key requirement for real-world semantic segmentation. Reliability encompasses robustness, predictive uncertainty and reduced bias. To improve reliability, we introduce Superpixel-mix, a new superpixel-based data augmentation method with teacher-student consistency training. Unlike other mixing-based augmentation techniques, mixing superpixels between images is aware of object boundaries, while yielding consistent gains in segmentation accuracy. Our proposed technique achieves state-of-the-art results in semi-supervised semantic segmentation on the Cityscapes dataset. Moreover, Superpixel-mix improves the reliability of semantic segmentation by reducing network uncertainty and bias, as confirmed by competitive results under strong distributions shift (adverse weather, image corruptions) and when facing out-of-distribution data. . . BibTeX . @inproceedings{franchi2021robust, title={Robust Semantic Segmentation with Superpixel-Mix}, author={Franchi, Gianni and Belkhir, Nacim and Ha, Mai Lan and Hu, Yufei and Bursuc, Andrei and Blanz, Volker and Yao, Angela}, booktitle={BMVC}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/superpixelmix/",
          "relUrl": "/publications/superpixelmix/",
          "date": ""
      }
      
  

  
      ,"page9": {
          "title": "What to Hide from Your Students: Attention-Guided Masked Image Modeling",
          "content": "What to Hide from Your Students: Attention-Guided Masked Image Modeling . Ioannis Kakogeorgiou&nbsp;&nbsp; Spyros Gidaris&nbsp;&nbsp; Bill Psomas &nbsp;&nbsp; Yannis Avrithis &nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Konstantinos Karantzalos &nbsp;&nbsp; Nikos Komodakis . ECCV 2022 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . . Abstract . Transformers and masked language modeling are quickly being adopted and explored in computer vision as vision transformers and masked image modeling (MIM). In this work, we argue that image token masking differs from token masking in text, due to the amount and correlation of tokens in an image. In particular, to generate a challenging pretext task for MIM, we advocate a shift from random masking to informed masking. We develop and exhibit this idea in the context of distillation-based MIM, where a teacher transformer encoder generates an attention map, which we use to guide masking for the student. We thus introduce a novel masking strategy, called attention-guided masking (AttMask), and we demonstrate its effectiveness over random masking for dense distillation-based MIM as well as plain distillation-based self-supervised learning on classification tokens. We confirm that AttMask accelerates the learning process and improves the performance on a variety of downstream tasks. . . BibTeX . @inproceedings{kakogeorgiou2022hide, title={What to Hide from Your Students: Attention-Guided Masked Image Modeling}, author={Kakogeorgiou, Ioannis and Gidaris, Spyros and Psomas, Bill and Avrithis, Yannis and Bursuc, Andrei and Karantzalos, Konstantinos and Komodakis, Nikos}, booktitle={ECCV}, year={2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/attmask/",
          "relUrl": "/publications/attmask/",
          "date": ""
      }
      
  

  
      ,"page10": {
          "title": "Raising context awareness in motion forecasting",
          "content": "Raising context awareness in motion forecasting . Hédi Ben-Younes, Éloi Zablocki, Mickaël Chen, Patrick Pérez, Matthieu Cord . CVPR Workshop on Autonomous Driving (WAD) 2022 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Learning-based trajectory prediction models have encountered great success, with the promise of leveraging contextual information in addition to motion history. Yet, we find that state-of-the-art forecasting methods tend to overly rely on the agent&#39;s current dynamics, failing to exploit the semantic contextual cues provided at its input. To alleviate this issue, we introduce CAB, a motion forecasting model equipped with a training procedure designed to promote the use of semantic contextual information. We also introduce two novel metrics, dispersion and convergence-to-range, to measure the temporal consistency of successive forecasts, which we found missing in standard metrics. Our method is evaluated on the widely adopted nuScenes Prediction benchmark as well as on a subset of the most difficult examples from this benchmark. . . BibTeX . @inproceedings{cab2022, author = {Hedi Ben{-}Younes and { &#39;{E}}loi Zablocki and Micka{ &quot;{e}}l Chen and Patrick P{ &#39;{e}}rez and Matthieu Cord}, title = {Raising context awareness in motion forecasting}, booktitle = {CVPR Workshop on Autonomous Driving (WAD)}, year = {2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/cab/",
          "relUrl": "/publications/cab/",
          "date": ""
      }
      
  

  
      ,"page11": {
          "title": "CSG0: Continual Urban Scene Generation with Zero Forgetting",
          "content": "CSG0: Continual Urban Scene Generation with Zero Forgetting . Himalaya Jain (*)&nbsp;&nbsp; Tuan-Hung Vu (*)&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Matthieu Cord (* equal contrib.) . CVPRW 2022 . Paper&nbsp;&nbsp; . . Abstract . With the rapid advances in generative adversarial networks (GANs), the visual quality of synthesised scenes keeps improving, including for complex urban scenes with applications to automated driving. We address in this work a continual scene generation setup in which GANs are trained on a stream of distinct domains; ideally, the learned models should eventually be able to generate new scenes in all seen domains. This setup reflects the real-life scenario where data are continuously acquired in different places at different times. In such a continual setup, we aim for learning with zero forgetting, i.e., with no degradation in synthesis quality over earlier domains due to catastrophic forgetting. To this end, we introduce a novel framework that not only (i) enables seamless knowledge transfer in continual training but also (ii) guarantees zero forgetting with a small overhead cost. While being more memory efficient, thanks to continual learning, our model obtains better synthesis quality as compared against the brute-force solution that trains one full model for each domain. Especially, under extreme low-data regimes, our approach outperforms the brute-force one by a large margin. . . BibTeX . @inproceedings{jain2022csg0, title={CSG0: Continual Urban Scene Generation with Zero Forgetting}, author={Jain, Himalaya and Vu, Tuan-Hung and P{ &#39;e}rez, Patrick and Cord, Matthieu}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshop}, year={2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/csg0/",
          "relUrl": "/publications/csg0/",
          "date": ""
      }
      
  

  
      ,"page12": {
          "title": "Cross-task Attention Mechanism for Dense Multi-task Learning",
          "content": "Cross-task Attention Mechanism for Dense Multi-task Learning . Ivan Lopes&nbsp;&nbsp; Tuan-Hung Vu&nbsp;&nbsp; Raoul de Charette . WACV 2023 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Multi-task learning has recently become a promising solution for a comprehensive understanding of complex scenes. Not only being memory-efficient, multi-task models with an appropriate design can favor exchange of complementary signals across tasks. In this work, we jointly address 2D semantic segmentation, and two geometry-related tasks, namely dense depth, surface normal estimation as well as edge estimation showing their benefit on indoor and outdoor datasets. We propose a novel multi-task learning architecture that exploits pair-wise cross-task exchange through correlation-guided attention and self-attention to enhance the average representation learning for all tasks. We conduct extensive experiments considering three multi-task setups, showing the benefit of our proposal in comparison to competitive baselines in both synthetic and real benchmarks. We also extend our method to the novel multi-task unsupervised domain adaptation setting. Our code is available at https://github.com/cv-rits/DenseMTL. . . BibTeX . @inproceedings{lopes2023densemtl, title={Cross-task Attention Mechanism for Dense Multi-task Learning}, author={Lopes, Ivan and Vu, Tuan-Hung and De Charette, Raoul}, booktitle={Proceedings of IEEE/CVF Winter Conference on Applications of Computer Vision}, year={2023} } . .",
          "url": "https://valeoai.github.io/blog/publications/2022_densemtl/",
          "relUrl": "/publications/2022_densemtl/",
          "date": ""
      }
      
  

  
      ,"page13": {
          "title": "Diverse Probabilistic Trajectory Forecasting with Admissibility Constraints",
          "content": "Diverse Probabilistic Trajectory Forecasting with Admissibility Constraints . Laura Calem &nbsp;&nbsp; Hedi Ben-Younes &nbsp;&nbsp; Patrick Pérez &nbsp;&nbsp; Nicolas Thome . ICPR 2022 . . . Abstract . Predicting multiple trajectories for road users is important for driving automation systems: ego-vehicle motion planning indeed requires a clear view of the possible motions of the surrounding agents. However, the generative models used for multiple-trajectory forecasting suffer from a lack of diversity in their proposals. To avoid this form of collapse, we propose a novel method for structured prediction of diverse trajectories. To this end, we complement an underlying pretrained generative model with a diversity component, based on a determinantal point process (DPP). We balance and structure this diversity with the inclusion of knowledge-based quality constraints, independent from the underlying generative model. We combine these two novel components with a gating operation, ensuring that the predictions are both diverse and within the drivable area. We demonstrate on the nuScenes driving dataset the relevance of our compound approach, which yields significant improvements in the diversity and the quality of the generated trajectories. . . BibTeX . @inproceedings{calem22diva, title={Diverse Probabilistic Trajectory Forecasting with Admissibility Constraints}, author={Laura Calem and Hedi Ben{-}Younes and Patrick P{ &#39;{e}}rez and Nicolas Thome},, booktitle={International Conference on Pattern Recognition (ICPR)} year={2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/diva/",
          "relUrl": "/publications/diva/",
          "date": ""
      }
      
  

  
      ,"page14": {
          "title": "Solving Disjunctive Temporal Networks with Uncertainty under Restricted Time-Based Controllability using Tree Search and Graph Neural Networks",
          "content": "Solving Disjunctive Temporal Networks with Uncertainty under Restricted Time-Based Controllability using Tree Search and Graph Neural Networks . Kevin Osanlou&nbsp;&nbsp; Jeremy Frank&nbsp;&nbsp; J. Benton&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Christophe Guettier&nbsp;&nbsp; Tristan Cazenave&nbsp;&nbsp; Eric Jacopin . AAAI 2022 . Paper&nbsp;&nbsp; . . Abstract . Planning under uncertainty is an area of interest in artificial intelligence. We present a novel approach based on tree search and graph machine learning for the scheduling problem known as Disjunctive Temporal Networks with Uncertainty (DTNU). Dynamic Controllability (DC) of DTNUs seeks a reactive scheduling strategy to satisfy temporal constraints in response to uncontrollable action durations. We introduce new semantics for reactive scheduling: Time-based Dynamic Controllability (TDC) and a restricted subset of TDC, R-TDC. We design a tree search algorithm to determine whether or not a DTNU is R-TDC. Moreover, we leverage a graph neural network as a heuristic for tree search guidance. Finally, we conduct experiments on a known benchmark on which we show R-TDC to retain significant completeness with regard to DC, while being faster to prove. This results in the tree search processing fifty percent more DTNU problems in R-TDC than the state-of-the-art DC solver does in DC with the same time budget. We also observe that graph neural network search guidance leads to substantial performance gains on benchmarks of more complex DTNUs, with up to eleven times more problems solved than the baseline tree search. . . BibTeX . @inproceedings{osanlou2022solving, title={Solving Disjunctive Temporal Networks with Uncertainty under Restricted Time-Based Controllability using Tree Search and Graph Neural Networks}, author={Osanlou, Kevin and Frank, Jeremy and Bursuc, Andrei and Cazenave, Tristan and Jacopin, Eric and Guettier, Christophe and Benton, J}, booktitle={AAAI}, year={2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/dtnu/",
          "relUrl": "/publications/dtnu/",
          "date": ""
      }
      
  

  
      ,"page15": {
          "title": "LaRa: Latents and Rays for Multi-Camera Bird's-Eye-View Semantic Segmentation",
          "content": "LaRa: Latents and Rays for Multi-Camera Bird&#39;s-Eye-View Semantic Segmentation . Florent Bartoccioni &nbsp;&nbsp; Éloi Zablocki &nbsp;&nbsp; Andrei Bursuc &nbsp;&nbsp; Patrick Pérez &nbsp;&nbsp; Matthieu Cord &nbsp;&nbsp; Karteek Alahari . CoRL 2022 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Recent works in autonomous driving have widely adopted the bird&#39;s-eye-view (BEV) semantic map as an intermediate representation of the world. Online prediction of these BEV maps involves non-trivial operations such as multi-camera data extraction as well as fusion and projection into a common top-view grid. This is usually done with error-prone geometric operations (e.g., homography or back-projection from monocular depth estimation) or expensive direct dense mapping between image pixels and pixels in BEV (e.g., with MLP or attention). In this work, we present &#39;LaRa&#39;, an efficient encoder-decoder, transformer-based model for vehicle semantic segmentation from multiple cameras. Our approach uses a system of cross-attention to aggregate information over multiple sensors into a compact, yet rich, collection of latent representations. These latent representations, after being processed by a series of self-attention blocks, are then reprojected with a second cross-attention in the BEV space. We demonstrate that our model outperforms on nuScenes the best previous works using transformers. . . BibTeX . @inproceedings{lara2022, author = {Florent Bartoccioni and { &#39;{E}}loi Zablocki and Andrei Bursuc and Patrick P{ &#39;{e}}rez and Matthieu Cord and Karteek Alahari}, title = {LaRa: Latents and Rays for Multi-Camera Bird&#39;s-Eye-View Semantic Segmentation}, booktitle = {CoRL}, year = {2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/lara/",
          "relUrl": "/publications/lara/",
          "date": ""
      }
      
  

  
      ,"page16": {
          "title": "Latent Discriminant deterministic Uncertainty",
          "content": "Latent Discriminant deterministic Uncertainty . Gianni Franchi&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Emanuel Aldea&nbsp;&nbsp; Severine Dubuisson&nbsp;&nbsp; David Filliat . ECCV 2022 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Predictive uncertainty estimation is essential for deploying Deep Neural Networks in real-world autonomous systems. However, most successful approaches are computationally intensive. In this work, we attempt to address these challenges in the context of autonomous driving perception tasks. Recently proposed Deterministic Uncertainty Methods (DUM) can only partially meet such requirements as their scalability to complex computer vision tasks is not obvious. In this work we advance a scalable and effective DUM for high-resolution semantic segmentation, that relaxes the Lipschitz constraint typically hindering practicality of such architectures. We learn a discriminant latent space by leveraging a distinction maximization layer over an arbitrarily-sized set of trainable prototypes. Our approach achieves competitive results over Deep Ensembles, the state-of-the-art for uncertainty prediction, on image classification, segmentation and monocular depth estimation tasks. . . BibTeX . @inproceedings{franchi2022latent, title={Latent Discriminant deterministic Uncertainty}, author={Franchi, Gianni and Yu, Xuanlong and Bursuc, Andrei and Aldea, Emanuel and Dubuisson, Severine and Filliat, David}, booktitle={ECCV}, year={2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/ldu/",
          "relUrl": "/publications/ldu/",
          "date": ""
      }
      
  

  
      ,"page17": {
          "title": "LiDARTouch: Monocular metric depth estimation with a few-beam LiDAR",
          "content": "LiDARTouch: Monocular metric depth estimation with a few-beam LiDAR . Florent Bartoccioni &nbsp;&nbsp; Éloi Zablocki &nbsp;&nbsp; Patrick Pérez &nbsp;&nbsp; Matthieu Cord &nbsp;&nbsp; Karteek Alahari . CVIU 2022 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Vision-based depth estimation is a key feature in autonomous systems, which often relies on a single camera or several independent ones. In such a monocular setup, dense depth is obtained with either additional input from one or several expensive LiDARs, e.g., with 64 beams, or camera-only methods, which suffer from scale-ambiguity and infinite-depth problems. In this paper, we propose a new alternative of densely estimating metric depth by combining a monocular camera with a light-weight LiDAR, e.g., with 4 beams, typical of today&#39;s automotive-grade mass-produced laser scanners. Inspired by recent self-supervised methods, we introduce a novel framework, called LiDARTouch, to estimate dense depth maps from monocular images with the help of touches of LiDAR, i.e., without the need for dense ground-truth depth. In our setup, the minimal LiDAR input contributes on three different levels: as an additional model&#39;s input, in a self-supervised LiDAR reconstruction objective function, and to estimate changes of pose (a key component of self-supervised depth estimation architectures). Our LiDARTouch framework achieves new state of the art in self-supervised depth estimation on the KITTI dataset, thus supporting our choices of integrating the very sparse LiDAR signal with other visual features. Moreover, we show that the use of a few-beam LiDAR alleviates scale ambiguity and infinite-depth issues that camera-only methods suffer from. We also demonstrate that methods from the fully-supervised depth-completion literature can be adapted to a self-supervised regime with a minimal LiDAR signal. . . BibTeX . @article{bartoccioni2022lidartouch, author = {Florent Bartoccioni and { &#39;{E}}loi Zablocki and Patrick P{ &#39;{e}}rez and Matthieu Cord and Karteek Alahari}, title = {LiDARTouch: Monocular metric depth estimation with a few-beam LiDAR}, journal = {Comput. Vis. Image Underst.}, year = {2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/lidartouch/",
          "relUrl": "/publications/lidartouch/",
          "date": ""
      }
      
  

  
      ,"page18": {
          "title": "Multi-Head Distillation for Continual Unsupervised Domain Adaptation in Semantic Segmentation",
          "content": "Multi-Head Distillation for Continual Unsupervised Domain Adaptation in Semantic Segmentation . Antoine Saporta&nbsp;&nbsp; Arthur Douillard&nbsp;&nbsp; Tuan-Hung Vu&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Matthieu Cord . CVPRW 2022 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Unsupervised Domain Adaptation (UDA) is a transfer learning task which aims at training on an unlabeled target domain by leveraging a labeled source domain. Beyond the traditional scope of UDA with a single source domain and a single target domain, real-world perception systems face a variety of scenarios to handle, from varying lighting conditions to many cities around the world. In this context, UDAs with several domains increase the challenges with the addition of distribution shifts within the different target domains. This work focuses on a novel framework for learning UDA, continuous UDA, in which models operate on multiple target domains discovered sequentially, without access to previous target domains. We propose MuHDi, for Multi-Head Distillation, a method that solves the catastrophic forgetting problem, inherent in continual learning tasks. MuHDi performs distillation at multiple levels from the previous model as well as an auxiliary target-specialist segmentation head. We report both extensive ablation and experiments on challenging multi-target UDA semantic segmentation benchmarks to validate the proposed learning scheme and architecture. . . BibTeX . @inproceedings{saporta2022muhdi, title={Multi-Head Distillation for Continual Unsupervised Domain Adaptation in Semantic Segmentation}, author={Saporta, Antoine and Douillard, Arthur and Vu, Tuan-Hung and P{ &#39;e}rez, Patrick and Cord, Matthieu}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshop}, year={2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/muhdi/",
          "relUrl": "/publications/muhdi/",
          "date": ""
      }
      
  

  
      ,"page19": {
          "title": "POCO: Point Convolution for Surface Reconstruction",
          "content": "POCO: Point Convolution for Surface Reconstruction . Alexandre Boulch&nbsp;&nbsp;Renaud Marlet . CVPR 2022 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . . Abstract . Implicit neural networks have been successfully used for surface reconstruction from point clouds. However, many of them face scalability issues as they encode the isosurface function of a whole object or scene into a single latent vector. To overcome this limitation, a few approaches infer latent vectors on a coarse regular 3D grid or on 3D patches, and interpolate them to answer occupancy queries. In doing so, they loose the direct connection with the input points sampled on the surface of objects, and they attach information uniformly in space rather than where it matters the most, i.e., near the surface. Besides, relying on fixed patch sizes may require discretization tuning. To address these issues, we propose to use point cloud convolutions and compute latent vectors at each input point. We then perform a learning-based interpolation on nearest neighbors using inferred weights. Experiments on both object and scene datasets show that our approach significantly outperforms other methods on most classical metrics, producing finer details and better reconstructing thinner volumes. &lt;/a&gt; . . . BibTeX . @inproceedings{boulch2022poco, title={POCO: Point Convolution for Surface Reconstruction}, author={Boulch, Alexandre and Marlet, Renaud}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, year={2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/poco/",
          "relUrl": "/publications/poco/",
          "date": ""
      }
      
  

  
      ,"page20": {
          "title": "Raw High-Definition Radar for Multi-Task Learning",
          "content": "Raw High-Definition Radar for Multi-Task Learning . Julien Rebut&nbsp;&nbsp;Arthur Ouaknine&nbsp;&nbsp;Waqas Walik&nbsp;&nbsp;Patrick Pérez . CVPR 2022 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . . Abstract . With their robustness to adverse weather conditions and ability to measure speeds, radar sensors have been part of the automotive landscape for more than two decades. Recent progress toward High Definition (HD) Imaging radar has driven the angular resolution below the degree, thus approaching laser scanning performance. However, the amount of data a HD radar delivers and the computational cost to estimate the angular positions remain a challenge. In this paper, we propose a novel HD radar sensing model, FFT-RadNet, that eliminates the overhead of computing the range-azimuth-Doppler 3D tensor, learning instead to recover angles from a range-Doppler spectrum. FFT-RadNet is trained both to detect vehicles and to segment free driving space. On both tasks, it competes with the most recent radar-based models while requiring less compute and memory. Also, we collected and annotated 2-hour worth of raw data from synchronized automotive-grade sensors (camera, laser, HD radar) in various environments (city street, highway, countryside road). This unique dataset, nick-named RADIal for &quot;Radar, Lidar et al.&quot;, is available at this https URL. . . . BibTeX . @inproceedings{rebut2022radial, title={Raw High-Definition Radar for Multi-Task Learning}, author={Rebut, Julien and Ouaknine, Arthur and Malik, Waqas and P{ &#39;e}rez, Patrick}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, year={2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/radial/",
          "relUrl": "/publications/radial/",
          "date": ""
      }
      
  

  
      ,"page21": {
          "title": "Spherical perspective on learning with normalization layers",
          "content": "Spherical perspective on learning with normalization layers . Simon Roburin &nbsp;&nbsp; Yann de Mont-Marin &nbsp;&nbsp; Andrei Bursuc &nbsp;&nbsp; Renaud Marlet &nbsp;&nbsp; Patrick Pérez &nbsp;&nbsp; Mathieu Aubry . Neurocomputing 2022 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Normalization Layers (NLs) are widely used in modern deep-learning architectures. Despite their apparent simplicity, their effect on optimization is not yet fully understood. This paper introduces a spherical framework to study the optimization of neural networks with NLs from a geometric perspective. Concretely, the radial invariance of groups of parameters, such as filters for convolutional neural networks, allows to translate the optimization steps on the L2 unit hypersphere. This formulation and the associated geometric interpretation shed new light on the training dynamics. Firstly, the first effective learning rate expression of Adam is derived. Then the demonstration that, in the presence of NLs, performing Stochastic Gradient Descent (SGD) alone is actually equivalent to a variant of Adam constrained to the unit hypersphere, stems from the framework. Finally, this analysis outlines phenomena that previous variants of Adam act on and their importance in the optimization process are experimentally validated. . . BibTeX . @article{spherical-adam-2022, author = {Simon Roburin and Yann de Mont{-}Marin and Andrei Bursuc and Renaud Marlet and Patrick P{ &#39;{e}}rez and Mathieu Aubry}, title = {Spherical perspective on learning with normalization layers}, journal = {Neurocomputing}, volume = {487}, pages = {66--74}, year = {2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/spherical-adam/",
          "relUrl": "/publications/spherical-adam/",
          "date": ""
      }
      
  

  
      ,"page22": {
          "title": "STEEX: Steering Counterfactual Explanations with Semantics",
          "content": "STEEX: Steering Counterfactual Explanations with Semantics . Paul Jacob &nbsp;&nbsp; Éloi Zablocki &nbsp;&nbsp; Hédi Ben-Younes &nbsp;&nbsp; Mickaël Chen &nbsp;&nbsp; Patrick Pérez &nbsp;&nbsp; Matthieu Cord . ECCV 2022 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . As deep learning models are increasingly used in safety-critical applications, explainability and trustworthiness become major concerns. For simple images, such as low-resolution face portraits, synthesizing visual counterfactual explanations has recently been proposed as a way to uncover the decision mechanisms of a trained classification model. In this work, we address the problem of producing counterfactual explanations for high-quality images and complex scenes. Leveraging recent semantic-to-image models, we propose a new generative counterfactual explanation framework that produces plausible and sparse modifications which preserve the overall scene structure. Furthermore, we introduce the concept of &quot;region-targeted counterfactual explanations&quot;, and a corresponding framework, where users can guide the generation of counterfactuals by specifying a set of semantic regions of the query image the explanation must be about. Extensive experiments are conducted on challenging datasets including high-quality portraits (CelebAMask-HQ) and driving scenes (BDD100k). . . Video . . . . BibTeX . @inproceedings{steex2022, author = {Paul Jacob and { &#39;{E}}loi Zablocki and Hedi Ben{-}Younes and Micka{ &quot;{e}}l Chen and Patrick P{ &#39;{e}}rez and Matthieu Cord}, title = {STEEX: Steering Counterfactual Explanations with Semantics}, booktitle = {ECCV}, publisher = {Springer}, year = {2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/steex/",
          "relUrl": "/publications/steex/",
          "date": ""
      }
      
  

  
      ,"page23": {
          "title": "Active Learning Strategies for Weakly-Supervised Object Detection",
          "content": "Active Learning Strategies for Weakly-Supervised Object Detection . Huy V. Vo &nbsp;&nbsp; Oriane Siméoni &nbsp;&nbsp; Spyros Gidaris &nbsp;&nbsp; Andrei Bursuc &nbsp;&nbsp; Patrick Pérez &nbsp;&nbsp; Jean Ponce . ECCV 2022 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . . Abstract . Object detectors trained with weak annotations are affordable alternatives to fully-supervised counterparts. However, there is still a significant performance gap between them. We propose to narrow this gap by fine-tuning a base pre-trained weakly-supervised detector with a few fully-annotated samples automatically selected from the training set using ``box-in-box&#39;&#39; (BiB), a novel active learning strategy designed specifically to address the well-documented failure modes of weakly-supervised detectors. Experiments on the VOC07 and COCO benchmarks show that BiB outperforms other active learning techniques and significantly improves the base weakly-supervised detector&#39;s performance with only a few fully-annotated images per class. BiB reaches 97% of the performance of fully-supervised Fast RCNN with only 10% of fully-annotated images on VOC07. On COCO, using on average 10 fully-annotated images per class, or equivalently 1% of the training set, BiB also reduces the performance gap (in AP) between the weakly-supervised detector and the fully-supervised Fast RCNN by over 70%, showing a good trade-off between performance and data efficiency. . . BibTeX . @inproceedings{vo2022active, title={Active Learning Strategies for Weakly-Supervised Object Detection}, author={Vo, Huy V and Sim{ &#39;e}oni, Oriane and Gidaris, Spyros and Bursuc, Andrei and P{ &#39;e}rez, Patrick and Ponce, Jean}, booktitle={ECCV}, year={2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/bib/",
          "relUrl": "/publications/bib/",
          "date": ""
      }
      
  

  
      ,"page24": {
          "title": "Explainability of deep vision-based autonomous driving systems: Review and challenges",
          "content": "Explainability of deep vision-based autonomous driving systems: Review and challenges . Éloi Zablocki &nbsp;&nbsp; Hédi Ben-Younes &nbsp;&nbsp; Patrick Pérez &nbsp;&nbsp; Matthieu Cord . IJCV 2022 . Paper&nbsp;&nbsp; Blog &nbsp;&nbsp; . . Abstract . This survey reviews explainability methods for vision-based self-driving systems trained with behavior cloning. The concept of explainability has several facets and the need for explainability is strong in driving, a safety-critical application. Gathering contributions from several research fields, namely computer vision, deep learning, autonomous driving, explainable AI (X-AI), this survey tackles several points. First, it discusses definitions, context, and motivation for gaining more interpretability and explainability from self-driving systems, as well as the challenges that are specific to this application. Second, methods providing explanations to a black-box self-driving system in a post-hoc fashion are comprehensively organized and detailed. Third, approaches from the literature that aim at building more interpretable self-driving systems by design are presented and discussed in detail. Finally, remaining open-challenges and potential future research directions are identified and examined. . . BibTeX . @article{xai-driving-survey-2022, author = {Eloi Zablocki and Hedi Ben{-}Younes and Patrick P{ &#39;{e}}rez and Matthieu Cord}, title = {Explainability of deep vision-based autonomous driving systems: Review and challenges}, journal = {IJCV}, year = {2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/xai-driving-survey/",
          "relUrl": "/publications/xai-driving-survey/",
          "date": ""
      }
      
  

  
      ,"page25": {
          "title": "Cross-modal Learning for Domain Adaptation in 3D Semantic Segmentation",
          "content": "Cross-modal Learning for Domain Adaptation in 3D Semantic Segmentation . Maximilian Jaritz&nbsp;&nbsp; Tuan-Hung Vu&nbsp;&nbsp; Raoul de Charette&nbsp;&nbsp; Émilie Wirbel&nbsp;&nbsp; Patrick Pérez . T-PAMI 2022 . Paper&nbsp;&nbsp; . . Abstract . Domain adaptation is an important task to enable learning when labels are scarce. While most works focus only on the image modality, there are many important multi-modal datasets. In order to leverage multi-modality for domain adaptation, we propose cross-modal learning, where we enforce consistency between the predictions of two modalities via mutual mimicking. We constrain our network to make correct predictions on labeled data and consistent predictions across modalities on unlabeled target-domain data. Experiments in unsupervised and semi-supervised domain adaptation settings prove the effectiveness of this novel domain adaptation strategy. Specifically, we evaluate on the task of 3D semantic segmentation from either the 2D image, the 3D point cloud or from both. We leverage recent driving datasets to produce a wide variety of domain adaptation scenarios including changes in scene layout, lighting, sensor setup and weather, as well as the synthetic-to-real setup. Our method significantly improves over previous uni-modal adaptation baselines on all adaption scenarios. Code will be made available upon publication. . . BibTeX . @article{jaritz2022xmossda, title={Cross-modal Learning for Domain Adaptation in 3D Semantic Segmentation}, author={Jaritz, Maximilian and Vu, Tuan-Hung and Charette, Raoul de and Wirbel, Emilie and P{ &#39;e}rez, Patrick}, journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, year={2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/xmossda/",
          "relUrl": "/publications/xmossda/",
          "date": ""
      }
      
  

  
      ,"page26": {
          "title": "ALSO: Automotive Lidar Self-supervision by Occupancy estimation",
          "content": "ALSO: Automotive Lidar Self-supervision by Occupancy estimation . Alexandre Boulch&nbsp;&nbsp; Corentin Sautier&nbsp;&nbsp; Björn Michele&nbsp;&nbsp; Gilles Puy &nbsp;&nbsp; Renaud Marlet . CVPR 2023 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . We propose a new self-supervised method for pre-training the backbone of deep perception models operating on point clouds. The core idea is to train the model on a pretext task which is the reconstruction of the surface on which the 3D points are sampled, and to use the underlying latent vectors as input to the perception head. The intuition is that if the network is able to reconstruct the scene surface, given only sparse input points, then it probably also captures some fragments of semantic information, that can be used to boost an actual perception task. This principle has a very simple formulation, which makes it both easy to implement and widely applicable to a large range of 3D sensors and deep networks performing semantic segmentation or object detection. In fact, it supports a single-stream pipeline, as opposed to most contrastive learning approaches, allowing training on limited resources. We conducted extensive experiments on various autonomous driving datasets, involving very different kinds of lidars, for both semantic segmentation and object detection. The results show the effectiveness of our method to learn useful representations without any annotation, compared to existing approaches.&lt;/a&gt; . . . Video . . . . BibTeX . @article{boulch2023also, title={ALSO: Automotive Lidar Self-supervision by Occupancy estimation}, author={Boulch, Alexandre and Sautier, Corentin and Michel, Bj{ &quot;o}rn and Puy, Gilles and Marlet, Renaud}, booktitle={CVPR}, year={2023} } . .",
          "url": "https://valeoai.github.io/blog/publications/also/",
          "relUrl": "/publications/also/",
          "date": ""
      }
      
  

  
      ,"page27": {
          "title": "DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion",
          "content": "DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion . Cédric Rommel &nbsp;&nbsp; Eduardo Valle &nbsp;&nbsp; Mickaël Chen &nbsp;&nbsp; Souhaiel Khalfaoui &nbsp;&nbsp; Renaud Marlet &nbsp;&nbsp; Matthieu Cord &nbsp;&nbsp; Patrick Pérez . ICCV 2023 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . We present an innovative approach to 3D Human Pose Estimation (3D-HPE) by integrating cutting-edge diffusion models, which have revolutionized diverse fields, but are relatively unexplored in 3D-HPE. We show that diffusion models enhance the accuracy, robustness, and coherence of human pose estimations. We introduce DiffHPE, a novel strategy for harnessing diffusion models in 3D-HPE, and demonstrate its ability to refine standard supervised 3D-HPE. We also show how diffusion models lead to more robust estimations in the face of occlusions, and improve the time-coherence and the sagittal symmetry of predictions. Using the Human3.6M dataset, we illustrate the effectiveness of our approach and its superiority over existing models, even under adverse situations where the occlusion patterns in training do not match those in inference. Our findings indicate that while standalone diffusion models provide commendable performance, their accuracy is even better in combination with supervised models, opening exciting new avenues for 3D-HPE research. . . . BibTeX . @INPROCEEDINGS{rommel2023diffhpe, title={DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion}, author={Rommel, C{ &#39;e}dric and Valle, Eduardo and Chen, Micka{ &quot;e}l and Khalfaoui, Souhaiel and Marlet, Renaud and Cord, Matthieu and P{ &#39;e}rez, Patrick}, booktitle={International Conference on Computer Vision Workshops (ICCVW)}, year = {2023} } . .",
          "url": "https://valeoai.github.io/blog/publications/diffhpe/",
          "relUrl": "/publications/diffhpe/",
          "date": ""
      }
      
  

  
      ,"page28": {
          "title": "Towards Motion Forecasting with Real-World Perception Inputs: Are End-to-End Approaches Competitive?",
          "content": "Towards Motion Forecasting with Real-World Perception Inputs: Are End-to-End Approaches Competitive? . Yihong Xu &nbsp;&nbsp; Loïck Chambon &nbsp;&nbsp; Éloi Zablocki &nbsp;&nbsp; Mickaël Chen &nbsp;&nbsp; Alexandre Alahi &nbsp;&nbsp; Matthieu Cord &nbsp;&nbsp; Patrick Pérez . ICCV Workshop ROAD++ 2023 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Motion forecasting is crucial in enabling autonomous vehicles to anticipate the future trajectories of surrounding agents. To do so, it requires solving mapping, detection, tracking, and then forecasting problems, in a multi-step pipeline. In this complex system, advances in conventional forecasting methods have been made using curated data, i.e., with the assumption of perfect maps, detection, and tracking. This paradigm, however, ignores any errors from upstream modules. Meanwhile, an emerging end-to-end paradigm, that tightly integrates the perception and forecasting architectures into joint training, promises to solve this issue. So far, however, the evaluation protocols between the two methods were incompatible and their comparison was not possible. In fact, and perhaps surprisingly, conventional forecasting methods are usually not trained nor tested in real-world pipelines (e.g., with upstream detection, tracking, and mapping modules). In this work, we aim to bring forecasting models closer to real-world deployment. First, we propose a unified evaluation pipeline for forecasting methods with real-world perception inputs, allowing us to compare the performance of conventional and end-to-end methods for the first time. Second, our in-depth study uncovers a substantial performance gap when transitioning from curated to perception-based data. In particular, we show that this gap (1) stems not only from differences in precision but also from the nature of imperfect inputs provided by perception modules, and that (2) is not trivially reduced by simply finetuning on perception outputs. Based on extensive experiments, we provide recommendations for critical areas that require improvement and guidance towards more robust motion forecasting in the real world. We will release an evaluation library to benchmark models under standardized and practical conditions. . . . BibTeX . @article{xu2023realworldforecasting, title = {Towards Motion Forecasting with Real-World Perception Inputs: Are End-to-End Approaches Competitive?}, author = {Yihong Xu and Lo{ &quot;{ i}}ck Chambon and { &#39;{E}}loi Zablocki and Micka{ &quot;{e}}l Chen and Alexandre Alahi and Matthieu Cord and Patrick P{ &#39;{e}}rez}, journal = {CoRR}, volume = {abs/2306/09281}, year = {2023} } . .",
          "url": "https://valeoai.github.io/blog/publications/real-world-forecasting/",
          "relUrl": "/publications/real-world-forecasting/",
          "date": ""
      }
      
  

  
      ,"page29": {
          "title": "Unsupervised Object Localization: Observing the Background to Discover Objects",
          "content": "Unsupervised Object Localization: Observing the Background to Discover Objects . Oriane Siméoni &nbsp;&nbsp; Chloé Sekkat &nbsp;&nbsp; Gilles Puy &nbsp;&nbsp; Antonin Vobecky &nbsp;&nbsp; Éloi Zablocki &nbsp;&nbsp; Patrick Pérez . CVPR 2023 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Recent advances in self-supervised visual representation learning have paved the way for unsupervised methods tackling tasks such as object discovery and instance segmentation. However, discovering objects in an image with no supervision is a very hard task; what are the desired objects, when to separate them into parts, how many are there, and of what classes? The answers to these questions depend on the tasks and datasets of evaluation. In this work, we take a different approach and propose to look for the background instead. This way, the salient objects emerge as a by-product without any strong assumption on what an object should be. We propose FOUND, a simple model made of a single conv1×1 initialized with coarse background masks extracted from self-supervised patch-based representations. After fast training and refining these seed masks, the model reaches state-of-the-art results on unsupervised saliency detection and object discovery benchmarks. Moreover, we show that our approach yields good results in the unsupervised semantic segmentation retrieval task. . . Video . . . . BibTeX . @inproceedings{simeoni2023found, title = {Unsupervised Object Localization: Observing the Background to Discover Objects}, author = {Oriane Sim &#39;eoni and Chlo &#39;e Sekkat and Gilles Puy and Antonin Vobecky and { &#39;{E}}loi Zablocki and Patrick P &#39;erez}, booktitle = {CVPR}, year = {2023} } . .",
          "url": "https://valeoai.github.io/blog/publications/found/",
          "relUrl": "/publications/found/",
          "date": ""
      }
      
  

  
      ,"page30": {
          "title": "OCTET: Object-aware Counterfactual Explanations",
          "content": "OCTET: Object-aware Counterfactual Explanations . Mehdi Zemni &nbsp;&nbsp; Mickaël Chen &nbsp;&nbsp; Éloi Zablocki &nbsp;&nbsp; Hédi Ben-Younes &nbsp;&nbsp; Patrick Pérez &nbsp;&nbsp; Matthieu Cord . CVPR 2023 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Nowadays, deep vision models are being widely deployed in safety-critical applications, e.g., autonomous driving, and explainability of such models is becoming a pressing concern. Among explanation methods, counterfactual explanations aim to find minimal and interpretable changes to the input image that would also change the output of the model to be explained. Such explanations point end-users at the main factors that impact the decision of the model. However, previous methods struggle to explain decision models trained on images with many objects, e.g., urban scenes, which are more difficult to work with but also arguably more critical to explain. In this work, we propose to tackle this issue with an object-centric framework for counterfactual explanation generation. Our method, inspired by recent generative modeling works, encodes the query image into a latent space that is structured in a way to ease object-level manipulations. Doing so, it provides the end-user with control over which search directions (e.g., spatial displacement of objects, style modification, etc.) are to be explored during the counterfactual generation. We conduct a set of experiments on counterfactual explanation benchmarks for driving scenes, and we show that our method can be adapted beyond classification, e.g., to explain semantic segmentation models. To complete our analysis, we design and run a user study that measures the usefulness of counterfactual explanations in understanding a decision model. . . Video . . . . BibTeX . @inproceedings{zemni2023octet, title = {OCTET: Object-aware Counterfactual Explanations}, author = {Mehdi Zemni and Micka{ &quot;{e}}l Chen and { &#39;{E}}loi Zablocki and Hedi Ben{-}Younes and Patrick P{ &#39;{e}}rez and Matthieu Cord}, booktitle = {CVPR}, year = {2023} } . .",
          "url": "https://valeoai.github.io/blog/publications/octet/",
          "relUrl": "/publications/octet/",
          "date": ""
      }
      
  

  
      ,"page31": {
          "title": "Packed Ensembles for efficient uncertainty estimation",
          "content": "Packed Ensembles for efficient uncertainty estimation . Olivier Laurent&nbsp;&nbsp; Adrien Lafage&nbsp;&nbsp; Enzo Tartaglione&nbsp;&nbsp; Geoffrey Daniel&nbsp;&nbsp; Jean-Marc Martinez Andrei Bursuc&nbsp;&nbsp; &nbsp;&nbsp; Gianni Franchi . ECCV 2023 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Deep Ensembles (DE) are a prominent approach for achieving excellent performance on key metrics such as accuracy, calibration, uncertainty estimation, and out-of-distribution detection. However, hardware limitations of real-world systems constrain to smaller ensembles and lower-capacity networks, significantly deteriorating their performance and properties. We introduce Packed-Ensembles (PE), a strategy to design and train lightweight structured ensembles by carefully modulating the dimension of their encoding space. We leverage grouped convolutions to parallelize the ensemble into a single shared backbone and forward pass to improve training and inference speeds. PE is designed to operate within the memory limits of a standard neural network. Our extensive research indicates that PE accurately preserves the properties of DE, such as diversity, and performs equally well in terms of accuracy, calibration, out-of-distribution detection, and robustness to distribution shift.. . . BibTeX . @inproceedings{laurent2022packed, title={Packed-Ensembles for Efficient Uncertainty Estimation}, author={Laurent, Olivier and Lafage, Adrien and Tartaglione, Enzo and Daniel, Geoffrey and Martinez, Jean-Marc and Bursuc, Andrei and Franchi, Gianni}, booktitle={ICLR}, year={2023} } . .",
          "url": "https://valeoai.github.io/blog/publications/packed-ensembles/",
          "relUrl": "/publications/packed-ensembles/",
          "date": ""
      }
      
  

  
      ,"page32": {
          "title": "PØDA: Prompt-driven Zero-shot Domain Adaptation",
          "content": "PØDA: Prompt-driven Zero-shot Domain Adaptation . Mohammad Fahes &nbsp;&nbsp; Tuan-Hung Vu &nbsp;&nbsp; Andrei Bursuc &nbsp;&nbsp; Patrick Pérez &nbsp;&nbsp; Raoul de Charette . ICCV 2023 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Domain adaptation has been vastly investigated in computer vision but still requires access to target images at train time, which might be intractable in some uncommon conditions. In this paper, we propose the task of ‘Prompt-driven Zero-shot Domain Adaptation’, where we adapt a model trained on a source domain using only a general description in natural language of the target domain, i.e., a prompt. First, we leverage a pretrained contrastive vision-language model (CLIP) to optimize affine transformations of source features, steering them towards the target text embedding while preserving their content and semantics. To achieve this, we propose Prompt-driven Instance Normalization (PIN). Second, we show that these prompt-driven augmentations can be used to perform zero-shot domain adaptation for semantic segmentation. Experiments demonstrate that our method significantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand, even surpassing one-shot unsupervised domain adaptation. A similar boost is observed on object detection and image classification. . . BibTeX . @inproceedings{fahes2023poda, title={P{ O}DA: Prompt-driven Zero-shot Domain Adaptation}, author={Fahes, Mohammad and Vu, Tuan-Hung and Bursuc, Andrei and P{ &#39;e}rez, Patrick and de Charette, Raoul}, booktitle={ICCV}, year={2023} } . .",
          "url": "https://valeoai.github.io/blog/publications/poda/",
          "relUrl": "/publications/poda/",
          "date": ""
      }
      
  

  
      ,"page33": {
          "title": "RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving",
          "content": "RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving . Angelika Ando &nbsp;&nbsp; Spyros Gidaris &nbsp;&nbsp; Andrei Bursuc &nbsp;&nbsp; Gilles Puy &nbsp;&nbsp; Alexandre Boulch&nbsp;&nbsp; Renaud Marlet . CVPR 2023 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Casting semantic segmentation of outdoor LiDAR point clouds as a 2D problem, e.g., via range projection, is an effective and popular approach. These projection-based methods usually benefit from fast computations and, when combined with techniques which use other point cloud representations, achieve state-of-the-art results. Today, projection-based methods leverage 2D CNNs but recent advances in computer vision show that vision transformers (ViTs) have achieved state-of-the-art results in many image-based benchmarks. In this work, we question if projection-based methods for 3D semantic segmentation can benefit from these latest improvements on ViTs. We answer positively but only after combining them with three key ingredients: (a) ViTs are notoriously hard to train and require a lot of training data to learn powerful representations. By preserving the same backbone architecture as for RGB images, we can exploit the knowledge from long training on large image collections that are much cheaper to acquire and annotate than point clouds. We reach our best results with pre-trained ViTs on large image datasets. (b) We compensate ViTs&#39; lack of inductive bias by substituting a tailored convolutional stem for the classical linear embedding layer. (c) We refine pixel-wise predictions with a convolutional decoder and a skip connection from the convolutional stem to combine low-level but fine-grained features of the the convolutional stem with the high-level but coarse predictions of the ViT encoder. With these ingredients, we show that our method, called RangeViT, outperforms existing projection-based methods on nuScenes and SemanticKITTI. . . BibTeX . @inproceedings{ando2023rangevit, title={RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving}, author={Ando, Angelika and Gidaris, Spyros and Bursuc, Andrei and Puy, Gilles and Boulch, Alexandre and Marlet, Renaud}, booktitle={CVPR}, year={2023} } . .",
          "url": "https://valeoai.github.io/blog/publications/rangevit/",
          "relUrl": "/publications/rangevit/",
          "date": ""
      }
      
  

  
      ,"page34": {
          "title": "You Never Get a Second Chance To Make a Good First Impression: Seeding Active Learning for 3D Semantic Segmentation",
          "content": "You Never Get a Second Chance To Make a Good First Impression: Seeding Active Learning for 3D Semantic Segmentation . Nermin Samet &nbsp;&nbsp; Oriane Siméoni&nbsp;&nbsp; Gilles Puy&nbsp;&nbsp; Georgy Ponimatkin&nbsp;&nbsp; Renaud Marlet&nbsp;&nbsp; Vincent Lepetit . ICCV 2023 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . We propose SeedAL, a method to seed active learning for efficient annotation of 3D point clouds for semantic segmentation. Active Learning (AL) iteratively selects relevant data fractions to annotate within a given budget, but requires a first fraction of the dataset (a &#39;seed&#39;) to be already annotated to estimate the benefit of annotating other data fractions. We first show that the choice of the seed can significantly affect the performance of many AL methods. We then propose a method for automatically constructing a seed that will ensure good performance for AL. Assuming that images of the point clouds are available, which is common, our method relies on powerful unsupervised image features to measure the diversity of the point clouds. It selects the point clouds for the seed by optimizing the diversity under an annotation budget, which can be done by solving a linear optimization problem. Our experiments demonstrate the effectiveness of our approach compared to random seeding and existing methods on both the S3DIS and SemanticKitti datasets. Code is available at this https URL. . . BibTeX . @inproceedings{samet2023seedal, title={You Never Get a Second Chance To Make a Good First Impression: Seeding Active Learning for 3D Semantic Segmentation}, author={Samet, Nermin and Sim &#39;eoni, Oriane and Puy, Gilles and Ponimatkin, Georgy and Marlet, Renaud and Lepetit, Vincent}, booktitle={ICCV}, year={2023} } . .",
          "url": "https://valeoai.github.io/blog/publications/seedal/",
          "relUrl": "/publications/seedal/",
          "date": ""
      }
      
  

  
      ,"page35": {
          "title": "Self-supervised learning with rotation-invariant kernels",
          "content": "Self-supervised learning with rotation-invariant kernels . Léon Zheng &nbsp;&nbsp; Gilles Puy &nbsp;&nbsp; Elisa Riccietti &nbsp;&nbsp;Patrick Pérez &nbsp;&nbsp; Rémi Gribonval . ICLR 2023 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . We introduce a regularization loss based on kernel mean embeddings with rotation-invariant kernels on the hypersphere (also known as dot-product kernels) for self-supervised learning of image representations. Besides being fully competitive with the state of the art, our method significantly reduces time and memory complexity for self-supervised training, making it implementable for very large embedding dimensions on existing devices and more easily adjustable than previous methods to settings with limited resources. Our work follows the major paradigm where the model learns to be invariant to some predefined image transformations (cropping, blurring, color jittering, etc.), while avoiding a degenerate solution by regularizing the embedding distribution. Our particular contribution is to propose a loss family promoting the embedding distribution to be close to the uniform distribution on the hypersphere, with respect to the maximum mean discrepancy pseudometric. We demonstrate that this family encompasses several regularizers of former methods, including uniformity-based and information-maximization methods, which are variants of our flexible regularization loss with different kernels. Beyond its practical consequences for state-of-the-art self-supervised learning with limited resources, the proposed generic regularization approach opens perspectives to leverage more widely the literature on kernel methods in order to improve self-supervised learning methods. . . . BibTeX . @inproceedings{ zheng2023selfsupervised, title={Self-supervised learning with rotation-invariant kernels}, author={L{ &#39;e}on Zheng and Gilles Puy and Elisa Riccietti and Patrick Perez and R{ &#39;e}mi Gribonval}, booktitle={The Eleventh International Conference on Learning Representations }, year={2023}, url={https://openreview.net/forum?id=8uu6JStuYm} } . .",
          "url": "https://valeoai.github.io/blog/publications/sfrik/",
          "relUrl": "/publications/sfrik/",
          "date": ""
      }
      
  

  
      ,"page36": {
          "title": "Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation",
          "content": "Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation . Gilles Puy &nbsp;&nbsp; Alexandre Boulch&nbsp;&nbsp; Renaud Marlet . ICCV 2023 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Semantic segmentation of point clouds in autonomous driving datasets requires techniques that can process large numbers of points efficiently. Sparse 3D convolutions have become the de-facto tools to construct deep neural networks for this task: they exploit point cloud sparsity to reduce the memory and computational loads and are at the core of today&#39;s best methods. In this paper, we propose an alternative method that reaches the level of state-of-the-art methods without requiring sparse convolutions. We actually show that such level of performance is achievable by relying on tools a priori unfit for large scale and high-performing 3D perception. In particular, we propose a novel 3D backbone, WaffleIron, made almost exclusively of MLPs and dense 2D convolutions and present how to train it to reach high performance on SemanticKITTI and nuScenes. We believe that WaffleIron is a compelling alternative to backbones using sparse 3D convolutions, especially in frameworks and on hardware where those convolutions are not readily available. . . BibTeX . @inproceedings{puy23waffleiron, title={Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation}, author={Puy, Gilles and Boulch, Alexandre and Marlet, Renaud}, booktitle={ICCV}, year={2023} } . .",
          "url": "https://valeoai.github.io/blog/publications/waffleiron/",
          "relUrl": "/publications/waffleiron/",
          "date": ""
      }
      
  

  
      ,"page37": {
          "title": "BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds",
          "content": "BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds . Corentin Sautier&nbsp;&nbsp; Gilles Puy &nbsp;&nbsp; Alexandre Boulch&nbsp;&nbsp; Renaud Marlet&nbsp;&nbsp; Vincent Lepetit . 3DV 2024 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . We present a surprisingly simple and efficient method for self-supervision of 3D backbone on automotive Lidar point clouds. We design a contrastive loss between features of Lidar scans captured in the same scene. Several such approaches have been proposed in the literature from PointConstrast, which uses a contrast at the level of points, to the state-of-the-art TARL, which uses a contrast at the level of segments, roughly corresponding to objects. While the former enjoys a great simplicity of implementation, it is surpassed by the latter, which however requires a costly pre-processing. In BEVContrast, we define our contrast at the level of 2D cells in the Bird&#39;s Eye View plane. Resulting cell-level representations offer a good trade-off between the point-level representations exploited in PointContrast and segment-level representations exploited in TARL: we retain the simplicity of PointContrast (cell representations are cheap to compute) while surpassing the performance of TARL in downstream semantic segmentation. . . BibTeX . @inproceedings{bevcontrast, title={{BEVContrast}: Self-Supervision in BEV Space for Automotive Lidar Point Clouds}, author={Corentin Sautier and Gilles Puy and Alexandre Boulch and Renaud Marlet and Vincent Lepetit}, booktitle={3DV}, year={2024} } . .",
          "url": "https://valeoai.github.io/blog/publications/bevcontrast/",
          "relUrl": "/publications/bevcontrast/",
          "date": ""
      }
      
  

  
      ,"page38": {
          "title": "CLIP-DIY: CLIP Dense Inference Yields Open-Vocabulary Semantic Segmentation For-Free",
          "content": "CLIP-DIY: CLIP Dense Inference Yields Open-Vocabulary Semantic Segmentation For-Free . Monika Wysoczańska &nbsp;&nbsp;Michaël Ramamonjisoa &nbsp;&nbsp;Tomasz Trzciński &nbsp;&nbsp; Oriane Siméoni . WACV 2024 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . The emergence of CLIP has opened the way for open-world image perception. The zero-shot classification capabilities of the model are impressive but are harder to use for dense tasks such as image segmentation. Several methods have proposed different modifications and learning schemes to produce dense output. Instead, we propose in this work an open-vocabulary semantic segmentation method, dubbed CLIP-DIY, which does not require any additional training or annotations, but instead leverages existing unsupervised object localization approaches. In particular, CLIP-DIY is a multi-scale approach that directly exploits CLIP classification abilities on patches of different sizes and aggregates the decision in a single map. We further guide the segmentation using foreground/background scores obtained using unsupervised object localization methods. With our method, we obtain state-of-the-art zero-shot semantic segmentation results on PASCAL VOC and perform on par with the best methods on COCO. . . . . BibTeX . @inproceedings{wysoczanska2024clipdiy, title = {CLIP-DIY: CLIP Dense Inference Yields Open-Vocabulary Semantic Segmentation For-Free}, author= {Monika Wysoczanska and Micha{ &quot;{e}}l Ramamonjisoa and Tomasz Trzcinski and Oriane Sim{ &#39;{e}}oni}, booktitle = {WACV}, year = {2024} } . .",
          "url": "https://valeoai.github.io/blog/publications/clip-diy/",
          "relUrl": "/publications/clip-diy/",
          "date": ""
      }
      
  

  
      ,"page39": {
          "title": "Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis",
          "content": "Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis . Victor Letzelter&nbsp;&nbsp; Mathieu Fontaine&nbsp;&nbsp; Mickaël Chen &nbsp;&nbsp; Patrick Pérez &nbsp;&nbsp; Slim Essid&nbsp;&nbsp; Gaël Richard . NeurIPS 2023 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; Slides&nbsp;&nbsp; . . Abstract . We introduce Resilient Multiple Choice Learning (rMCL), an extension of the MCL approach for conditional distribution estimation in regression settings where multiple targets may be sampled for each training input. Multiple Choice Learning is a simple framework to tackle multimodal density estimation, using the Winner-Takes-All (WTA) loss for a set of hypotheses. In regression settings, the existing MCL variants focus on merging the hypotheses, thereby eventually sacrificing the diversity of the predictions. In contrast, our method relies on a novel learned scoring scheme underpinned by a mathematical framework based on Voronoi tessellations of the output space, from which we can derive a probabilistic interpretation. After empirically validating rMCL with experiments on synthetic data, we further assess its merits on the sound source localization problem, demonstrating its practical usefulness and the relevance of its interpretation. . . BibTeX . @inproceedings{Letzelter23, author = {Victor Letzelter and Mathieu Fontaine and Mickaël Chen and Patrick Pérez and Gael Richard and Slim Essid}, title = {Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis}, booktitle = {Advances in Neural Information Processing Systems}, year = 2023 } . .",
          "url": "https://valeoai.github.io/blog/publications/rmcl/",
          "relUrl": "/publications/rmcl/",
          "date": ""
      }
      
  

  
      ,"page40": {
          "title": "SALUDA: Surface-based Automotive Lidar Unsupervised Domain Adaptation",
          "content": "SALUDA: Surface-based Automotive Lidar Unsupervised Domain Adaptation . Björn Michele&nbsp;&nbsp; Alexandre Boulch&nbsp;&nbsp; Gilles Puy &nbsp;&nbsp; Tuan-Hung Vu &nbsp;&nbsp; Renaud Marlet&nbsp;&nbsp; Nicolas Courty . 3DV 2024 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Learning models on one labeled dataset that generalize well on another domain is a difficult task, as several shifts might happen between the data domains. This is notably the case for lidar data, for which models can exhibit large performance discrepancies due for instance to different lidar patterns or changes in acquisition conditions. This paper addresses the corresponding Unsupervised Domain Adaptation (UDA) task for semantic segmentation. To mitigate this problem, we introduce an unsupervised auxiliary task of learning an implicit underlying surface representation simultaneously on source and target data. As both domains share the same latent representation, the model is forced to accommodate discrepancies between the two sources of data. This novel strategy differs from classical minimization of statistical divergences or lidar-specific domain adaptation techniques. Our experiments demonstrate that our method achieves a better performance than the current state of the art, both in real-to-real and synthetic-to-real scenarios. . . BibTeX . @inproceedings{michele24saluda, title={SALUDA: Surface-based Automotive Lidar Unsupervised Domain Adaptation}, author={Michele, Bjoern and Boulch, Alexandre and Puy, Gilles and Vu, Tuan-Hung and Marlet, Renaud and Courty, Nicolas}, booktitle={3DV}, year={2024} } . .",
          "url": "https://valeoai.github.io/blog/publications/SALUDA/",
          "relUrl": "/publications/SALUDA/",
          "date": ""
      }
      
  

  
      ,"page41": {
          "title": "AdvEnt: Adversarial Entropy minimization for domain adaptation in semantic segmentation",
          "content": "AdvEnt: Adversarial Entropy minimization for domain adaptation in semantic segmentation . Tuan-Hung Vu&nbsp;&nbsp; Himalaya Jain&nbsp;&nbsp; Maxime Bucher&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . CVPR 2019 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; Blog &nbsp;&nbsp; . . Abstract . Semantic segmentation is a key problem for many computer vision tasks. While approaches based on convolutional neural networks constantly break new records on different benchmarks, generalizing well to diverse testing environments remains a major challenge. In numerous real world applications, there is indeed a large gap between data distributions in train and test domains, which results in severe performance loss at run-time. In this work, we address the task of unsupervised domain adaptation in semantic segmentation with losses based on the entropy of the pixel-wise predictions. To this end, we propose two novel, complementary methods using (i) an entropy loss and (ii) an adversarial loss respectively. We demonstrate state-of-the-art performance in semantic segmentation on two challenging synthetic-2-real set-ups and show that the approach can also be used for detection. . . Video . . . . BibTeX . @inproceedings{vu2018advent, title={ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation}, author={Vu, Tuan-Hung and Jain, Himalaya and Bucher, Maxime and Cord, Mathieu and P{ &#39;e}rez, Patrick}, booktitle={CVPR}, year={2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/advent/",
          "relUrl": "/publications/advent/",
          "date": ""
      }
      
  

  
      ,"page42": {
          "title": "Boosting Few-Shot Visual Learning With Self-Supervision",
          "content": "Boosting Few-Shot Visual Learning With Self-Supervision . Spyros Gidaris&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Nikos Komodakis&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Matthieu Cord . ICCV 2019 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Few-shot learning and self-supervised learning address different facets of the same problem: how to train a model with little or no labeled data. Few-shot learning aims for optimization methods and models that can learn efficiently to recognize patterns in the low data regime. Self-supervised learning focuses instead on unlabeled data and looks into it for the supervisory signal to feed high capacity deep neural networks. In this work we exploit the complementarity of these two domains and propose an approach for improving few-shot learning through self-supervision. We use self-supervision as an auxiliary task in a few-shot learning pipeline, enabling feature extractors to learn richer and more transferable visual representations while still using few annotated samples. Through self-supervision, our approach can be naturally extended towards using diverse unlabeled data from other datasets in the few-shot setting. We report consistent improvements across an array of architectures, datasets and self-supervision techniques. We provide the implementation code at: https://github.com/valeoai/BF3S . . . BibTeX . @inproceedings{gidaris2019boosting, title={Boosting few-shot visual learning with self-supervision}, author={Gidaris, Spyros and Bursuc, Andrei and Komodakis, Nikos and P{ &#39;e}rez, Patrick and Cord, Matthieu}, booktitle={Proceedings of the IEEE International Conference on Computer Vision}, pages={8059--8068}, year={2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/bf3s/",
          "relUrl": "/publications/bf3s/",
          "date": ""
      }
      
  

  
      ,"page43": {
          "title": "Learning Representations by Predicting Bags of Visual Words",
          "content": "Learning Representations by Predicting Bags of Visual Words . Spyros Gidaris&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Nikos Komodakis&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Matthieu Cord . CVPR 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Self-supervised representation learning targets to learn convnet-based image representations from unlabeled data. Inspired by the success of NLP methods in this area, in this work we propose a self-supervised approach based on spatially dense image descriptions that encode discrete visual concepts, here called visual words. To build such discrete representations, we quantize the feature maps of a first pre-trained self-supervised convnet, over a k-means based vocabulary. Then, as a self-supervised task, we train another convnet to predict the histogram of visual words of an image (i.e., its Bag-of-Words representation) given as input a perturbed version of that image. The proposed task forces the convnet to learn perturbation-invariant and context-aware image features, useful for downstream image understanding tasks. We extensively evaluate our method and demonstrate very strong empirical results, e.g., our pre-trained self-supervised representations transfer better on detection task and similarly on classification over classes &quot;unseen&quot; during pre-training, when compared to the supervised case. This also shows that the process of image discretization into visual words can provide the basis for very powerful self-supervised approaches in the image domain, thus allowing further connections to be made to related methods from the NLP domain that have been extremely successful so far. . . . BibTeX . @inproceedings{gidaris2020learning, title={Learning Representations by Predicting Bags of Visual Words}, author={Gidaris, Spyros and Bursuc, Andrei and Komodakis, Nikos and P{ &#39;e}rez, Patrick and Cord, Matthieu}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={6928--6938}, year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/bownet/",
          "relUrl": "/publications/bownet/",
          "date": ""
      }
      
  

  
      ,"page44": {
          "title": "Handling new target classes in semantic segmentation with domain adaptation",
          "content": "Handling new target classes in semantic segmentation with domain adaptation . Maxime Bucher&nbsp;&nbsp; Tuan-Hung Vu&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . CVIU 2021 . Paper&nbsp;&nbsp; . . Abstract . In this work, we define and address a novel domain adaptation (DA) problem in semantic scene segmentation, where the target domain not only exhibits a data distribution shift w.r.t. the source domain, but also includes novel classes that do not exist in the latter. Different to &quot;open-set&quot; and &quot;universal domain adaptation&quot;, which both regard all objects from new classes as &quot;unknown&quot;, we aim at explicit test-time prediction for these new classes. To reach this goal, we propose a framework that leverages domain adaptation and zero-shot learning techniques to enable &quot;boundless&quot; adaptation in the target domain. It relies on a novel architecture, along with a dedicated learning scheme, to bridge the source-target domain gap while learning how to map new classes&#39; labels to relevant visual representations. The performance is further improved using self-training on target-domain pseudo-labels. For validation, we consider different domain adaptation set-ups, namely synthetic-2-real, country-2-country and dataset-2-dataset. Our framework outperforms the baselines by significant margins, setting competitive standards on all benchmarks for the new task. . . . BibTeX . @inproceedings{bucher2021buda, title={Handling new target classes in semantic segmentation with domain adaptation}, author={Bucher, Maxime and Tuan-Hung, VU and Cord, Matthieu and P{ &#39;e}rez, Patrick}, journal={Computer Vision and Image Understanding}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/buda/",
          "relUrl": "/publications/buda/",
          "date": ""
      }
      
  

  
      ,"page45": {
          "title": "CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations",
          "content": "CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations . Arthur Ouaknine&nbsp;&nbsp;Alasdair Newson&nbsp;&nbsp; Julien Rebut&nbsp;&nbsp; Florence Tupin&nbsp;&nbsp; Patrick Pérez . ICPR 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; Blog &nbsp;&nbsp; . . Abstract . High quality perception is essential for autonomous driving (AD) systems. To reach the accuracy and robustness that are required by such systems, several types of sensors must be combined. Currently, mostly cameras and laser scanners (lidar) are deployed to build a representation of the world around the vehicle. While radar sensors have been used for a long time in the automotive industry, they are still under-used for AD despite their appealing characteristics (notably, their ability to measure the relative speed of obstacles and to operate even in adverse weather conditions). To a large extent, this situation is due to the relative lack of automotive datasets with real radar signals that are both raw and annotated. In this work, we introduce CARRADA, a dataset of synchronized camera and radar recordings with range-angle-Doppler annotations. We also present a semi-automatic annotation approach, which was used to annotate the dataset, and a radar semantic segmentation baseline, which we evaluate on several metrics. . . BibTeX . @INPROCEEDINGS{9413181, author={Ouaknine, Arthur and Newson, Alasdair and Rebut, Julien and Tupin, Florence and Pérez, Patrick}, booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, title={CARRADA Dataset: Camera and Automotive Radar with Range- Angle- Doppler Annotations}, year={2021}, pages={5068-5075}, doi={10.1109/ICPR48806.2021.9413181} } . .",
          "url": "https://valeoai.github.io/blog/publications/carrada/",
          "relUrl": "/publications/carrada/",
          "date": ""
      }
      
  

  
      ,"page46": {
          "title": "Addressing Failure Prediction by Learning Model Confidence",
          "content": "Addressing Failure Prediction by Learning Model Confidence . Charles Corbière&nbsp;&nbsp; Nicolas Thome&nbsp;&nbsp; Avner Bar-Hen&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . NeurIPS 2019 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; BibTeX&nbsp;&nbsp; . . Abstract . Assessing reliably the confidence of a deep neural net and predicting its failures is of primary importance for the practical deployment of these models. In this paper, we propose a new target criterion for model confidence, corresponding to the True Class Probability (TCP). We show how using the TCP is more suited than relying on the classic Maximum Class Probability (MCP). We provide in addition theoretical guarantees for TCP in the context of failure prediction. Since the true class is by essence unknown at test time, we propose to learn TCP criterion on the training set, introducing a specific learning scheme adapted to this context. Extensive experiments are conducted for validating the relevance of the proposed approach. We study various network architectures, small and large scale datasets for image classification and semantic segmentation. We show that our approach consistently outperforms several strong methods, from MCP to Bayesian uncertainty, as well as recent approaches specifically designed for failure prediction. . . BibTeX . @incollection{NIPS2019_8556, title = {Addressing Failure Prediction by Learning Model Confidence}, author = {Corbi `{e}re, Charles and THOME, Nicolas and Bar-Hen, Avner and Cord, Matthieu and &#39;{e}rez, Patrick}, booktitle = {Advances in Neural Information Processing Systems 32}, pages = {2902--2913}, year = {2019}, } . .",
          "url": "https://valeoai.github.io/blog/publications/confidnet/",
          "relUrl": "/publications/confidnet/",
          "date": ""
      }
      
  

  
      ,"page47": {
          "title": "Confidence Estimation via Auxiliary Models",
          "content": "Confidence Estimation via Auxiliary Models . Charles Corbière&nbsp;&nbsp; Nicolas Thome&nbsp;&nbsp; Antoine Saporta&nbsp;&nbsp; Tuan-Hung Vu&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . TPAMI 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; BibTeX&nbsp;&nbsp; . . Abstract . Reliably quantifying the confidence of deep neural classifiers is a challenging yet fundamental requirement for deployingsuch models in safety-critical applications. In this paper, we introduce a novel target criterion for model confidence, namely the true class probability (TCP). We show that TCP offers better properties for confidence estimation than standard maximum class probability (MCP). Since the true class is by essence unknown at test time, we propose to learn TCP criterion from data with an auxiliary model, introducing a specific learning scheme adapted to this context. We evaluate our approach on the task of failure prediction and ofself-training with pseudo-labels for domain adaptation, which both necessitate effective confidence estimates. Extensive experiments are conducted for validating the relevance of the proposed approach in each task. We study various network architectures andexperiment with small and large datasets for image classification and semantic segmentation. In every tested benchmark, our approach outperforms strong baselines . . BibTeX . @article{corbiere2021confidence, author={Corbiere, Charles and Thome, Nicolas and Saporta, Antoine and Vu, Tuan-Hung and Cord, Matthieu and Perez, Patrick}, journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, title={Confidence Estimation via Auxiliary Models}, year={2021}, volume={}, number={}, pages={1-1}, doi={10.1109/TPAMI.2021.3085983}} } . .",
          "url": "https://valeoai.github.io/blog/publications/confidnet_conda/",
          "relUrl": "/publications/confidnet_conda/",
          "date": ""
      }
      
  

  
      ,"page48": {
          "title": "DADA: Depth-aware Domain Adaptation in Semantic Segmentation",
          "content": "DADA: Depth-aware Domain Adaptation in Semantic Segmentation . Tuan-Hung Vu&nbsp;&nbsp; Himalaya Jain&nbsp;&nbsp; Maxime Bucher&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . ICCV 2019 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Unsupervised domain adaptation (UDA) is important for applications where large scale annotation of representative data is challenging. For semantic segmentation in particular, it helps deploy on real &quot;target domain&quot; data models that are trained on annotated images from a different &quot;source domain&quot;, notably a virtual environment. To this end, most previous works consider semantic segmentation as the only mode of supervision for source domain data, while ignoring other, possibly available, information like depth. In this work, we aim at exploiting at best such a privileged information while training the UDA model. We propose a unified depth-aware UDA framework that leverages in several complementary ways the knowledge of dense depth in the source domain. As a result, the performance of the trained semantic segmentation model on the target domain is boosted. Our novel approach indeed achieves state-of-the-art performance on different challenging synthetic-2-real benchmarks. . . . BibTeX . @inproceedings{vu2019dada, title={Dada: Depth-aware domain adaptation in semantic segmentation}, author={Vu, Tuan-Hung and Jain, Himalaya and Bucher, Maxime and Cord, Matthieu and P{ &#39;e}rez, Patrick}, booktitle={Proceedings of the IEEE International Conference on Computer Vision}, pages={7364--7373}, year={2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/dada/",
          "relUrl": "/publications/dada/",
          "date": ""
      }
      
  

  
      ,"page49": {
          "title": "Artificial Dummies for Urban Dataset Augmentation",
          "content": "Artificial Dummies for Urban Dataset Augmentation . Antonín Vobecký&nbsp;&nbsp; David Hurych&nbsp;&nbsp; Michal Uříčář&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Josef Šivic . AAAI 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Existing datasets for training pedestrian detectors in images suffer from limited appearance and pose variation. The most challenging scenarios are rarely included because they are too difficult to capture due to safety reasons, or they are very unlikely to happen. The strict safety requirements in assisted and autonomous driving applications call for an extra high detection accuracy also in these rare situations. Having the ability to generate people images in arbitrary poses, with arbitrary appearances and embedded in different background scenes with varying illumination and weather conditions, is a crucial component for the development and testing of such applications. The contributions of this paper are three-fold. First, we describe an augmentation method for controlled synthesis of urban scenes containing people, thus producing rare or neverseen situations. This is achieved with a data generator (called DummyNet) with disentangled control of the pose, the appearance, and the target background scene. Second, the proposed generator relies on novel network architecture and associated loss that takes into account the segmentation of the foreground person and its composition into the background scene. Finally, we demonstrate that the data generated by our DummyNet improve performance of several existing person detectors across various datasets as well as in challenging situations, such as night-time conditions, where only a limited amount of training data is available. In the setup with only day-time data available, we improve the night-time detector by 17% log-average miss rate over the detector trained with the day-time data only . . . BibTeX . @inproceedings{vobecky2021artificial, title={Artificial Dummies for Urban Dataset Augmentation}, author={Vobeck{ &#39;y}, Anton{ &#39;i}n and Hurych, David and U{ vr}i{ vc}{ &#39;a}{ vr}, Michal and P{ &#39;e}rez, Patrick and Sivic, Josef}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, pages={0--0}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/dummynet/",
          "relUrl": "/publications/dummynet/",
          "date": ""
      }
      
  

  
      ,"page50": {
          "title": "Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems",
          "content": "Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems . Isabelle Liang&nbsp;&nbsp; Ganesh Sistu&nbsp;&nbsp; Fabian Burger&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Senthil Yogamani&nbsp;&nbsp; . ITSC 2020 . Paper&nbsp;&nbsp; . . Abstract . Deep multi-task networks are of particular interest for autonomous driving systems. They can potentially strike an excellent trade-off between predictive performance, hardware constraints and efficient use of information from multiple types of annotations and modalities. However, training such models is non-trivial and requires balancing learning over all tasks as their respective losses display different scales, ranges and dynamics across training. Multiple task weighting methods that adjust the losses in an adaptive way have been proposed recently on different datasets and combinations of tasks, making it difficult to compare them. In this work, we review and systematically evaluate nine task weighting strategies on common grounds on three automotive datasets (KITTI, Cityscapes and WoodScape). We then propose a novel method combining evolutionary meta-learning and task-based selective backpropagation, for computing task weights leading to reliable network training. Our method outperforms state-of-the-art methods by a significant margin on a two-task application. . . . Results . . Comparison of various task-weighting methods for two-task network training. . Task weights and asynchronous backpropagation frequencies computed by several task-weighting methods. . BibTeX . @article{leang2020dynamic, title={Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems}, author={Leang, Isabelle and Sistu, Ganesh and Burger, Fabian and Bursuc, Andrei and Yogamani, Senthil}, journal={arXiv preprint arXiv:2001.02223}, year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/dynamic-mtl/",
          "relUrl": "/publications/dynamic-mtl/",
          "date": ""
      }
      
  

  
      ,"page51": {
          "title": "End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances",
          "content": "End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances . Marin Toromanoff&nbsp;&nbsp; Emilie Wirbel&nbsp;&nbsp; Fabien Moutarde . CVPR 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Reinforcement Learning (RL) aims at learning an optimal behavior policy from its own experiments and not rule-based control methods. However, there is no RL algorithm yet capable of handling a task as difficult as urban driving. We present a novel technique, coined implicit affordances, to effectively leverage RL for urban driving thus including lane keeping, pedestrians and vehicles avoidance, and traffic light detection. To our knowledge we are the first to present a successful RL agent handling such a complex task especially regarding the traffic light detection. Furthermore, we have demonstrated the effectiveness of our method by winning the Camera Only track of the CARLA challenge. . . . Video . . . . BibTeX . @inproceedings{toromanoff2020end, title={End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances}, author={Toromanoff, Marin and Wirbel, Emilie and Moutarde, Fabien}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={7153--7162}, year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/e2e-rl-driving/",
          "relUrl": "/publications/e2e-rl-driving/",
          "date": ""
      }
      
  

  
      ,"page52": {
          "title": "ESL: Entropy-guided Self-supervised Learning for Domain Adaptation in Semantic Segmentation",
          "content": "ESL: Entropy-guided Self-supervised Learning for Domain Adaptation in Semantic Segmentation . Antoine Saporta&nbsp;&nbsp;Tuan-Hung Vu&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . CVPRW 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . While fully-supervised deep learning yields good models for urban scene semantic segmentation, these models struggle to generalize to new environments with different lighting or weather conditions for instance. In addition, producing the extensive pixel-level annotations that the task requires comes at a great cost. Unsupervised domain adaptation (UDA) is one approach that tries to address these issues in order to make such systems more scalable. In particular, self-supervised learning (SSL) has recently become an effective strategy for UDA in semantic segmentation. At the core of such methods liespseudo-labeling&#39;, that is, the practice of assigning high-confident class predictions as pseudo-labels, subsequently used as true labels, for target data. To collect pseudo-labels, previous works often rely on the highest softmax score, which we here argue as an unfavorable confidence measurement. . . BibTeX . @inproceedings{saporta2020esl, title={Esl: Entropy-guided self-supervised learning for domain adaptation in semantic segmentation}, author={Saporta, Antoine and Vu, Tuan-Hung and Cord, Matthieu and P{ &#39;e}rez, Patrick}, booktitle={CVPRW}, year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/esl/",
          "relUrl": "/publications/esl/",
          "date": ""
      }
      
  

  
      ,"page53": {
          "title": "FKAConv: Feature-Kernel Alignment for Point Cloud Convolutiont",
          "content": "FKAConv: Feature-Kernel Alignment for Point Cloud Convolutiont . Alexandre Boulch &nbsp;&nbsp; Gilles Puy &nbsp;&nbsp; Renaud Marlet&nbsp;&nbsp; . ACCV 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Recent state-of-the-art methods for point cloud processing are based on the notion of point convolution, for which several approaches have been proposed. In this paper, inspired by discrete convolution in image processing, we provide a formulation to relate and analyze a number of point convolution methods. We also propose our own convolution variant, that separates the estimation of geometry-less kernel weights and their alignment to the spatial support of features. Additionally, we define a point sampling strategy for convolution that is both effective and fast. Finally, using our convolution and sampling strategy, we show competitive results on classification and semantic segmentation benchmarks while being time and memory efficient. . . BibTeX . @inproceedings{boulch2020fka, title={FKAConv: Feature-Kernel Alignment for Point Cloud Convolutions}, author={Boulch, Alexandre and Puy, Gilles and Marlet, Renaud}, booktitle={15th Asian Conference on Computer Vision (ACCV 2020)}, year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/fkaconv/",
          "relUrl": "/publications/fkaconv/",
          "date": ""
      }
      
  

  
      ,"page54": {
          "title": "FLOT: Scene Flow on Point Clouds guided by Optimal Transport",
          "content": "FLOT: Scene Flow on Point Clouds guided by Optimal Transport . Gilles Puy &nbsp;&nbsp; Alexandre Boulch &nbsp;&nbsp; Renaud Marlet . ECCV 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . We propose and study a method called FLOT that estimates scene flow on point clouds. We start the design of FLOT by noticing that scene flow estimation on point clouds reduces to estimating a permutation matrix in a perfect world. Inspired by recent works on graph matching, we build a method to find these correspondences by borrowing tools from optimal transport. Then, we relax the transport constraints to take into account real-world imperfections. The transport cost between two points is given by the pairwise similarity between deep features extracted by a neural network trained under full supervision using synthetic datasets. Our main finding is that FLOT can perform as well as the best existing methods on synthetic and real-world datasets while requiring much less parameters and without using multiscale analysis. Our second finding is that, on the training datasets considered, most of the performance can be explained by the learned transport cost. This yields a simpler method, FLOT0, which is obtained using a particular choice of optimal transport parameters and performs nearly as well as FLOT. . . BibTeX . @inproceedings{puy20flot, title={FLOT: Scene Flow on Point Clouds Guided by Optimal Transport}, author={Puy, Gilles and Boulch, Alexandre and Marlet, Renaud}, booktitle={European Conference on Computer Vision} year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/flot/",
          "relUrl": "/publications/flot/",
          "date": ""
      }
      
  

  
      ,"page55": {
          "title": "This dataset does not exist: training models from generated images",
          "content": "This dataset does not exist: training models from generated images . Victor Besnier&nbsp;&nbsp; Himalaya Jain&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . ICASSP 2020 . Paper&nbsp;&nbsp; . . Abstract . Current generative networks are increasingly proficient in generating high-resolution realistic images. These generative networks, especially the conditional ones, can potentially become a great tool for providing new image datasets. This naturally brings the question: Can we train a classifier only on the generated data? This potential availability of nearly unlimited amounts of training data challenges standard practices for training machine learning models, which have been crafted across the years for limited and fixed size datasets. In this work we investigate this question and its related challenges. We identify ways to improve significantly the performance over naive training on randomly generated images with regular heuristics. We propose three standalone techniques that can be applied at different stages of the pipeline, i.e., data generation, training on generated data, and deploying on real data. We evaluate our proposed approaches on a subset of the ImageNet dataset and show encouraging results compared to classifiers trained on real images. . . . Results . . Effect of applying HSM at different iterations during classifier training. The first image of each category is sampled randomly. The other two images are generated from HSM-computed codes at two different steps during training. The difference between the images shows that the effect of HSM is specific to the classifier. . Results for ImageNet-10 real test images. Performance of classifiers trained on generated images with all combinations of the proposed methods. Each classifier is trained for $150$ epochs (except Long training, where we let DS run for $150$ epochs) over a set of $N = 13K$ images; in case of continuous sampling we replace $50 %$ (i.e., $6,500$) of the images every epoch, while fixed dataset is the usual setup where no images are replaced during training. In all setups we use $N$ images per epoch. First column, without applying any of the proposed methods, is the baseline. Each of the proposed methods individually shows improvement over the baseline. The combination of the methods further improves the results. Effect of replacement fraction $r$ in DS. Classification accuracy using DS on real images with varying $r$, i.e., fraction of the dataset being replaced with new images every epoch. The figure shows plots for DS with and without BNA. . . BibTeX . @inproceedings{besnier2020dataset, title={This dataset does not exist: training models from generated images}, author={Besnier, Victor and Jain, Himalaya and Bursuc, Andrei and Cord, Matthieu and P{ &#39;e}rez, Patrick}, booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages={1--5}, year={2020}, organization={IEEE} } . .",
          "url": "https://valeoai.github.io/blog/publications/gan-dataset/",
          "relUrl": "/publications/gan-dataset/",
          "date": ""
      }
      
  

  
      ,"page56": {
          "title": "Large-Scale Unsupervised Object Discovery",
          "content": "Large-Scale Unsupervised Object Discovery . Huy V. Vo&nbsp;&nbsp; Elena Sizikova&nbsp;&nbsp; Cordelia Schmid&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Jean Ponce . NeurIPS 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Existing approaches to unsupervised object discovery (UOD) do not scale up to large datasets without approximations that compromise their performance. We propose a novel formulation of UOD as a ranking problem, amenable to the arsenal of distributed methods available for eigenvalue problems and link analysis. Through the use of self-supervised features, we also demonstrate the first effective fully unsupervised pipeline for UOD. Extensive experiments on COCO [42] and OpenImages [35] show that, in the single-object discovery setting where a single prominent object is sought in each image, the proposed LOD (Large-scale Object Discovery) approach is on par with, or better than the state of the art for mediumscale datasets (up to 120K images), and over 37% better than the only other algorithms capable of scaling up to 1.7M images. In the multi-object discovery setting where multiple objects are sought in each image, the proposed LOD is over 14% better in average precision (AP) than all other methods for datasets ranging from 20K to 1.7M images. Using self-supervised features, we also show that the proposed method obtains state-of-the-art UOD performance on OpenImages. . . . BibTeX . @inproceedings{Vo21LOD, title = {Large-Scale Unsupervised Object Discovery}, author = {Vo, Huy V. and Sizikova, Elena and Schmid, Cordelia and P{ &#39;e}rez, Patrick and Ponce, Jean}, booktitle = {Advances in Neural Information Processing Systems 34 ({NeurIPS})} year = {2021}, } . .",
          "url": "https://valeoai.github.io/blog/publications/lod/",
          "relUrl": "/publications/lod/",
          "date": ""
      }
      
  

  
      ,"page57": {
          "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
          "content": "Localizing Objects with Self-Supervised Transformers and no Labels . Oriane Siméoni &nbsp;&nbsp; Gilles Puy &nbsp;&nbsp; Huy V. Vo &nbsp;&nbsp; Simon Roburin &nbsp;&nbsp; Spyros Gidaris &nbsp;&nbsp; Andrei Bursuc &nbsp;&nbsp; Patrick Pérez &nbsp;&nbsp; Renaud Marlet &nbsp;&nbsp; Jean Ponce . BMVC 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Localizing objects in image collections without supervision can help to avoid expensive annotation campaigns. We propose a simple approach to this problem, that leverages the activation features of a vision transformer pre-trained in a self-supervised manner. Our method, LOST, does not require any external object proposal nor any exploration of the image collection; it operates on a single image. Yet, we outperform state-of-the-art object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. We also show that training a class-agnostic detector on the discovered objects boosts results by another 7 points. Moreover, we show promising results on the unsupervised object discovery task. . . BibTeX . @inproceedings{LOST, title = {Localizing Objects with Self-Supervised Transformers and no Labels}, author = {Oriane Sim &#39;eoni and Gilles Puy and Huy V. Vo and Simon Roburin and Spyros Gidaris and Andrei Bursuc and Patrick P &#39;erez and Renaud Marlet and Jean Ponce}, journal = {Proceedings of the British Machine Vision Conference (BMVC)}, month = {November}, year = {2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/lost/",
          "relUrl": "/publications/lost/",
          "date": ""
      }
      
  

  
      ,"page58": {
          "title": "Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation",
          "content": "Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation . Antoine Saporta&nbsp;&nbsp;Tuan-Hung Vu&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . ICCV 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . In this work, we address the task of unsupervised domain adaptation (UDA) for semantic segmentation in presence of multiple target domains: The objective is to train a single model that can handle all these domains at test time. Such a multi-target adaptation is crucial for a variety of scenarios that real-world autonomous systems must handle. It is a challenging setup since one faces not only the domain gap between the labeled source set and the unlabeled target set, but also the distribution shifts existing within the latter among the different target domains. To this end, we introduce two adversarial frameworks: (i) multi-discriminator, which explicitly aligns each target domain to its counterparts, and (ii) multi-target knowledge transfer, which learns a target-agnostic model thanks to a multi-teacher/single-student distillation mechanism.The evaluation is done on four newly-proposed multi-target benchmarks for UDA in semantic segmentation. In all tested scenarios, our approaches consistently outperform baselines, setting competitive standards for the novel task. . . BibTeX . @inproceedings{saporta2021mtaf, title={Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation}, author={Saporta, Antoine and Vu, Tuan-Hung and Cord, Mathieu and P{ &#39;e}rez, Patrick}, booktitle={ICCV}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/mtaf/",
          "relUrl": "/publications/mtaf/",
          "date": ""
      }
      
  

  
      ,"page59": {
          "title": "Multi-View Radar Semantic Segmentation",
          "content": "Multi-View Radar Semantic Segmentation . Arthur Ouaknie&nbsp;&nbsp;Alasdair Newson&nbsp;&nbsp;Patrick Pérez&nbsp;&nbsp; Florence Tupin&nbsp;&nbsp; Julien Rebut . ICCV 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; Blog &nbsp;&nbsp; . . Abstract . Understanding the scene around the ego-vehicle is key to assisted and autonomous driving. Nowadays, this is mostly conducted using cameras and laser scanners, despite their reduced performances in adverse weather conditions. Automotive radars are low-cost active sensors that measure properties of surrounding objects, including their relative speed, and have the key advantage of not being impacted by rain, snow or fog. However, they are seldom used for scene understanding due to the size and complexity of radar raw data and the lack of annotated datasets. Fortunately, recent open-sourced datasets have opened up research on classification, object detection and semantic segmentation with raw radar signals using end-to-end trainable models. In this work, we propose several novel architectures, and their associated losses, which analyse multiple &quot;views&quot; of the range-angle-Doppler radar tensor to segment it semantically. Experiments conducted on the recent CARRADA dataset demonstrate that our best model outperforms alternative models, derived either from the semantic segmentation of natural images or from radar scene understanding, while requiring significantly fewer parameters. . . BibTeX . @inproceedings{ouaknine_2021_multi-view, title={Multi-View Radar Semantic Segmentation}, author={Ouaknine, Arthur and Alasdair, Newson and P{ &#39;e}rez, Patrick and Tupin, Florence and Rebut, Julien}, booktitle={ICCV}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/mvrss/",
          "relUrl": "/publications/mvrss/",
          "date": ""
      }
      
  

  
      ,"page60": {
          "title": "Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning",
          "content": "Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning . Spyros Gidaris&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Gilles Puy&nbsp;&nbsp; Nikos Komodakis Patrick Pérez&nbsp;&nbsp; Matthieu Cord . CVPR 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . . Abstract . Learning image representations without human supervision is an important and active research field. Several recent approaches have successfully leveraged the idea of making such a representation invariant under different types of perturbations, especially via contrastive-based instance discrimination training. Although effective visual representations should indeed exhibit such invariances, there are other important characteristics, such as encoding contextual reasoning skills, for which alternative reconstruction-based approaches might be better suited. With this in mind, we propose a teacher-student scheme to learn representations by training a convnet to reconstruct a bag-of-visual-words (BoW) representation of an image, given as input a perturbed version of that same image. Our strategy performs an online training of both the teacher network (whose role is to generate the BoW targets) and the student network (whose role is to learn representations), along with an online update of the visual-words vocabulary (used for the BoW targets). This idea effectively enables fully online BoW-guided unsupervised learning. Extensive experiments demonstrate the interest of our BoW-based strategy which surpasses previous state-of-the-art methods (including contrastive-based ones) in several applications. For instance, in downstream tasks such Pascal object detection, Pascal classification and Places205 classification, our method improves over all prior unsupervised approaches, thus establishing new state-of-the-art results that are also significantly better even than those of supervised pre-training. . . . Results . . Evaluation of ImageNet pre-trained ResNet50 models. The &#39;&#39;Epochs&#39;&#39; and &#39;&#39;Batch&#39;&#39; columns provide the number of pre-training epochs and the batch size of each model respectively. The first section includes models pre-trained with a similar number of epochs as our model (second section). We boldfaced the best results among all sections as well as of only the top two. For the linear classification tasks, we provide the top-1 accuracy. For object detection, we fine-tuned Faster R-CNN (R50-C4) on VOC $ texttt{trainval07+12}$ and report detection AP scores by testing on $ texttt{test07}$. For semi-supervised learning, we fine-tune the pre-trained models on $1 %$ and $10 %$ of ImageNet and report the top-5 accuracy. Note that, in this case the &#39;&#39;Supervised&#39;&#39; entry results come from (Zhai et al.) and are obtained by supervised training using only $1 %$ or $10 %$ of the labelled data. All the classification results are computed with single-crop testing. $^ dagger$: results computed by us. . BibTeX . @inproceedings{gidaris2021obow, title={Learning Representations by Predicting Bags of Visual Words}, author={Gidaris, Spyros and Bursuc, Andrei and Puy, Gilles and Komodakis, Nikos and Cord, Matthieu and P{ &#39;e}rez, Patrick}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/obow/",
          "relUrl": "/publications/obow/",
          "date": ""
      }
      
  

  
      ,"page61": {
          "title": "Unsupervised Image Matching and Object Discovery as Optimization",
          "content": "Unsupervised Image Matching and Object Discovery as Optimization . Huy V. Vo&nbsp;&nbsp; Francis Bach&nbsp;&nbsp; Minsu Cho&nbsp;&nbsp; Kai Han&nbsp;&nbsp; Yann LeCun&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Jean Ponce . CVPR 2019 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Learning with complete or partial supervision is power- ful but relies on ever-growing human annotation efforts. As a way to mitigate this serious problem, as well as to serve specific applications, unsupervised learning has emerged as an important field of research. In computer vision, unsu- pervised learning comes in various guises. We focus here on the unsupervised discovery and matching of object cate- gories among images in a collection, following the work of Cho et al. [12]. We show that the original approach can be reformulated and solved as a proper optimization problem. Experiments on several benchmarks establish the merit of our approach. . . . BibTeX . @inproceedings{Vo19UOD, title = {Unsupervised image matching and object discovery as optimization}, author = {Vo, Huy V. and Bach, Francis and Cho, Minsu and Han, Kai and LeCun, Yann and P{ &#39;e}rez, Patrick and Ponce, Jean}, booktitle = {CVPR}, year = {2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/osd/",
          "relUrl": "/publications/osd/",
          "date": ""
      }
      
  

  
      ,"page62": {
          "title": "PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds",
          "content": "PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds . Anh-Quan Cao &nbsp;&nbsp; Gilles Puy &nbsp;&nbsp; Alexandre Boulch &nbsp;&nbsp; Renaud Marlet . ICCV 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Rigid registration of point clouds with partial overlaps is a longstanding problem usually solved in two steps: (a) finding correspondences between the point clouds; (b) filtering these correspondences to keep only the most reliable ones to estimate the transformation. Recently, several deep nets have been proposed to solve these steps jointly. We built upon these works and propose PCAM: a neural network whose key element is a pointwise product of cross-attention matrices that permits to mix both low-level geometric and high-level contextual information to find point correspondences. These cross-attention matrices also permits the exchange of context information between the point clouds, at each layer, allowing the network construct better matching features within the overlapping regions. The experiments show that PCAM achieves state-of-the-art results among methods which, like us, solve steps (a) and (b) jointly via deepnets. . . BibTeX . @inproceedings{cao21pcam, title={PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds}, author={Cao, Anh-Quan and Puy, Gilles and Boulch, Alexandre and Marlet, Renaud}, booktitle={International Conference on Computer Vision}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/pcam/",
          "relUrl": "/publications/pcam/",
          "date": ""
      }
      
  

  
      ,"page63": {
          "title": "PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving",
          "content": "PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving . Thibault Buhet &nbsp;&nbsp; Emilie Wirbel&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Xavier Perrotton . CoRL 2020 . Paper&nbsp;&nbsp; . . Abstract . To navigate safely in urban environments, an autonomous vehicle (*ego vehicle*) must understand and anticipate its surroundings, in particular the behavior and intents of other road users (*neighbors*). Most of the times, multiple decision choices are acceptable for all road users (e.g., turn right or left, or different ways of avoiding an obstacle), leading to a highly uncertain and multi-modal decision space. We focus here on predicting multiple feasible future trajectories for both ego vehicle and neighbors through a probabilistic framework. We rely on a conditional imitation learning algorithm, conditioned by a navigation command for the ego vehicle (e.g., *turn right*). Our model processes ego vehicle front-facing camera images and bird-eye view grid, computed from Lidar point clouds, with detections of past and present objects, in order to generate multiple trajectories for both ego vehicle and its neighbors. Our approach is computationally efficient and relies only on on-board sensors. We evaluate our method offline on the publicly available dataset nuScenes, achieving state-of-the-art performance, investigate the impact of our architecture choices on online simulated experiments and show preliminary insights for real vehicle control. . . . Results . Comparison on nuScenes 4s prediction with published results of DESIRE-plan, ESP and PRECOG from (Rhinehart et al., 2019) (results from their Table II, with a fixed 5 agents training), over minMSD metric. . Closed loop error locations for urban and track test data (internal data), visualized for PLOP and constant velocity baseline. We note that braking behind a vehicle can induce multiple high speed errors and stack multiple red dots on the same location. Points of interest (traffic lights, roundabout, stop signs) are highlighted on the map. . Video . . . . BibTeX . @article{buhet2020plop, title={PLOP: Probabilistic poLynomial Objects trajectory Planning for autonomous driving}, author={Buhet, Thibault and Wirbel, Emilie and Bursuc, Andrei and Perrotton, Xavier}, journal={Conference on Robot Learning (CoRL)}, year={2020} } . . . References . Rhinehart, N., McAllister, R., Kitani, K., &amp; Levine, S. (2019). Precog: Prediction conditioned on goals in visual multi-agent settings. Iccv. |",
          "url": "https://valeoai.github.io/blog/publications/plop/",
          "relUrl": "/publications/plop/",
          "date": ""
      }
      
  

  
      ,"page64": {
          "title": "QuEST: Quantized Embedding Space for Transferring Knowledge",
          "content": "QuEST: Quantized Embedding Space for Transferring Knowledge . Himalaya Jain&nbsp;&nbsp; Spyros Gidaris&nbsp;&nbsp; Nikos Komodakis&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Matthieu Cord . ECCV 2020 . Paper&nbsp;&nbsp; . . Abstract . Knowledge distillation refers to the process of training a student network to achieve better accuracy by learning from a pre-trained teacher network. Most of the existing knowledge distillation methods direct the student to follow the teacher by matching the teacher&#39;s output, feature maps or their distribution. In this work, we propose a novel way to achieve this goal: by distilling the knowledge through a quantized visual words space. According to our method, the teacher&#39;s feature maps are first quantized to represent the main visual concepts (i.e., visual words) encompassed in these maps and then the student is asked to predict those visual word representations. Despite its simplicity, we show that our approach is able to yield results that improve the state of the art on knowledge distillation for model compression and transfer learning scenarios. To that end, we provide an extensive evaluation across several network architectures and most commonly used benchmark datasets. . . . Video . . . . BibTeX . @article{jain2019quest, title={QUEST: Quantized embedding space for transferring knowledge}, author={Jain, Himalaya and Gidaris, Spyros and Komodakis, Nikos and P{ &#39;e}rez, Patrick and Cord, Matthieu}, journal={arXiv preprint arXiv:1912.01540}, year={2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/quest/",
          "relUrl": "/publications/quest/",
          "date": ""
      }
      
  

  
      ,"page65": {
          "title": "Toward Unsupervised, Multi-Object Discovery in Large-Scale Image Collections",
          "content": "Toward Unsupervised, Multi-Object Discovery in Large-Scale Image Collections . Huy V. Vo&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Jean Ponce . ECCV 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . This paper addresses the problem of discovering the objects present in a collection of images without any supervision. We build on the optimization approach of Vo et al. [34] with several key novelties: (1) We propose a novel saliency-based region proposal algorithm that achieves significantly higher overlap with ground-truth objects than other competitive methods. This procedure leverages off-the-shelf CNN features trained on classification tasks without any bounding box information, but is otherwise unsupervised. (2) We exploit the inherent hierarchical structure of proposals as an effective regularizer for the approach to object discovery of [34], boosting its performance to significantly improve over the state of the art on several standard benchmarks. (3) We adopt a two-stage strategy to select promising proposals using small random sets of images before using the whole image collection to discover the objects it depicts, allowing us to tackle, for the first time (to the best of our knowledge), the discovery of multiple objects in each one of the pictures making up datasets with up to 20,000 images, an over five-fold increase compared to existing methods, and a first step toward true large-scale unsupervised image interpretation. . . . BibTeX . @inproceedings{Vo20rOSD, title = {Toward unsupervised, multi-object discovery in large-scale image collections}, author = {Vo, Huy V. and P{ &#39;e}rez, Patrick and Ponce, Jean}, booktitle = {Proceedings of the European Conference on Computer Vision ({ECCV})}, year = {2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/rosd/",
          "relUrl": "/publications/rosd/",
          "date": ""
      }
      
  

  
      ,"page66": {
          "title": "Semantic Palette: Guiding Scene Generation with Class Proportions ",
          "content": "Semantic Palette: Guiding Scene Generation with Class Proportions . Guillaume Le Moing&nbsp;&nbsp; Tuan-Hung Vu&nbsp;&nbsp; Himalaya Jain&nbsp;&nbsp; Patrick Pérez &nbsp;&nbsp; Matthieu Cord . CVPR 2021 . . . Abstract . Despite the recent progress of generative adversarial networks (GANs) at synthesizing photo-realistic images, producing complex urban scenes remains a challenging problem. Previous works break down scene generation into two consecutive phases: unconditional semantic layout synthesis and image synthesis conditioned on layouts. In this work, we propose to condition layout generation as well for higher semantic control: given a vector of class proportions, we generate layouts with matching composition. To this end, we introduce a conditional framework with novel architecture designs and learning objectives, which effectively accommodates class proportions to guide the scene generation process. The proposed architecture also allows partial layout editing with interesting applications. Thanks to the semantic control, we can produce layouts close to the real distribution, helping enhance the whole scene generation process. On different metrics and urban scene benchmarks, our models outperform existing baselines. Moreover, we demonstrate the merit of our approach for data augmentation: semantic segmenters trained on real layout-image pairs along with additional ones generated by our approach outperform models only trained on real pairs. . . Video . . . . BibTeX . @inproceedings{lemoing2021semanticpalette, title={Semantic Palette: Guiding Scene Generation with Class Proportions}, author={Le Moing, Guillaume and Vu, Tuan-Hung and Jain, Himalaya and P{ &#39;e}rez, Patrick and Cord, Mathieu}, booktitle={CVPR}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/semanticpalette/",
          "relUrl": "/publications/semanticpalette/",
          "date": ""
      }
      
  

  
      ,"page67": {
          "title": "Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data",
          "content": "Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data . Corentin Sautier &nbsp;&nbsp; Gilles Puy &nbsp;&nbsp; Spyros Gidaris &nbsp;&nbsp; Alexandre Boulch &nbsp;&nbsp; Andrei Bursuc &nbsp;&nbsp; Renaud Marlet . CVPR 2022 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Segmenting or detecting objects in sparse Lidar point clouds are two important tasks in autonomous driving to allow a vehicle to act safely in its 3D environment. The best performing methods in 3D semantic segmentation or object detection rely on a large amount of annotated data. Yet annotating 3D Lidar data for these tasks is tedious and costly. In this context, we propose a self-supervised pre-training method for 3D perception models that is tailored to autonomous driving data. Specifically, we leverage the availability of synchronized and calibrated image and Lidar sensors in autonomous driving setups for distilling self-supervised pre-trained image representations into 3D models. Hence, our method does not require any point cloud nor image annotations. The key ingredient of our method is the use of superpixels which are used to pool 3D point features and 2D pixel features in visually similar regions. We then train a 3D network on the self-supervised task of matching these pooled point features with the corresponding pooled image pixel features. The advantages of contrasting regions obtained by superpixels are that: (1) grouping together pixels and points of visually coherent regions leads to a more meaningful contrastive task that produces features well adapted to 3D semantic segmentation and 3D object detection; (2) all the different regions have the same weight in the contrastive loss regardless of the number of 3D points sampled in these regions; (3) it mitigates the noise produced by incorrect matching of points and pixels due to occlusions between the different sensors. Extensive experiments on autonomous driving datasets demonstrate the ability of our image-to-Lidar distillation strategy to produce 3D representations that transfer well on semantic segmentation and object detection tasks. . . BibTeX . @inproceedings{sautier22slidr, title={Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data}, author={Corentin Sautier and Gilles Puy and Spyros Gidaris and Alexandre Boulch and Andrei Bursuc and Renaud Marlet}, booktitle={CVPR} year={2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/slidr/",
          "relUrl": "/publications/slidr/",
          "date": ""
      }
      
  

  
      ,"page68": {
          "title": "TRADI: Tracking deep neural network weight distributions for uncertainty estimation",
          "content": "TRADI: Tracking deep neural network weight distributions for uncertainty estimation . Gianni Franchi&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Emanuel Aldea&nbsp;&nbsp; Severine Dubuisson&nbsp;&nbsp; Isabelle Bloch . ECCV 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . During training, the weights of a Deep Neural Network (DNN) are optimized from a random initialization towards a nearly optimum value minimizing a loss function. Only this final state of the weights is typically kept for testing, while the wealth of information on the geometry of the weight space, accumulated over the descent towards the minimum is discarded. In this work we propose to make use of this knowledge and leverage it for computing the distributions of the weights of the DNN. This can be further used for estimating the epistemic uncertainty of the DNN by sampling an ensemble of networks from these distributions. To this end we introduce a method for tracking the trajectory of the weights during optimization, that does not require any changes in the architecture nor on the training procedure. We evaluate our method on standard classification and regression benchmarks, and on out-of-distribution detection for classification and semantic segmentation. We achieve competitive results, while preserving computational efficiency in comparison to other popular approaches. . . . Results . . Results on a synthetic regression task comparing MC dropout, Deep Ensembles, and TRADI. $x$-axis: spatial coordinate of the Gaussian process. Black lines: ground truth curve. Blue points: training points. Orange areas: estimated variance. . Distinguishing in- and out-of-distribution data for semantic segmentation (CamVid, StreetHazards, BDD Anomaly) and image classification (MNIST/notMNIST). . Qualitative results on CamVid-OOD. Columns: $(a)$ input image and ground truth; $(b)-(d)$ predictions and confidence scores by MC Dropout, Deep Ensembles, and TRADI. Rows: $(1)$ input and confidence maps; $(2)$ class predictions; $(3)$ zoomed-in area on input and confidence maps . BibTeX . @article{franchi2019tradi, title={TRADI: Tracking deep neural network weight distributions for uncertainty estimation}, author={Franchi, Gianni and Bursuc, Andrei and Aldea, Emanuel and Dubuisson, S{ &#39;e}verine and Bloch, Isabelle}, journal={arXiv preprint arXiv:1912.11316}, year={2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/tradi/",
          "relUrl": "/publications/tradi/",
          "date": ""
      }
      
  

  
      ,"page69": {
          "title": "VRUNet: Multi-Task Learning Model for Intent Prediction of Vulnerable Road Users",
          "content": "VRUNet: Multi-Task Learning Model for Intent Prediction of Vulnerable Road Users . Adithya Ranga&nbsp;&nbsp; Filippo Giruzzi&nbsp;&nbsp; Jagdish Bhanushali&nbsp;&nbsp; Emilie Wirbel&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Tuan-Hung Vu&nbsp;&nbsp; Xavier Perotton&nbsp;&nbsp; . Electronic Imaging 2020 . Paper&nbsp;&nbsp; . . Abstract . Advanced perception and path planning are at the core for any self-driving vehicle. Autonomous vehicles need to understand the scene and intentions of other road users for safe motion planning. For urban use cases it is very important to perceive and predict the intentions of pedestrians, cyclists, scooters, etc., classified as vulnerable road users (VRU). Intent is a combination of pedestrian activities and long term trajectories defining their future motion. In this paper we propose a multi-task learning model to predict pedestrian actions, crossing intent and forecast their future path from video sequences. We have trained the model on naturalistic driving open-source JAAD [1] dataset, which is rich in behavioral annotations and real world scenarios. Experimental results show state-of-the-art performance on JAAD dataset and how we can benefit from jointly learning and predicting actions and trajectories using 2D human pose features and scene context. . . BibTeX . @article{ranga2020vrunet, title={VRUNet: Multi-Task Learning Model for Intent Prediction of Vulnerable Road Users}, author={Ranga, Adithya and Giruzzi, Filippo and Bhanushali, Jagdish and Wirbel, Emilie and P{ &#39;e}rez, Patrick and Vu, Tuan-Hung and Perotton, Xavier}, journal={Electronic Imaging}, year={2020}} . .",
          "url": "https://valeoai.github.io/blog/publications/vrunet/",
          "relUrl": "/publications/vrunet/",
          "date": ""
      }
      
  

  
      ,"page70": {
          "title": "xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation",
          "content": "xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation . Maximilian Jaritz&nbsp;&nbsp; Tuan-Hung Vu&nbsp;&nbsp; Raoul de Charette&nbsp;&nbsp; Émilie Wirbel&nbsp;&nbsp; Patrick Pérez . CVPR 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Unsupervised Domain Adaptation (UDA) is crucial to tackle the lack of annotations in a new domain. There are many multi-modal datasets, but most UDA approaches are uni-modal. In this work, we explore how to learn from multi-modality and propose cross-modal UDA (xMUDA) where we assume the presence of 2D images and 3D point clouds for 3D semantic segmentation. This is challenging as the two input spaces are heterogeneous and can be impacted differently by domain shift. In xMUDA, modalities learn from each other through mutual mimicking, disentangled from the segmentation objective, to prevent the stronger modality from adopting false predictions from the weaker one. We evaluate on new UDA scenarios including day-to-night, country-to-country and dataset-to-dataset, leveraging recent autonomous driving datasets. xMUDA brings large improvements over uni-modal UDA on all tested scenarios, and is complementary to state-of-the-art UDA techniques. Code is available at: https://github.com/valeoai/xmuda . . Video . . . . BibTeX . @inproceedings{jaritz2020xmuda, title={xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation}, author={Jaritz, Maximilian and Vu, Tuan-Hung and Charette, Raoul de and Wirbel, Emilie and P{ &#39;e}rez, Patrick}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={12605--12614}, year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/xmuda/",
          "relUrl": "/publications/xmuda/",
          "date": ""
      }
      
  

  
      ,"page71": {
          "title": "Zero-Shot Semantic Segmentation",
          "content": "Zero-Shot Semantic Segmentation . Maxime Bucher&nbsp;&nbsp; Tuan-Hung Vu&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . NeurIPS 2019 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Semantic segmentation models are limited in their ability to scale to large numbers of object classes. In this paper, we introduce the new task of zero-shot semantic segmentation: learning pixel-wise classifiers for never-seen object categories with zero training examples. To this end, we present a novel architecture, ZS3Net, combining a deep visual segmentation model with an approach to generate visual representations from semantic word embeddings. By this way, ZS3Net addresses pixel classification tasks where both seen and unseen categories are faced at test time (so called &quot;generalized&quot; zero-shot classification). Performance is further improved by a self-training step that relies on automatic pseudo-labeling of pixels from unseen classes. On the two standard segmentation datasets, Pascal-VOC and Pascal-Context, we propose zero-shot benchmarks and set competitive baselines. For complex scenes as ones in the Pascal-Context dataset, we extend our approach by using a graph-context encoding to fully leverage spatial context priors coming from class-wise segmentation maps. . . . BibTeX . @inproceedings{bucher2019zero, title={Zero-shot semantic segmentation}, author={Bucher, Maxime and Tuan-Hung, VU and Cord, Matthieu and P{ &#39;e}rez, Patrick}, booktitle={Advances in Neural Information Processing Systems}, pages={468--479}, year={2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/zs3/",
          "relUrl": "/publications/zs3/",
          "date": ""
      }
      
  


}