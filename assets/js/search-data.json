{
  
    
        "post0": {
            "title": "valeo.ai at CVPR 2022",
            "content": "The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR) is a major event for researchers and engineers working on computer vision and machine learning. At the 2022 edition the valeo.ai team will present four papers in the main conference, three papers in workshops and one workshop keynote. The team will be at CVPR to present these works and will be happy to discuss more about these projects and ideas, and share our exciting ongoing research. . Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data . Authors: Corentin Sautier, Gilles Puy, Spyros Gidaris, Alexandre Boulch, Andrei Bursuc, Renaud Marlet . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . Self-driving vehicles require object detection or segmentation to safely maneuver in their environment. Such safety-critical tasks are usually performed by neural networks demanding huge Lidar datasets with high quality annotations, and no domain shift between training and testing conditions. However, annotating 3D Lidar data for these tasks is tedious and costly. In Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data, we propose a self-supervised pre-training method for 3D perception models that is tailored to autonomous driving data and that does not require any annotation. Specifically, we leverage the availability of synchronized and calibrated image and Lidar data in autonomous driving setups for distilling self-supervised pre-trained image representations into 3D models, using neither point cloud nor image annotations. . . Synchronized Lidar and camera frames are encoded through two modality-specific features extractors. The camera backbone has pre-trained weights obtained with no annotations (e.g., with MoCo v2 (Chen et al., 2020)). Features are pooled at a pseudo-object level using image superpixels, and contrasted between both modalities A key ingredient of our method is the use of superpixels which are used to pool 3D point features and 2D pixel features in visually similar regions. We then train a 3D network on the self-supervised task of matching these pooled 3D-point features with the corresponding pooled image pixel features. Extensive experiments on autonomous driving datasets demonstrate the ability of our image-to-Lidar distillation strategy to produce 3D representations that transfer well to semantic segmentation and object detection tasks. . . The similarity between a query point&#39;s features (in red) and all other Lidar points is shown, to assert the quality of the learned representation. Color scale goes from purple (low similarity) to yellow (high similarity). With our pre-training, a Lidar network can learn features that are mostly consistent within an object class. This pre-training greatly improves data annotation efficiency, both in semantic segmentation and object detection, and is even applicable in cross-dataset setups. . . Raw High-Definition Radar for Multi-Task Learning . Authors: Julien Rebut, Arthur Ouaknine, Waqas Walik, Patrick Pérez . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . With their robustness to adverse weather conditions and their ability to measure speeds, radar sensors have been part of the automotive landscape for more than two decades. Recent progress toward High Definition (HD) Imaging radars has driven the angular resolution below the degree, thus approaching laser scanning performance. However, the amount of data a HD radar delivers and the computational cost to estimate the angular positions remain a challenge. In this paper, we propose a novel HD radar sensing model, FFT-RadNet, that eliminates the overhead of computing the range-azimuth-Doppler 3D tensor, learning instead to recover angles from a range-Doppler spectrum. This architecture can be leveraged for various perception tasks with raw HD radar signals. In particular we show how to train FFT-RadNet both to detect vehicles and to segment free driving space. On both tasks, it competes with the most recent radar-based models while requiring less compute and memory. . . Overview of FFT-RadNet for vehicle detection and drivable space segmentation in raw HD radar signal. Also, and importantly, we collected and annotated 2-hour worth of raw data from synchronized automotive-grade sensors (camera, laser, HD radar) in various environments (city street, highway, countryside road). This unique dataset, nick-named RADIal for “Radar, Lidar et al.”, is publicly available. . . Scene sample form RADIal dataset with (a) camera image, (b) radar power spectrum, (c) free-space in bird-eye view, (d) Range-azimuth map in Cartesian coordinates, and (e) GPS trace (red) and odometry trajectory (green); laser (resp. radar) points are in red (resp. indigo), annotated vehicle bounding boxes in orange and annotated drivable space in green. . POCO: Point convolution for surface reconstruction . Authors: Alexandre Boulch, Renaud Marlet . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . Implicit neural networks have been successfully used for surface reconstruction from point clouds. However, many of them face scalability issues as they encode the isosurface function of a whole object or scene into a single latent vector. To overcome this limitation, a few approaches infer latent vectors on a coarse regular 3D grid or on 3D patches, and interpolate them to answer occupancy queries. In doing so, they lose the direct connection with the input points sampled on the surface of objects, and they attach information uniformly in space rather than where it matters the most, i.e., near the surface. Besides, relying on fixed patch sizes may require discretization tuning. . In POCO, we propose to use point cloud convolution and compute a latent vector at each input point. We then perform a learning-based interpolation on nearest neighbors using inferred weights. On the one hand, using a convolutional backbone allows the aggregation of global information about the shape needed to correctly orientate the surface (decide which side of the surface is inside or outside). On the other hand, surface location is inferred via a local attention-based approach which enables accurate surface positioning. . . POCO overview. Top row: the decoding mechanism takes as input local latent vectors and local coordinates which are lifted with a point-wise MLP. The resulting representations are weighted with an attention mechanism in order to take the occupancy decision. Bottom row: reconstruction examples with POCO, scene reconstruction with a model trained on objects (left), object reconstruction with noisy point cloud (middle) and out of domain object reconstruction (right). We show that our approach, while being very simple to set up, reaches the state of the art on several reconstruction-from-point-cloud benchmarks. It underlines the importance of reasoning about the surface location at a local scale, close to the input points. POCO also shows good generalization properties including the possibility of learning on object datasets while being able to reconstruct complex scenes. . . DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion . Authors: Arthur Douillard, Alexandre Ramé, Guillaume Couairon, Matthieu Cord . [Paper] &nbsp;&nbsp; [Code] . Deep network architectures struggle to continually learn new tasks without forgetting the previous tasks. In this paper, we propose a transformer architecture based on a dedicated encoder/decoder framework. Critically, the encoder and decoder are shared among all tasks. Through a dynamic expansion of special tokens, we specialize each forward of our decoder network on a task distribution. Our strategy scales to a large number of tasks while having negligible memory and time overheads due to strict control of the parameters expansion. Moreover, this efficient strategy does not need any hyperparameter tuning to control the network’s expansion. Our model reaches excellent results on CIFAR100 and state-of-the-art performances on the large-scale ImageNet100 and ImageNet1000 while having fewer parameters than concurrent dynamic frameworks. . . DyTox transformer model. . FlexIT: Towards Flexible Semantic Image Translation . Authors: Guillaume Couairon, Asya Grechka, Jakob Verbeek, Holger Schwenk, Matthieu Cord . [Paper] . Deep generative models, like GANs, have considerably improved the state of the art in image synthesis, and are able to generate near photo-realistic images in structured domains such as human faces. We propose FlexIT, a novel method which can take any input image and a user-defined text instruction for editing. Our method achieves flexible and natural editing, pushing the limits of semantic image translation. First, FlexIT combines the input image and text into a single target point in the CLIP multimodal embedding space. Via the latent space of an auto-encoder, we iteratively transform the input image toward the target point, ensuring coherence and quality with a variety of novel regularization terms. We propose an evaluation protocol for semantic image translation, and thoroughly evaluate our method on ImageNet. . . FlexIT transformation examples. From top to bottom: input image, transformed image, and text query. . Raising context awareness in motion forecasting . CVPR 2022 Workshop on Autonomous Driving . Authors: Hédi Ben-Younes, Éloi Zablocki, Mickaël Chen, Patrick Pérez, Matthieu Cord . [Paper] . . Overview of CAB. CAB employs a CVAE backbone which produces distributions over the latent variable and the future trajectory. During training, a blind input is forwarded into the CVAE and the resulting distribution over the latent variable is used to encourage the prediction of the model to be different from the context-agnostic distribution, thanks to the CAB-KL loss. Learning-based trajectory prediction models have encountered great success, with the promise of leveraging contextual information in addition to motion history. Yet, we find that state-of-the-art forecasting methods tend to overly rely on the agent’s current dynamics, failing to exploit the semantic contextual cues provided at its input. To alleviate this issue, we introduce CAB, a motion forecasting model equipped with a training procedure designed to promote the use of semantic contextual information. We also introduce two novel metrics, dispersion and convergence-to-range, to measure the temporal consistency of successive forecasts, which we found missing in standard metrics. Our method is evaluated on the widely adopted nuScenes Prediction benchmark as well as on a subset of the most difficult examples from this benchmark. . . CSG0: Continual Urban Scene Generation with Zero Forgetting . CVPR 2022 Workshop on Continual Learning (CLVision) . Authors: Himalaya Jain, Tuan-Hung Vu, Patrick Pérez, Matthieu Cord . [Paper] &nbsp;&nbsp; [Project page] . With the rapid advances in generative adversarial networks (GANs), the visual quality of synthesized scenes keeps improving, including for complex urban scenes with applications to automated driving. We address in this work a continual scene generation setup in which GANs are trained on a stream of distinct domains; ideally, the learned models should eventually be able to generate new scenes in all seen domains. This setup reflects the real-life scenario where data are continuously acquired in different places at different times. In such a continual setup, we aim for learning with zero forgetting, i.e., with no degradation in synthesis quality over earlier domains due to catastrophic forgetting. To this end, we introduce a novel framework, named CSG0, that not only (i) enables seamless knowledge transfer in continual training but also (ii) guarantees zero forgetting with a small overhead cost. . . Overview of CSG0. Our continual setup for urban-scene generation involves a stream of datasets, with GANs trained from one dataset to another. Our framework makes use of the knowledge learned from previous domains and adapts to new ones with a small overhead. To showcase the merit of our framework, we conduct intensive experiments on various continual urban scene setups, covering both synthetic-to-real and real-to-real scenarios. Quantitative evaluations and qualitative visualizations demonstrate the interest of our CSG0 framework, which operates with minimal overhead cost (in terms of architecture size and training). Benefiting from continual learning, CSG0 outperforms the state-of-the-art OASIS model trained on single domains. We also provide experiments with three datasets to emphasize how well our strategy generalizes despite its cost constraints. Under extreme low-data regimes, our approach outperforms the baseline by a large margin. . . Multi-Head Distillation for Continual Unsupervised Domain Adaptation in Semantic Segmentation . CVPR 2022 Workshop on Continual Learning (CLVision) . Authors: Antoine Saporta, Arthur Douillard, Tuan-Hung Vu, Patrick Pérez, Matthieu Cord . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . This work focuses on a novel framework for learning UDA, continuous UDA, in which models operate on multiple target domains discovered sequentially, without access to previous target domains. We propose MuHDi, for Multi-Head Distillation, a method that solves the catastrophic forgetting problem, inherent in continual learning tasks. MuHDi performs distillation at multiple levels from the previous model as well as an auxiliary target-specialist segmentation head. We report both extensive ablation and experiments on challenging multi-target UDA semantic segmentation benchmarks to validate the proposed learning scheme and architecture. . . Predictions of continual baseline and MuHDi in a Cityscapes scene. The baseline model suffers from catastrophic forgetting when adapting from one domain to another. The proposed MuHDi is more resilient to continual adaptation and preserve predictive accuracy. References . Chen, X., Fan, H., Girshick, R., &amp; He, K. (2020). Improved baselines with momentum contrastive learning. ArXiv Preprint ArXiv:2003.04297. |",
            "url": "https://valeoai.github.io/blog/2022/06/14/valeoai-at-cvpr-2022.html",
            "relUrl": "/2022/06/14/valeoai-at-cvpr-2022.html",
            "date": " • Jun 14, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "valeo.ai at ICCV 2021",
            "content": "The International Conference on Computer Vision (ICCV) is a top event for researchers and engineers working on computer vision and machine learning. The valeo.ai team will present six papers in the main conference, four of which are presented below. Join us to find out more about these projects and ideas, meet our team and learn about our exciting ongoing research. See you at ICCV! . Multi-View Radar Semantic Segmentation . Authors: Arthur Ouaknine, Alasdair Newson, Patrick Pérez, Florence Tupin, Julien Rebut . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . . Example of a scene from the CARRADA dataset (Ouaknine et al., 2021). From left to right: camera image, range-angle view, range-Doppler view, angle, Doppler view. Understanding the scene around the ego-vehicle is key to assisted and autonomous driving. Nowadays, this is mostly conducted using cameras and laser scanners, despite their reduced performance in adverse weather conditions. Automotive radars are low-cost active sensors that measure properties of surrounding objects, including their relative speed, and have the key advantage of not being impacted by rain, snow or fog and could effectively complement the other perception sensors mounted on the car, e.g., cameras, LIDAR. However, they are seldom used for scene understanding due to the size and complexity of radar raw data and the lack of annotated datasets. Fortunately, recent open-sourced datasets have opened up research on classification, object detection and semantic segmentation with raw radar signals using end-to-end trainable models. . . Sequences of raw radar tensors are aggregated and used as input for our multi-view architecture to segment semantically range-angle and range-Doppler views simultaneously. In our paper, Multi-View Radar Semantic Segmentation, we propose a set of deep neural network architectures to segment simultaneously range-angle and range-Doppler radar representations, providing the location and the radial velocity of the detected objects. Our best model takes a sequence of radar views as input, extracts features using individual branches including ASPP blocks, and recovers the range-angle and range-Doppler view dimensions with two decoding branches. We also propose a combination of loss functions composed of a weighted cross entropy, a soft dice and an additional coherence term. We introduce a coherence loss to impose a spatial consistency between the segmented radar views. Our experiments on the CARRADA dataset (Ouaknine et al., 2021) demonstrate that our best model outperforms competing methods with a large margin while requiring significantly fewer parameters. . . Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation . Authors: Victor Besnier, Andrei Bursuc, Alexandre Briot, David Picard . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . . Uncertainty map visualization on the BDD-Anomaly dataset. 1st col.: We highlight the ground truth locations of the OOD objects to help visualize them (red bounding box). 2nd col.: Segmentation map of the SegNet. 3rd to 5th col.: Uncertainty Map highlighted in yellow. Our method produces stronger responses on OOD regions compared to other methods, while being as strong on regular error regions, e.g., boundaries. For real-world decision systems such as autonomous vehicles, accuracy is not the only performance requirement and it often comes second to reliability, robustness, and safety concerns, as any failure carries serious consequences. Component modules of such systems frequently rely on powerful Deep Neural Networks (DNNs), that however do not always generalize to objects unseen in the training data. Simple uncertainty estimation techniques, e.g., entropy of softmax predictions, are less effective since modern DNNs are consistently overconfident on both in-domain and out-of-distribution (OOD) data samples. This hinders further the performance of downstream components relying on their predictions. Dealing successfully with the “unknown unknown”, e.g., by launching an alert or failing gracefully, is crucial. . . By making our target model to fail we can learn its behavior when failing and more reliably detect it at test time. In this work we take inspiration from practices in industrial validation, where the performance of a target model is tested in various extreme cases. Instead of simply verifying the performance of the model we learn how this model behaves in face of failures. To this end we propose a new OOD detection architecture called ObsNet and an associated training scheme based on Local Adversarial Attacks (LAA). Finding failure modes in a trained DNN is quite challenging as such models typically achieve high accuracy, i.e., are rarely wrong, and corner-case samples are rather inserted in the training set than used for validation. LAA triggers failure modes in the target model that are a good proxy for failures in face of unknown OOD data. ObsNet achieves reliable detection of failure and OOD objects without compromising on predictive accuracy and computational time. . . Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation . Authors: Antoine Saporta, Tuan-Hung Vu, Matthieu Cord, Patrick Pérez . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . . Single-target unsupervised domain adaptation fails to produce models that perform on multiple target domains. The aim of multi-target unsupervised domain adaptation is to train a model that excels on these multiple target domains. Autonomous vehicles rely on perception models that require a tremendous amount of annotated data to be trained in a supervised fashion. To reduce the reliance on manual annotation which can get extremely expensive when we consider semantic segmentation of urban scenes for instance, domain adaptation is a popular topic that leverages annotated data from a source domain to train a model on a target domain. More specifically, the unsupervised domain adaptation (UDA) setting only relies on unlabeled data from the target domain and aims at bridging the gap between target and source domains. Most UDA approaches tackle the alignment between a single source domain and a single target domain but don’t generalize well to more domains. Yet, real-world perception systems need to be confronted to a variety of scenarios, such as multiple cities or multiple weather conditions, motivating to extend UDA to multi-target settings. . In our work, Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation, we introduce two UDA frameworks to tackle multi-target adaptation: (i) multi-discriminator, which extends single target UDA approaches to multiple target domains by explicitly aligning each target domain to its counterparts; (ii) multi-target knowledge transfer, which learns a target-agnostic model thanks to a multiple teachers/single student distillation mechanism. We also propose multiple new challenging evaluation benchmarks for multi-target UDA in semantic segmentation based on existing urban scenes datasets. . . PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds . Authors: Anh-Quan Cao, Gilles Puy, Alexandre Boulch, Renaud Marlet . [Paper] &nbsp;&nbsp; [Code] &nbsp;&nbsp; [Project page] . . Point cloud registration has many applications in various domains such as autonomous driving, motion and pose estimation, 3D reconstruction, simultaneous localisation and mapping (SLAM), and augmented reality. The most famous method to solve this task is ICP, but is mostly suited for small transformations. Several improvements have been made and the most recent techniques leverage deep learning. . The typical pipeline for point cloud registration is (a) point matching followed by (b) point-pairs filtering to remove incorrect matches in, e.g., non-overlapping regions. One natural way to improve this pipeline is to use deep learning in step (a) to obtain point features of high quality and get pairs of matching points with a nearest neighbors search in this learned feature space. Then, one can typically rely on a classical RANSAC-based method in step (b). Another category of methods exploits deep learning in step (a) and step (b), as proposed by, e.g., DCP, PRNet, DGR. PCAM belongs to this second category where a first network outputs pairs of matching points and a second network filters incorrect pairs. . We construct PCAM by observing that one needs two types of information to correctly match points between two point clouds. First, one needs local fine geometric information to precisely select the best corresponding point. Second, one also needs high-level contextual information to differentiate between points with similar local geometry but from different parts of the scene. Therefore, we compute point correspondences at every layer of our deep network via cross-attention matrices, and combine these matrices via a pointwise multiplication. This simple yet very effective solution naturally ensures that both low-level geometric and high-level context information are exploited when matching points. It also permits to remove spurious matches found only at one scale. Furthermore, these cross-attention matrices are also exploited to exchange information between the point clouds at each layer, allowing the network to use context information to find the best matching point within the overlapping regions. . References . Ouaknine, A., Newson, A., Rebut, J., Tupin, F., &amp; Pérez, P. (2021). CARRADA Dataset: Camera and Automotive Radar with Range- Angle- Doppler Annotations. 2020 25th International Conference on Pattern Recognition (ICPR), 5068–5075. |",
            "url": "https://valeoai.github.io/blog/2021/10/08/valeoai-at-iccv-2021.html",
            "relUrl": "/2021/10/08/valeoai-at-iccv-2021.html",
            "date": " • Oct 8, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "How can we make driving systems explainable?",
            "content": "This post is an introduction to our survey on the explainability of vision-based driving systems, which can be found on arXiv here. . Research on autonomous vehicles is blooming thanks to recent advances in deep learning and computer vision, as well as the development of autonomous driving datasets and simulators. The number of academic publications on this subject is rising in most machine learning, computer vision, robotics and transportation conferences, and journals. On the industry side, several suppliers are already producing cars equipped with advanced computer vision technologies for automatic lane following, assisted parking, or collision detection among other things. Meanwhile, constructors are working on and designing prototypes with level 4 and 5 autonomy. . In the 2010s, we observe an interest in approaches aiming to train driving systems, usually in the form of neural networks, either by leveraging large quantities of expert recordings or through simulation. In both cases, these systems learn a highly complex transformation that operates over input sensor data and produces end-commands (steering angle, throttle). While these neural driving models overcome some of the limitations of the traditional modular pipeline stack, they are sometimes described as black-boxes for their critical lack of transparency and interpretability. Thus, being able to explain the behavior of neural driving models is of paramount importance for their deployment and social acceptance. . Explainability? . Many terms are related to the concept of explainability and several definitions have been proposed for each of these terms. The boundaries between concepts are fuzzy and constantly evolving. In human-machine interactions, explainability is defined as the ability for the human user to understand the agent’s logic (Rosenfeld &amp; Richardson, 2019). The explanation is based on how the human user understands the connections between inputs and outputs of the model. According to (Doshi-Velez &amp; Kortz, 2017), an explanation is a human-interpretable description of the process by which a decision-maker took a particular set of inputs and reached a particular conclusion. They state that in practice, an explanation should answer at least one of the three following questions: . What were the main factors in the decision? | Would changing a certain factor have changed the decision? | Why did two similar-looking cases get different decisions, or vice versa? | . The term explainability often co-occurs with the concept of interpretability. Some recent work of (Beaudouin et al., 2020) simply advocate that explainability and interpretability are synonyms. However, (Gilpin et al., 2018) provide a nuance between these terms that we find interesting. According to them, interpretability designates to which extent an explanation is understandable by a human. They state that an explanation should be designed and assessed in a trade-off between its interpretability and its completeness, which measures how accurate the explanation is as it describes the inner workings of the system. The whole challenge in explaining neural networks is to provide explanations that are both interpretable and complete. . Interestingly, depending on who is the explanation geared towards, it is expected to have varying nature, form and should convey different types of information. . End-users and citizens need to trust the autonomous system and to be reassured. They put their life in the hands of the driving system and thus need to gain trust in it. It appears that user trust is heavily impacted by the system transparency (Zhang et al., 2020): providing information that helps the user understand how the system functions foster his or her trust in the system. Interestingly, research on human-computer interactions argues that an explanation should be provided before the vehicle takes an action, in a formulation which is concise and direct. | Designers of self-driving models need to understand their limitations to validate them and improve future versions. The concept of Operational Design Domain (ODD) is often used by carmakers to designate the conditions under which the car is expected to behave safely. Thus, whenever a machine learning model is built to address the task of driving, it is crucial to know and understand its failure modes, and to verify that these situations do not overlap with the ODD. A common practice is to stratify the evaluation into situations, as is done by the European New Car Assessment Program (Euro NCAP) to test and assess assisted driving functionalities in new vehicles. But even if these in-depth performance analyses are helpful to improve the model’s performance, it is not possible to exhaustively list and evaluate every situation the model may possibly encounter. As a fallback solution, explainability can help delving deeper into the inner workings of the model and to understand why it makes these errors and correct the model/training data accordingly. | Legal and regulatory bodies are interested in explanations for liability and accountability purposes, especially when a self-driving system is involved in a car accident. Notably, explanations generated for legal or regulatory institutions are likely to be different from those addressed to the end-user, as all aspects of the decision process could be required to identify the reasons for a malfunction. | . Driving system? . The history of autonomous driving systems started in the late ’80s and early ’90s with the European Eureka project called Prometheus. This has later been followed by driving challenges proposed by the Defense Advanced Research Projects Agency (DARPA). The vast majority of autonomous systems competing in these challenges is characterized by their modularity: several sub-modules are assembled, each completing a very specific task. Broadly speaking, these subtasks deal with sensing the environment, forecasting future events, planning, taking high-level decisions, and controlling the vehicle. . As pipeline architectures split the driving task into easier-to-solve problems, they offer somewhat interpretable processing of sensor data through specialized modules (perception, planning, decision, control). However, these approaches have several drawbacks: . First, they rely on human heuristics and manually-chosen intermediate representations, which are not proven to be optimal for the driving task. | Second, they lack flexibility to account for real-world uncertainties and to generalize to unplanned scenarios. | Finally, they are prone to error propagation between the multiple sub-modules. | . To circumvent these issues, and nurtured by the deep learning revolution, researchers put more and more efforts on machine learning-based driving systems, and in particular on deep neural networks which can leverage large quantities of data. . We can distinguish four key elements involved in the design of a neural driving system: input sensors, input representations, output type, and learning paradigm . . Figure 1. Overview of neural network-based autonomous driving systems. Sensors. They are the hardware interface through which the neural network perceives its environment. They include cameras, radars, LiDARs, GPS, but also sensors about internal vehicle state such as speed or yaw. For a thorough review of driving sensors, we refer the reader to (Yurtsever et al., 2020). | Input representation. Once sensory inputs are acquired by the system, they are processed by computer vision models to build a structured representation, before being passed to the neural driving system. In the mediated perception approach, several perception systems provide their understanding of the world, and their outputs are aggregated to build an input for the driving model. An example of such vision tasks is object detection and semantic segmentation, tracking objects across time, extracting depth information (i.e. knowing the distance that separates the vehicle from each point in the space), recognizing pedestrian intent… Mediated perception contrasts with the direct perception approach, which instead extracts visual affordances from an image. Affordances are scalar indicators that describe the road situation such as curvature, deviation to neighboring lanes, or distances between ego and other vehicles. | Outputs. Ultimately, the goal is to generate vehicle controls. Some approaches, called end-to-end, tackle this problem by training the deep network to directly output the commands. However, in practice most methods instead predict the future trajectory of the autonomous vehicle; they are called end-to-mid methods. The trajectory is then expected to be followed by a low-level controller, such as the proportional–integral–derivative (PID) controller. | Learning. Two families of methods coexist for training self-driving neural models: behavior cloning approaches, which leverage datasets of human driving sessions, and reinforcement learning approaches, which train models through trial-and-error simulation. Behavior cloning (BC) approaches leverage huge quantities of recorded human driving sessions to learn the input-output driving mapping by imitation. In this setting, the network is trained to mimic the commands applied by the expert driver (end-to-end models), or the future trajectory (end-to-mid models), in a supervised fashion. An initial attempt to behavior cloning of vehicle controls was made by (Pomerleau, 1988), and continued later in (Bojarski et al., 2016). | Reinforcement learning (RL) was alternatively explored by researchers to train neural driving systems. This paradigm learns a policy by balancing self-exploration and reinforcement. This training paradigm relies on a simulator (such as CARLA (Dosovitskiy et al., 2017)). | . | . The challenges of explainability of neural driving systems . Introducing explainability in the design of learning-based self-driving systems is a challenging task. These concerns arise from two aspects: . From a Deep Learning perspective, explainability hurdles of self-driving models are shared with most deep learning models, across many application domains. Indeed, decisions of deep systems are intrinsically hard to explain as the functions these systems represent, mapping from inputs to outputs, are not transparent. In particular, although it may be possible for an expert to broadly understand the structure of the model, the parameter values, which have been learned, are yet to be explained. Several factors cause interpretability issues for self-driving machine learning models. First, a finite training dataset cannot exhaustively cover all possible driving situations. It will likely under- and over-represent some specific cases, and questions such as Has the model encountered situations like X? are legitimate. Moreover, datasets contain numerous biases of various nature (omitted variable bias, cause-effect bias, sampling bias), which also gives rise to explainability issues related to fairness. Second, the mapping function represented by the trained model is poorly understood and is considered as a black-box. The model is highly non-linear and does not provide any robustness guarantee as small input changes may dramatically change the output behavior. Explainability issues thus occur regarding the generalizability and robustness aspects: How will the model behave under these new scenarios? Third, the learning phase is not perfectly understood. Among other things, there are no guarantees that the model will settle at a minimum point that generalizes well to new situations. Thus, the model may learn to ground its decisions on spurious correlations during training instead of on the true causes. We aim at finding answers to questions like Which factors caused this decision to be taken? | . . Figure 2. Explainability hurdles and questions for autonomous driving models, as seen from a machine learning point of view. From a driving perspective, it has been shown that humans tackle this task by solving many intermediate sub-problems, at different levels of hierarchy (Michon, 1984). In the effort towards building an autonomous driving system, researchers aim at providing the machine with these intermediate capabilities. Thus, explaining the general behavior of an autonomous vehicle inevitably requires understanding how each of these intermediate steps is carried and how it interacts with others. We can categorize these capabilities into three types: Perception: information about the system’s understanding of its local environment. This includes the objects that have been recognized and assigned to a semantic label (persons, cars, urban furniture, driveable area, crosswalks, traffic lights), their localization, properties of their motion (velocity, acceleration), intentions of other agents, etc.; | Reasoning: information about how the different components of the perceived environment are organized and assembled by the system. This includes global explanations about the rules that are learned by the model, instance-wise explanation showing which objects are relevant in a given scene, traffic pattern recognition, object occlusion reasoning, etc.; | Decision: information about how the system processes the perceived environment and its associated reasoning to produce a decision. This decision can be a high-level goal stating that the car should turn right, a prediction of the ego vehicle’s trajectory, its low-level relative motion or even the raw controls, etc. | . | . . Figure 3. Explainability hurdles and questions for autonomous driving models, as seen from an autonomous driving point of view. While the separation between perception, reasoning, and decision is clear in modular driving systems, some recent end-to-end neural networks such as PilotNet (Bojarski et al., 2016) blur the lines and perform these simultaneously. Indeed, when an explanation method is developed for a neural driving system, it is often not clear whether it attempts to explain the perception, the reasoning, or the decision step. Considering the nature of neural networks architecture and training, disentangling perception, reasoning, and decision in neural driving systems constitutes a non-trivial challenge. . Conclusion . As an answer to such problems, many explanation methods have been proposed and are usually organized into two categories: applying post-hoc methods on an already-trained driving model, and directly building driving models which are inherently interpretable by design. In our survey, we provide details on existing explainability techniques, show how they tackle to the problem of explaining driving models and highlight their limitations. In addition, we detail remaining challenges and open research avenues to increase explainability of self-driving models. We hope our survey will enable increased awareness in this area from researchers and practitioners in the field, as well as from other potentially related fields. . References . Rosenfeld, A., &amp; Richardson, A. (2019). Explainability in human-agent systems. Auton. Agents Multi Agent Syst. | Doshi-Velez, F., &amp; Kortz, M. A. (2017). Accountability of AI Under the Law: The Role of Explanation. CoRR. | Beaudouin, V., Bloch, I., Bounie, D., Clémençon, S., d’Alché-Buc, F., Eagan, J., Maxwell, W., Mozharovskyi, P., &amp; Parekh, J. (2020). Flexible and Context-Specific AI Explainability: A Multidisciplinary Approach. CoRR. | Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., &amp; Kagal, L. (2018). Explaining Explanations: An Overview of Interpretability of Machine Learning. DSSA. | Zhang, Q., Yang, X. J., &amp; Robert, L. P. (2020). Expectations and Trust in Automated Vehicles. CHI. | Haspiel, J., Du, N., Meyerson, J., Jr., L. P. R., Tilbury, D. M., Yang, X. J., &amp; Pradhan, A. K. (2018). Explanations and Expectations: Trust Building in Automated Vehicles. HRI. | Du, N., Haspiel, J., Zhang, Q., Tilbury, D., Pradhan, A. K., Yang, X. J., &amp; Robert Jr, L. P. (2019). Look who’s talking now: Implications of AV’s explanations on driver’s trust, AV preference, anxiety and mental workload. Transportation Research Part C: Emerging Technologies. | Yurtsever, E., Lambert, J., Carballo, A., &amp; Takeda, K. (2020). A Survey of Autonomous Driving: Common Practices and Emerging Technologies. IEEE Access. | Chen, C., Seff, A., Kornhauser, A. L., &amp; Xiao, J. (2015). DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving. ICCV. | Toromanoff, M., Émilie Wirbel, &amp; Moutarde, F. (2020). End-to-End Model-Free Reinforcement Learning for Urban Driving Using Implicit Affordances. CVPR. | Pomerleau, D. (1988). ALVINN: An Autonomous Land Vehicle in a Neural Network. NIPS. | Bojarski, M., Testa, D. D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., Jackel, L. D., Monfort, M., Muller, U., Zhang, J., Zhang, X., Zhao, J., &amp; Zieba, K. (2016). End to End Learning for Self-Driving Cars. CoRR. | Dosovitskiy, A., Ros, G., Codevilla, F., López, A., &amp; Koltun, V. (2017). CARLA: An Open Urban Driving Simulator. CoRL. | Michon, J. A. (1984). A Critical View of Driver Behavior Models: What Do We Know, what Should We Do? Human behavior and traffic safety. |",
            "url": "https://valeoai.github.io/blog/2021/02/18/explainable-driving.html",
            "relUrl": "/2021/02/18/explainable-driving.html",
            "date": " • Feb 18, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving",
            "content": "This post describes our recent work on probabilistic trajectory prediction for autonomous driving presented at CORL 2020. PLOP is a trajectory prediction method that intent to control an autonomous vehicle (ego vehicle) in urban environment while considering and predicting the intents of other road users (neighbors). We focus here on predicting multiple feasible future trajectories for both ego vehicle and neighbors through a probabilistic framework and rely on a conditional imitation learning algorithm, conditioned by a navigation command for the ego vehicle (e.g., ``turn right’’). Our model processes only onboard sensor data (camera and lidars) along with detections of past and presents objects relaxing the necessity of an HDMap and is computationally efficient as it can run in real time (25 fps) on an embedded board in the real vehicle. We evaluate our method offline on the publicly available dataset nuScenes (Caesar et al., 2020), achieving state-of-the-art performance, investigate the impact of our architecture choices on online simulated experiments and show preliminary insights for real vehicle control. . . Figure 1. Qualitative example of trajectory predictions on a test sample from nuScenes dataset. The top image show a bird&#39;s eye view of PLOP&#39;s predictions for the ego and neighbor vehicles (to be compared with the ground truth in green). The bottom row present the input image (left) in which we added object correspondance with the bird&#39;s eye view and the auxiliary semantic segmentation of this image (right) Predicting the future positions of other agents of the road, or of the autonomous vehicle itself, is critical for autonomous driving. This trajectory prediction must not only respect the rules of the road, but capture the interactions of the agents over time. It is also important to allow multiple possible predictions, as there is usually not a single valid trajectory. . Some approaches such as ChauffeurNet (Bansal et al., 2018) use a high-levelscene representation (road map, traffic lights, speed limit, route, dynamic bounding boxes, etc.). More recently, MultiPath (Chai et al., 2019) uses trajectory anchors, used in one-step object detection, extracted from the training data for ego vehicle prediction. (Hong et al., 2019) use a high level representation which includes some dynamic context. In contrast, we choose to leverage also low level sensor data, here Lidar point clouds and camera image. In that domain, recent approaches address the variation in agent behaviors by predicting multiple trajectories, often in a stochastic way. Many works, e.g., PRECOG (Rhinehart et al., 2019), MFP (Tang &amp; Salakhutdinov, 2019), SocialGAN (Gupta et al., 2018) and others (Rhinehart et al., 2018), focus on this aspect through a probabilistic framework on the network output or latent representations, producing multiple trajectories for ego vehicle, nearby vehicles or both. (Phan-Minh et al., 2020) generate a trajectory set, then classify correct trajectories. (Marchetti et al., 2020) generate multiple futures from encodings of similar trajectories stored in a memory. (Ohn-Bar et al., 2020) learn a weighted mixture of expert policies trained to mimic agents with specific behaviors. In PRECOG, (Rhinehart et al., 2019) advance a probabilistic formulation that explicitly models interactions between agents, using latent variables to model their plausible reactions, with the possibility to precondition the trajectory of the ego vehicle by a goal. . PLOP method . Contributions . Our main goal is to produce a trajectory prediction which can be used to drive the ego vehicle relying on a conditional imitation learning algorithm, conditioned by a navigation command for the ego vehicle (e.g., “turn right”). To do so, we propose a single-shot, anchor-less trajectory prediction method, based on Mixture Desity Networks (MDNs) and polynomial trajectory constraints, relying only on on-board sensors which relaxes the HD map requirement and allow more flexibility for driving in the real world. The polynomial formulation ensures that the predicted trajectories are coherent and smooth, while providing more learning flexibility through the extra parameters. We find that this mitigates training instability and mode collapse that are common to MDNs (Cui et al., 2019). PLOP is trainable end-to-end from imitation learning, where data is relatively easier to obtain and it is computationally efficient during both training and inference as it predicts trajectory coefficients in a single step, without requiring a RNN-based decoder. The polynomial function trajectory coefficients eschew the need for anchors (Chai et al., 2019), whose quality can vary across datasets. . We propose an extensive evaluation of PLOP and show its effectiveness across datasets and settings. We conduct a comparison showing the improvement over state-of-the-art PRECOG (Rhinehart et al., 2019) on the public dataset nuScenes (Caesar et al., 2020); . Then for a better evaluation of the driving capacities of PLOP, we study closed loop performance for the ego vehicle, on simulation and with preliminary insights for real vehicle control. . Network architecture . PLOP takes as inputs: the ego and neighbor vehicles past positions represented as time sequences of x and y over the last 2 seconds, the frontal camera image of the ego vehicle, and 2 second history of bird’s eye views with a cell resolution of 1m square containing the lidar point cloud and the object detections information represented in Figure 2. The objects detections being the output of a state of the art perception algorithm. . . Figure 2. Image and Bird&#39;s eye view. The left image is an example of a front camera input image of PLOP and the diagram on the right is a representation of the bird&#39;eye view input. We pass these inputs through a multibranch neural network represented in Figure 3 to predict the ego vehicle future trajectory and two auxiliary tasks that are the future trajectory prediction for the neighbors vehicles and the semantic segmentation of the camera image. . . Figure 3. PLOP&#39;s Architecture. PLOP&#39;s architecture is reprented on the left while the polynomial multimodal gaussian trajectory representation is on the right The front camera image features, the bird’s eye view features and the ego vehicle past positions features are passed down to conditional fully connected architecture to output multiple future trajectories for the ego vehicle regarding the current navigation order. The trajectories are predicted using MDNs where gaussian means are generated using polynomial functions of degree 4 over x and y . To improve the learning stability of our training and inject awareness about the scene layouts into the camera features we pass them through a U-Net decoder to output semantic segmentation and then use an auxiliary cross entropy loss. To improve the encoding of interactions between the differents agents of the scene in the bird’s eye features, we predict the future possible trajectories for each neighbor feeding the bird’s eye views encoding and its past positions encoded through a LSTM layer to a small fully connected network. The weights of LSTMs and fully connected layers are shared between all neighbors. This output allows us to get useful information about the ego vehicle environment that can be used online to improve the ego vehicle driving with safety collision checks for example. . Offline evaluation . To evaluate PLOP, we use the nuScenes dataset to train the trajectory loss along with the Audi (Geyer et al., 2019) dataset to train the semantic segmentation loss. We choose to compare our method with the DESIRE (Lee et al., 2017) baseline and against two state of the art methods that are PRECOG and ESP (Rhinehart et al., 2019) using the minimum Mean Squared Deviation metric to avoid penalizing valid trajectories that are not matching the ground truth. For one agent, meaning ego vehicle only, PRECOG and ESP have access to the future desired target position and PRECOG return significantly better results than PLOP but PLOP still reaches similar results as ESP. For multiple agents PLOP outperforms other presented methods . We note that the comparison if fairer for neighbor trajectories and the performance is relevant since they are by definition open loop. . . Figure 4. Comparison with state-of-the-art: Against the DESIRE, ESP and PRECOG for predicting a trajectory of 4 seconds into the future But we argue that such evaluation is not totally relevant for controling the ego vehicle in real conditions. Such metrics does not value the situations in which the errors are made, failing to brake at a traffic light is a critical error for example but it is quick and represent a very small part of the test set so it will impact very poorly the overall metrics. However, making a small constant error such as driving 2kph too slow over the whole test set set might be an acceptable and non impacting error but will lead to a considerable overall error. Also, using only offline metrics where the method can’t control the vehicle does not allow us to evaluate its capacities to react to its own mistakes. . Online Evaluation through simulation . To simulate driving, we developped a data driven simulator that allows us to use real driving data to simulate applying the prediction to the ego vehicle. We can generate the input data that corresponds to the new vehicle position after following the trajectory using reprojections (for the image and the pointcloud), then use it to predict a new trajectory, and so on. This allows us the measure the performance in closed loop, and in particular to count failures which would have resulted in a takeover. We rely on 3 metrics: lateral (&gt;1m from expert), high speed (catching up to a vehicle 15% faster than the real vehicle up to 0.6s in the future) and low speed (&gt; 20kph under the expert speed) errors count. . . Figure 5. Evaluation using the simulator. Comparison with PLOP without semantic segmentation loss, Constant velocity baseline and Multi-Layer Perceptron baseline in the table on the left. Additionnal qualitative results about the errors positioning on the differents test tracks are on the right. We trained PLOP on an internal dataset combining both open road and urban test track and compared PLOP, PLOP without auxiliary semantic loss, the constant velocity baseline and a MLP baseline in our simulator using test data. We note that semantic segmentation improve the driving performance and that MLP has better offline metrics than constant velocity approach but still perform worse due to the simulated driving conditions. As expected, offline metrics are not discriminating enough for the online behavior since the best model checkpoints in simulation are not necessarily the ones with the better offline metrics. An additionnal ablation study where we remove mandatory information (such as the camera image input) shows that it may even be dangerous to trust them blindly. . Conclusion . In this work, we demonstrate the interest of our multi-input multimodal approach PLOP for vehicle trajectory prediction in an urban environment. Our architecture leverages frontal camera and Lidar inputs, to produce multiple trajectories using reparameterized Mixture Density Networks, with an auxiliary semantic segmentation task. We show that we can improve open loop state-of-the-art performance in a multi-agent system, by evaluating the vehicle trajectories from the nuScenes dataset. We also provide a simulated closed loop evaluation, to go towards real vehicle online application. Please check out our paper along with supplementary materials for greater details about our approach and experiments and feel free to contact us for any question. . References . Caesar, H., Bankiti, V., Lang, A. H., Vora, S., Liong, V. E., Xu, Q., Krishnan, A., Pan, Y., Baldan, G., &amp; Beijbom, O. (2020). nuScenes: A Multimodal Dataset for Autonomous Driving. Cvpr. | Bansal, M., Krizhevsky, A., &amp; Ogale, A. S. (2018). ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst. CoRR. | Chai, Y., Sapp, B., Bansal, M., &amp; Anguelov, D. (2019). MultiPath: Multiple Probabilistic Anchor Trajectory Hypotheses for Behavior Prediction. | Hong, J., Sapp, B., &amp; Philbin, J. (2019). Rules of the Road: Predicting Driving Behavior with a Convolutional Model of Semantic Interactions. CoRR. | Rhinehart, N., McAllister, R., Kitani, K., &amp; Levine, S. (2019). Precog: Prediction conditioned on goals in visual multi-agent settings. Iccv. | Tang, C., &amp; Salakhutdinov, R. R. (2019). Multiple futures prediction. Advances in Neural Information Processing Systems, 15424–15434. | Gupta, A., Johnson, J., Fei-Fei, L., Savarese, S., &amp; Alahi, A. (2018). Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks. CoRR. | Rhinehart, N., McAllister, R., &amp; Levine, S. (2018). Deep Imitative Models for Flexible Inference, Planning, and Control. CoRR. | Phan-Minh, T., Grigore, E. C., Boulton, F. A., Beijbom, O., &amp; Wolff, E. M. (2020). Covernet: Multimodal behavior prediction using trajectory sets. Cvpr. | Marchetti, F., Becattini, F., Seidenari, L., &amp; Bimbo, A. D. (2020). Mantra: Memory augmented networks for multiple trajectory prediction. Cvpr. | Ohn-Bar, E., Prakash, A., Behl, A., Chitta, K., &amp; Geiger, A. (2020). Learning Situational Driving. Cvpr. | Cui, H., Radosavljevic, V., Chou, F.-C., Lin, T.-H., Nguyen, T., Huang, T.-K., Schneider, J., &amp; Djuric, N. (2019). Multimodal trajectory predictions for autonomous driving using deep convolutional networks. Icra. | Geyer, J., Kassahun, Y., Mahmudi, M., Ricou, X., Durgesh, R., Chung, A. S., Hauswald, L., Pham, V. H., Mühlegg, M., Dorn, S., Fernandez, T., Jänicke, M., Mirashi, S., Savani, C., Sturm, M., Vorobiov, O., &amp; Schuberth, P. (2019). A2D2: AEV Autonomous Driving Dataset. http://www.a2d2.audi | Lee, N., Choi, W., Vernaza, P., Choy, C., Torr, P., &amp; Chandraker, M. (2017). DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents. 2165–2174. https://doi.org/10.1109/CVPR.2017.233 |",
            "url": "https://valeoai.github.io/blog/2020/11/26/plop-trajectory-prediction.html",
            "relUrl": "/2020/11/26/plop-trajectory-prediction.html",
            "date": " • Nov 26, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Is Deep Reinforcement Learning Really Superhuman on Atari?",
            "content": "This post describes our recent work on Deep Reinforcement Learning (DRL) on the Atari benchmark. DRL appears today as one of the closest paradigm to Artificial General Intelligence and large progress in this field has been enabled by the popular Atari benchmark. However, training and evaluation protocols on Atari vary across papers leading to biased comparisons, difficulty in reproducing results and in estimating the true contributions of the methods. Here we attempt to mitigate this problem with SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms. SABER allows us to compare multiple methods under the same conditions against a human baseline and to note that previous claims of superhuman performance on DRL do not hold. Finally, we propose a new state-of-the-art algorithm R-IQN combining Rainbow with Implicit Quantile Networks (IQN). Our code is available. . Deep Reinforcement Learning is a learning scheme based on trial-and-error in which an agent learns an optimal policy from its own experiments and a reward signal. The goal of the agent is to maximize the sum of future accumulated rewards and thus the agent needs to think about sequences of actions rather than instantaneous ones. The Atari benchmark is valuable for evaluating general AI algorithms as it includes more than 50 games displaying high variability in the task to solve, ranging from simple paddle control in the ball game Pong to complex labyrinth exploration in Montezuma’s Revenge which remains unsolved by general algorithms up to today. . We notice however that training and evaluation procedures on Atari can be different from paper to paper and thus leading to bias in comparison. Moreover this leads to difficulties to reproduce results of published works as some training or evaluation parameters are barely explained or sometimes not mentioned. In order to facilitate reproducible and comparable DRL, we introduce SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms. Furthermore, we introduce a human world record baseline and argue that previous claims of superhuman performance of DRL might not hold. Finally, we propose a new state-of-the-art algorithm R-IQN by combining the current state-of-the-art Rainbow (Hessel et al., 2018) along with Implicit Quantile Networks (IQN (Dabney et al., 2018)). We release an open-source implementation of distributed R-IQN following the ideas from Ape-X! . DQN’s human baseline vs human world record on Atari Games . A common way to evaluate AI for games is to let agents compete against the best humans. Recent examples for DRL include the victory of AlphaGo versus Lee Sedol for Go, OpenAI Five on Dota 2 or AlphaStar versus Mana for StarCraft 2. For this reason one of the most used metrics for evaluating RL agents on Atari is to compare them to the human baseline introduced in DQN. . Previous works use the normalized human score, i.e., 0% is the score of a random player and 100% is the score of the human baseline, which allows one to summarize the performance on the whole Atari set in one number, instead of individually comparing raw scores for each of the 61 games. However we argue that this human baseline is far from being representative of the best human player, which means that using it to claim superhuman performance is misleading. . The current world records are available online for 58 of the 61 evaluated Atari games. For example, on VideoPinball, the world record is 50,000 times higher than the human baseline of DQN. Evaluating these world records scores using the usual human normalized score has a median of 4,400% and a mean of 99,300% (see Figure below for details on each game), to be compared to 200% and 800% of the current state-of-the-art Rainbow! . . Figure 1: World record scores vs. the usual beginner human baseline (log scale) (Dabney et al., 2018) baseline can be up to 50k times lower than registered world records. Beating that baseline does not necessarily make the agent superhuman. We estimate that evaluating the algorithms under the world record baseline instead of the DQN human baseline will give a better view of the gap remaining between best human players and DRL agents. Results are confirming this, as Rainbow reaches only a median human-normalized score of 3% (see Figure 2 below) meaning that for half of Atari games, the agent doesn’t even reach 3% of the way from random to best human run. . In the video below we analyze agents previously claimed as above human-level but far from the world record. By taking a closer look at the AI playing, we discovered that on most of Atari games DRL agents fail to understand the goal of the game. Sometimes they just don’t explore other levels than the initial one, sometimes they are stuck in a loop giving small amount of reward, etc. A compelling example of this is the game Riverraid. In this game, the agent must shoot everything and take fuel to survive: the agent dies if there is a collision with an enemy or if out of fuel. But as shooting fuel actually gives points, the agent doesn’t understand that he could play way longer and win even more points by actually taking this fuel bonus and not shooting them! . . SABER: a Standardized Atari BEnchmark for general Reinforcement learning algorithms . In our work we extend the recommendations proposed by (Machado et al., 2018) They also point out divergences in training and evaluating agents on Atari. Consequently they compile and propose a set of recommendations for more reproducible and comparable RL including sticky actions, ignoring life signal and using full action set. . There is however one major parameter that is left out in (Machado et al., 2018): the maximum number of frames allowed per episode. This parameter ends the episode after a fixed number of time steps even if the game is not over. In most of the recent works, this is set to 30 min of game play (i.e., 108k frames) and only to 5 min in some others (i.e.,18k frames). This means that the reported scores can not be compared fairly. For example, in easy games (e.g., Atlantis), the agent never dies and the score is more or less linear with the allowed time: the reported score will be 6 times higher if capped at 30 minutes instead of 5 minutes. . Another issue with this time cap comes from the fact that some games are designed for much longer gameplay than 5 or 30 minutes. On those games (e.g., Atlantis, Video Pinball, Enduro) the scores reported of Ape-X, Rainbow and IQN are almost the same. This is due to all agents reaching the time limit and getting the maximum possible score in 30 minutes: the difference in scores is due to minor variations, not algorithmic difference and thus the comparison is not significant. As a consequence, the more successful agents are, the more games are incomparable because they reach the maximum possible score in the time cap while still being far behind human world record. . This parameter can also be a source of ambiguity and error. The best score on Atlantis (2,311,815) is reported by Proximal Policy Optimization by (Schulman et al., 2017), however this score is likely wrong: it seems impossible to reach in 30 minutes only! The first distributional paper by (Bellemare et al.) also did this mistake and reported wrong results before adding an erratum in a later version on ArXiv (Bellemare et al., 2017). . . Table 1: Game parameters of SABER. We first re-implemented the current state-of-the-art Rainbow and evaluated it on SABER. We noticed that the same algorithm under different evaluation settings can lead to significantly different results. This showed again the necessity of a common and standardized benchmark, more details can be found in the paper. . Then we implemented a new algorithm, R-IQN, by replacing the C51 algorithm (which is one of the 6 components of Rainbow) by Implicit Quantile Network (IQN). Both C51 and IQN belong to the field of Distributional RL which aims to predict the full distribution of the Q-value function instead of just predicting the mean of it. The fundamental difference between these two algorithms is how they parametrize the Q-value distribution. C51, which is the first algorithm of Distributional RL, approximates the Q-value as a categorical distribution with fixed support and just learns the mass to attribute to each category. On the other hand, IQN approximates the Q-value with quantile regression and both the support and the mass arelearned resulting in a major improvement in performance and data-efficiency over C51. As IQN arises much better performance than C51 while still designed for the same goal (predict the full distribution of the Q-function), combining Rainbow with IQN is relevant and natural. . As shown in the graph below, R-IQN outperforms Rainbow and thus becomes the new state-of-the-art on Atari. However, we acknowledge that in order to make a more confident state-of-the-art claim we should run multiple times with different seeds. Testing an increased number of random seeds across the 60 Atari games is a computationally costly endeavor and is beyond the scope of this study. We test the stability of the performances of R-IQN across 5 random seeds on a subset of 14 games. We compare against Rainbow and report similar results in Figure 3. . . Figure 2: Comparison of Rainbow and Rainbow-IQN on SABER. We report median normalized scores w.r.t training steps. . Figure 3: Comparison of Rainbow and Rainbow-IQN on a subset of 14 games using 5 seeds. We report median normalized scores w.r.t training steps. Open-source implementation of distributed Rainbow-IQN: R-IQN Ape-X . We release our code of a distributed version of Rainbow-IQN following ideas from Ape-X (Horgan et al., 2018). The distributed part is our main practical advantage over some existing DRL repositories (particularly Dopamine a popular open-source implementation of DQN, C51, IQN and a small-Rainbow but in which all algorithms are single worker). Indeed, when using DRL algorithms for other tasks (other than Atari and MuJoCO) a major bottleneck is the speed of the environment. DRL algorithms often need a huge amount of data before reaching reasonable performance. This amount may be practically impossible to reach if the environment is real-time and if collecting data from multiple environments at the same time is not possible. . Building upon ideas from Ape-X, we use REDIS as a side server to store our replay memory. Multiple actors will act in their own instances of the environment to fill as fast as they can the replay memory. Finally, the learner will sample from this replay memory (the learner is actually completely independent of the environment) for backprop. The learner will also periodically update the weight of each actor as shown in the schema below. . . Figure 4: Ape-X architecture. image taken from (Horgan et al., 2018) This scheme allowed us to use R-IQN Ape-X for the task of autonomous driving using CARLA as environment. This enabled us to win the CARLA Challenge on Track 2 Cameras Only showing the strength of R-IQN Ape-X as a general algorithm. . Conclusion . In this work, we confirm the impact of standardized guidelines for DRL evaluation, and build a consolidated benchmark, SABER. In order to provide a more significant comparison, we build a new baseline based on human world records and show that the state-of-the-art Rainbow agent is in fact far from human world record performance. In the paper we share possible reasons for this failure. We hope that SABER will facilitate better comparisons and enable new exciting methods to prove their effectiveness without ambiguity. . Check out our paper to find out more about intuitions, experiments and interpretations. . References . Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., &amp; Silver, D. (2018). Rainbow: Combining improvements in deep reinforcement learning. AAAI. | Dabney, W., Ostrovski, G., Silver, D., &amp; Munos, R. (2018). Implicit quantile networks for distributional reinforcement learning. ICML. | Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., &amp; Bowling, M. (2018). Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61, 523–562. | Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017). Proximal policy optimization algorithms. ArXiv Preprint ArXiv:1707.06347. | Bellemare, M. G., Dabney, W., &amp; Munos, R. (2017). A distributional perspective on reinforcement learning. ArXiv Preprint ArXiv:1707.06887. | Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., Van Hasselt, H., &amp; Silver, D. (2018). Distributed prioritized experience replay. ArXiv Preprint ArXiv:1803.00933. |",
            "url": "https://valeoai.github.io/blog/2020/10/19/saber.html",
            "relUrl": "/2020/10/19/saber.html",
            "date": " • Oct 19, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "ADVENT - Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation",
            "content": "This post describes our recent work on unsupervised domain adaptation for semantic segmentation presented at CVPR 2019. ADVENT is a flexible technique for bridging the gap between two different domains through entropy minimization. Our work builds upon a simple observation: models trained only on source domain tend to produce over-confident, i.e., low-entropy, predictions on source-like images and under-confident, i.e., high-entropy, predictions on target-like ones. Consequently by minimizing the entropy on the target domain, we make the feature distributions from the two domains more similar. We show that our approach achieves competitive performances on standard semantic segmentation benchmarks and that it can be successfully extended to other tasks such as object detection. . Visual perception is a remarkable ability that human drivers leverage for understanding their surroundings and for supporting the multiple micro-decisions needed in traffic. Since many years, researchers have been working on mimicking this human capability by means of computer algorithms. This research field is known as computer vision and it has seen impressive progress and wide adoption. Most of the modern computer vision systems rely on Deep Neural Networks (DNNs) which are powerful and widely employed tools able to learn from large amounts of data and make accurate predictions. In autonomous driving, DNN-based visual perception is also at the heart of the complex architectures under intelligent cars, and supports downstream decisions of the vehicle, e.g., steering, braking, signaling, etc. . The diversity and complexity of the situations encountered in real-world driving is tremendous. Unlike humans who can extrapolate effortlessly from previous experience in order to adapt to new environments and conditions, the scope of DNNs beyond the types of conditions and scenes seen during training is limited. For instance a model trained on data from a sunny country, would have a hard time delivering the same performance on streets with mixed weather conditions in a different country (with different urban architecture, furniture, vegetation, types of cars and pedestrian appearance and clothing). Similarly a model trained on a particular type of camera, is expected to see a drop in performance with images coming from a camera with different specifications. This difference between environments that leads to performance drops is referred to as domain gap. . Bridging domains . We can resort to two options for narrowing the domain gap: (i) annotate more data; (ii) leverage the experience acquired on an initial environment and transfer it to the new environment. More annotated data has been shown to always improve performance of DNNs (Sun et al., 2017). However the labeling process brings a significant financial and temporal burden. The time required for a high-quality annotation, such as the ones from the popular Cityscapes dataset is ∼90 minutes per image (Cordts et al., 2016). The amount of images required to train high performance DNNs typically counts in hundreds of thousands. The acquisition of diverse data across seasons and weather conditions adds up even more time. It makes then sense to look for a solution elsewhere and the second option seems now more appealing, though achieving it remains technically challenging. This is actually the area of research of domain adaptation (DA) which addresses the domain-gap problem by transferring knowledge from a source domain (with full annotations) to a target domain (with fewer annotations if any), aiming to reach good performances on target samples. DA has consistently attracted interest from different communities across years (Csurka, 2017). . Here we are working on Unsupervised DA (UDA), which is a more challenging task where we have access to labeled source samples and only unlabeled target samples. We use as source, data generated by a simulator or video game engine, while for target we consider real-data from car-mounted cameras. In Figure 1 we illustrate the difficulty of this task and the impact of our UDA technique, ADVENT. . . Figure 1. Proposed entropy-based unsupervised domain adaptation for semantic segmentation. The top two rows show results on source and target domain scenes of the model trained without adaptation. The bottom row shows the result on the same target domain scene of the model trained with entropy-based adaptation. The left and right columns visualize respectively the semantic segmentation outputs and the corresponding prediction entropy maps. The main approaches for UDA include discrepancy minimization between source and target feature distributions usually achieved via adversarial training (Ganin &amp; Lempitsky, 2015), (Tzeng et al., 2017), self-training with pseudo-labels (Zou et al., 2018) and generative approaches (Hoffman et al., 2018), (Wu et al., 2018). . Entropy minimization has been shown to be useful for semi-supervised learning (Grandvalet &amp; Bengio, 2005), clustering (Jain et al., 2018) and more recently to domain adaptation for classification (Long et al., 2016). We chose to explore entropy based UDA training to obtain competitive performance on semantic segmentation. . Approach . We present our two proposed approaches for entropy minimization using (i) an unsupervised entropy loss and (ii) adversarial training. To build our models, we start from existing semantic segmentation frameworks and add an additional network branch used for domain adaptation. Figure 2 illustrates our architectures. . . Figure 2. Approach overview. First, direct entropy minimization decreases the entropy of the target $P_{x_t}$, which is equivalent to minimizing the sum of weighted self-information maps $I_{x_t}$. In the second approach, we use adversarial training to enforce the consistency in $P_x$ across domains. Red arrows are used for target domain, blue arrows for source. Direct entropy minimization . On the source domain we train our model, denoted as $F$, as usual using a supervised loss. For the target domain, we do not have annotations and we can no longer use the segmentation loss to train $F$. We notice that models trained only on source domain tend to produce over-confident predictions on source-like images and under-confident predictions on target-like ones. Motivated by this observation, we propose a supervision signal that could leverage visual information from the target samples, in spite of the lack of annotations. The objective is to constrain $F$ to produce high-confident predictions on target samples similarly to source samples. To this effect, we introduce the entropy loss $ mathcal{L}_{ent}$​ to maximize directly the prediction confidence in the target domain. Here we consider the Shannon Entropy (Shannon). During training, we jointly optimize the supervised segmentation loss $ mathcal{L}_{seg}$ on source samples and the unsupervised entropy loss $ mathcal{L}_{ent}$​​ on target samples. . Entropy minimization by adverarial learning . A limitation of the entropy loss is related to the absence of structural dependencies between local semantics. This is caused by the aggregation of the pixel-wise prediction entropies by summation. We address this through a unified adversarial training framework which minimizes indirectly the entropy of target data, by encouraging it to become similar to the source one. This allows the exploitation of the structural consistency between the domains. To this end, we formulate the UDA task as minimizing distribution distance between source and target on the weighted self-information space. Since the trained model produces naturally low-entropy predictions on source-like images, by aligning weighted self-information distributions of target and source domains, we reach the same behavior on target-like data. . We perform the adversarial adaptation on weighted self-information maps using a fully-convolutional discriminator network $D$. The discriminator produces domain classification outputs, i.e., class label $1$ (resp. $0$) for the source (resp. target) domain. We train $D$ to discriminate outputs coming from source and target images, and at the same time, train the segmentation network to fool the discriminator. . Experiments . We evaluate our approaches on the challenging synthetic-2-real unsupervised domain adaptation set-ups. Models are trained on fully-annotated synthetic data and are validated on real-world data. In such set-ups, the models have access to some unlabeled real images during training. . Semantic Segmentation . To train our models, we use either GTA5 (Richter et al., 2016) or SYNTHIA (Ros et al., 2016) as source synthetic data, along with the training split of Cityscapes dataset (Cordts et al., 2016) as target domain data. . In Table 1 we report our results on semantic segmentation from models trained on GTA5 $ rightarrow$ Cityscapes and from SYNTHIA $ rightarrow$ Cityscapes. We compare here only with the top performing method Adapt-SegMap (Tsai et al., 2018), while additional baselines and related methods are covered in the paper. . . Table 1. Segmentation performance in mIoU with ResNet-101 based model and Deeplab-V2 as the segmentation network.We show results of our approaches using the direct entropy loss (MinEnt) and using adversarial training (AdvEnt). Our first approach of direct entropy minimization (MinEnt) achieves comparable performance to state-of-the-art baselines. The light overhead of the entropy loss makes training time shorter for the MinEnt model, while being easier train compared to adversarial networks. Our second approach using adversarial training on the weighted self-information space, noted as AdvEnt, shows consistent improvement to the baselines. In general, AdvEnt works better than MinEnt, confirming the importance of structural adaptation. The two approaches are complementary as their combination boosts performance further. . In Figure 3, we illustrate a few qualitative results of our models. Without domain adaptation, the model trained only on source supervision produces noisy segmentation predictions as well as high entropy activations, with a few exceptions on some classes like “building” and “car”. However, there are many confident predictions (low entropy) which are completely wrong. Our models, on the other hand, manage to produce correct predictions at high level of confidence. . . Figure 3. Segmentation and detection qualitative results. Segmentation on Cityscapes validation set with ResNet-101 + DeepLab-V2; Detection on Cityscapes-foggy with VGG-16 as the backbone and SSD. UDA for object detection . The proposed entropy-based approaches are not limited to semantic segmentation and can be applied to UDA for other recognition tasks, e.g. object detection. We conducted experiments in the UDA object detection set-up Cityscapes $ rightarrow$ Cityscapes-Foggy, similar to the one in (Chen et al., 2018). We report quantitative results in Table 2 and qualitative ones in Figure 3. In spite of the unfavorable factors, our improvement over the baseline ($+11.5 %$ mAP using AdvEnt) is larger than the one reported in (Chen et al., 2018) ($+8.8 %$). Additional experiments and implementation details can be found in the paper. These encouraging preliminary results suggest the feasibility of applying entropy-based approached on UDA for detection. . . Table 2. Object detection performance on Cityscapes Foggy. Conclusion . In this work, we propose two approaches for unsupervised domain adaptation reaching state-of-the-art performances on standard synthetic-2-real benchmarks. Interestingly the method can be easily extended to UDA for object detection with promising preliminary results. Check out our paper to find out more about intuitions, experiments and implementation details for AdvEnt and try out our code. . References . Sun, C., Shrivastava, A., Singh, S., &amp; Gupta, A. (2017). Revisiting unreasonable effectiveness of data in deep learning era. Proceedings of the IEEE International Conference on Computer Vision, 843–852. | Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., &amp; Schiele, B. (2016). The cityscapes dataset for semantic urban scene understanding. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3213–3223. | Csurka, G. (2017). Domain adaptation in computer vision applications. 8. | Ganin, Y., &amp; Lempitsky, V. (2015). Unsupervised domain adaptation by backpropagation. International Conference on Machine Learning, 1180–1189. | Tzeng, E., Hoffman, J., Saenko, K., &amp; Darrell, T. (2017). Adversarial discriminative domain adaptation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 7167–7176. | Zou, Y., Yu, Z., Vijaya Kumar, B. V. K., &amp; Wang, J. (2018). Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. Proceedings of the European Conference on Computer Vision (ECCV), 289–305. | Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y., Isola, P., Saenko, K., Efros, A., &amp; Darrell, T. (2018). Cycada: Cycle-consistent adversarial domain adaptation. International Conference on Machine Learning, 1989–1998. | Wu, Z., Han, X., Lin, Y.-L., Gokhan Uzunbas, M., Goldstein, T., Nam Lim, S., &amp; Davis, L. S. (2018). Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation. Proceedings of the European Conference on Computer Vision (ECCV), 518–534. | Grandvalet, Y., &amp; Bengio, Y. (2005). Semi-supervised learning by entropy minimization. Advances in Neural Information Processing Systems, 529–536. | Jain, H., Zepeda, J., Pérez, P., &amp; Gribonval, R. (2018). Learning a complete image indexing pipeline. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 4933–4941. | Long, M., Zhu, H., Wang, J., &amp; Jordan, M. I. (2016). Unsupervised domain adaptation with residual transfer networks. Advances in Neural Information Processing Systems, 136–144. | Richter, S. R., Vineet, V., Roth, S., &amp; Koltun, V. (2016). Playing for data: Ground truth from computer games. European Conference on Computer Vision, 102–118. | Ros, G., Sellart, L., Materzynska, J., Vazquez, D., &amp; Lopez, A. M. (2016). The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3234–3243. | Tsai, Y.-H., Hung, W.-C., Schulter, S., Sohn, K., Yang, M.-H., &amp; Chandraker, M. (2018). Learning to adapt structured output space for semantic segmentation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 7472–7481. | Chen, Y., Li, W., Sakaridis, C., Dai, D., &amp; Van Gool, L. (2018). Domain adaptive faster r-cnn for object detection in the wild. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3339–3348. |",
            "url": "https://valeoai.github.io/blog/2020/07/07/advent-domain-adaptation.html",
            "relUrl": "/2020/07/07/advent-domain-adaptation.html",
            "date": " • Jul 7, 2020"
        }
        
    
  

  
  
      ,"page0": {
          "title": "Projects",
          "content": "Projects . Multi-sensor perception . Automated driving relies first on a diverse range of sensors, like Valeo’s fish-eye cameras, LiDARs, radars and ultrasonics. Exploiting at best the outputs of each of these sensors at any instant is fundamental to understand the complex environment of the vehicle and gain robustness. To this end, we explore various machine learning approaches where sensors are considered either in isolation (as radar in Carrada at ICPR’20) or collectively (as in xMUDA at CVPR’20). . . 3D perception . Each sensor delivers information about the 3D world around the vehicle. Making sense of this information in terms of drivable space and important objects (road users, curb, obstacles, street furnitures) in 3D is required for the driving system to plan and act in the safest and most confortable way. This encompasses several challenging tasks, in particular detection and segmentation of objects in point clouds as in FKAConv at ACCV’20. . . Frugal learning . Collecting diverse enough data, and annotating it precisely, is complex, costly and time-consuming. To reduce dramatically these needs, we explore various alternatives to fully-supervised learning, e.g, training that is unsupervised (as rOSD at ECCCV’20), self-supervised (as BoWNet at CVPR’20), semi-supervised, active, zero-shot (as ZS3 at NeurIPS’19) or few-shot. We also investigate training with fully-synthetic data (in combination with unsupervised domain adaptation) and with GAN-augmented data (as Semantic Palette at CVPR’21). . . Domain adaptation . Deep learning and reinforcement learning are key technologies for autonomous driving. One of the challenges they face is to adapt to conditions which differ from those met during training. To improve systems’ performance in such situations, we explore so-called “domain adaptation” techniques, as in AdvEnt at CVPR’19 and DADA its extension at ICCV’19. We propose new solutions to more practical DA scenarios in MTAF (ICCV&#39;21) to handle multiple target domains and in BUDA (CVIU&#39;21) to handle new target classes. In xMUDA (CVPR&#39;20), we introduce a new framework to tackle the challenging adaptation problem on both 2D image and 3D point-cloud spaces. . . Reliability . When the unexpected happens, when the weather badly degrades, when a sensor gets blocked, the embarked perception system should diagnose the situation and react accordingly, e.g., by calling an alternative system or the human driver. With this in mind, we investigate ways to improve the robustness of neural nets to input variations, including to adversarial attacks, and to predict automatically the performance and the confidence of their predictions as in ConfidNet at NeurIPS’19. . . Driving in action . Getting from sensory inputs to car control goes either through a modular stack (perception &gt; localization &gt; forecast &gt; planning &gt; actuation) or, more radically, through a single end-to-end model. We work on both strategies, more specificaly on action forecasting, automatic interpretation of decisions taken by a driving system, and reinforcement / imitation learning for end-to-end systems (as in RL work at CVPR’20). . . Core Deep Learning . Deep learning being now a key component of AD systems, it is important to get a better understanding of its inner workings, in particular the link between the specifics of the learning optimization and the key properities (performance, regularity, robustness, generalization) of the trained models. Among other things, we investigate the impact of popular batch normalization on standard learning procedures and the ability to learn through unsupervised distillation. .",
          "url": "https://valeoai.github.io/blog/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
      ,"page1": {
          "title": "Publications",
          "content": "Selected publications . 2022 . STEEX: Steering Counterfactual Explanations with Semantics . Paul Jacob, Éloi Zablocki, Hédi Ben-Younes, Mickaël Chen, Patrick Pérez, Matthieu Cord European Conference on Computer Vision, 2022 . . Explainability of deep vision-based autonomous driving systems: Review and challenges . Éloi Zablocki*, Hédi Ben-Younes*, Patrick Pérez, Matthieu Cord International Journal of Computer Vision, 2022 . . POCO: Point Convolution for Surface Reconstruction . Alexandre Boulch and Renaud Marlet Computer Vision and Pattern Recognition (CVPR), 2022 . . Multi-Head Distillation for Continual Unsupervised Domain Adaptation in Semantic Segmentation . Antoine Saporta, Arthur Douillard, Tuan-Hung Vu, Patrick Pérez and Matthieu Cord Computer Vision and Pattern Recognition (CVPR) Workshop, 2022 . . Diverse Probabilistic Trajectory Forecasting with Admissibility Constraints . Laura Calem, Hedi Ben-Younes, Patrick Pérez, Nicolas Thome International Conference on Pattern Recognition, 2022 . . Raw High-Definition Radar for Multi-Task Learning . Julien Rebut, Arthur Ouaknine, Waqas Malik, and Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2022 . . CSG0: Continual Urban Scene Generation with Zero Forgetting . Himalaya Jain (*), Tuan-Hung Vu (*), Patrick Pérez and Matthieu Cord (* equal contrib.) Computer Vision and Pattern Recognition (CVPR) Workshop, 2022 . . Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data . Corentin Sautier, Gilles Puy, Spyros Gidaris, Alexandre Boulch, Andrei Bursuc, and Renaud Marlet Computer Vision and Pattern Recognition (CVPR), 2022 . . Cross-modal Learning for Domain Adaptation in 3D Semantic Segmentation . Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Émilie Wirbel, and Patrick Pérez IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022 . 2021 . Large-Scale Unsupervised Object Discovery . Huy V. Vo, Elena Sizikova, Cordelia Schmid, Patrick Pérez and Jean Ponce Advances in Neural Information Processing Systems (NeurIPS), 2021 . . Handling new target classes in semantic segmentation with domain adaptation . Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick Pérez Computer Vision and Image Understanding (CVIU), 2021 . . Localizing Objects with Self-Supervised Transformers and no Labels . Oriane Siméoni, Gilles Puy, Huy V. Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Renaud Marlet, Jean Ponce British Machine Vision Conference (BMVC), 2021 . . Generative Zero-Shot Learning for Semantic Segmentation of 3D Point Clouds . Bjoern Michele, Alexandre Boulch, Gilles Puy, Maxime Bucher, and Renaud Marlet International Conference on 3D Vision (3DV), 2021 . . Driving behavior explanation with multi-level fusion . Hédi Ben-Younes*, Éloi Zablocki*, Patrick Pérez, Matthieu Cord Pattern Recognition (PR), 2021 . . NeeDrop: Unsupervised Shape Representation from Sparse Point Clouds using Needle Dropping . Alexandre Boulch, Pierre-Alain Langlois, Gilles Puy, and Renaud Marlet International Conference on 3D Vision (3DV), 2021 . . Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation . Victor Besnier, Andrei Bursuc, Alexandre Briot, and David Picard International Conference on Computer Vision (ICCV), 2021 . . Multi-View Radar Semantic Segmentation . Arthur Ouaknine, Alasdair Newson, Patrick Pérez, Florence Tupin and Julien Rebut International Conference on Computer Vision (ICCV), 2021 . . Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation . Antoine Saporta, Tuan-Hung Vu, Matthieu Cord and Patrick Pérez International Conference on Computer Vision (ICCV), 2021 . . PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds . Anh-Quan Cao, Gilles Puy, Alexandre Boulch, and Renaud Marlet International Conference on Computer Vision (ICCV), 2021 . . StyleLess layer: Improving robustness for real-world driving . Julien Rebut, Andrei Bursuc, and Patrick Pérez IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021 . . Confidence Estimation via Auxiliary Models . Charles Corbière, Nicolas Thome, Antoine Saporta, Tuan-Hung Vu, Matthieu Cord, and Patrick Pérez IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021 . . Semantic Palette: Guiding Scene Generation with Class Proportions . Guillaume Le Moing, Tuan-Hung Vu, Himalaya Jain, Patrick Pérez and Matthieu Cord Computer Vision and Pattern Recognition (CVPR), 2021 . . Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning . Spyros Gidaris, Andrei Bursuc, Gilles Puy, Nikos Komodakis, Patrick Pérez, and Matthieu Cord Computer Vision and Pattern Recognition (CVPR), 2021 . . Artificial Dummies for Urban Dataset Augmentation . Antonín Vobecký, David Hurych, Michal Uřičář, Patrick Pérez, and Josef Šivic AAAI Conference on Artificial Intelligence (AAAI), 2021 . 2020 . FKAConv: Feature-Kernel Alignment for Point Cloud Convolutiont . Alexandre Boulch, Gilles Puy, and Renaud Marlet Asian Conference on Computer Vision (ACCV), 2020 . . CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations . Arthur Ouaknine, Alasdair Newson, Julien Rebut, Florence Tupin and Patrick Pérez International Conference on Pattern Recognition (ICPR), 2020 . . STaRFlow: A SpatioTemporal Recurrent Cell for Lightweight Multi-Frame Optical Flow Estimation . Pierre Godet, Alexandre Boulch, Aurelien Plyer and Guy Le Besnerais International Conference on Pattern Recognition (ICPR), 2020 . . PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving . Thibault Buhet, Emilie Wirbel, Andrei Bursuc and Xavier Perrotton Conference on Robot Learning (CoRL), 2020 . . Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems . Isabelle Leang, Ganesh Sistu, Fabian Burger, Andrei Bursuc, and Senthil Yogamani IEEE International Conference on Intelligent Transportation Systems (ITSC), 2020 . . TRADI: Tracking deep neural network weight distributions . Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson, and Isabelle Bloch European Conference on Computer Vision (ECCV), 2020 . . FLOT: Scene Flow on Point Clouds guided by Optimal Transport . Gilles Puy, Alexandre Boulch, and Renaud Marlet European Conference on Computer Vision (ECCV), 2020 . . Toward Unsupervised, Multi-Object Discovery in Large-Scale Image Collections . Huy V. Vo, Patrick Pérez and Jean Ponce European Conference on Computer Vision (ECCV), 2020 . . QuEST: Quantized Embedding Space for Transferring Knowledge . Himalaya Jain, Spyros Gidaris, Nikos Komodakis, Patrick Pérez, and Matthieu Cord European Conference on Computer Vision (ECCV), 2020 . . End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances . Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde Computer Vision and Pattern Recognition (CVPR), 2020 . . xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation . Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Émilie Wirbel, and Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2020 . . ESL: Entropy-guided Self-supervised Learning for Domain Adaptation in Semantic Segmentation . Antoine Saporta, Tuan-Hung Vu, Matthieu Cord and Patrick Pérez Computer Vision and Pattern Recognition Workshop on Scalability in Autonomous Driving, 2020 . . Learning Representations by Predicting Bags of Visual Words . Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pérez, and Matthieu Cord Computer Vision and Pattern Recognition (CVPR), 2020 . . This dataset does not exist: training models from generated images . Victor Besnier, Himalaya Jain, Andrei Bursuc, Matthieu Cord, and Patrick Pérez International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2020 . . VRUNet: Multi-Task Learning Model for Intent Prediction of Vulnerable Road Users . Adithya Ranga, Filippo Giruzzi, Jagdish Bhanushali, Emilie Wirbel, Patrick Pérez, Tuan-Hung Vu, Xavier Perotton Electronic Imaging, 2020 . . ConvPoint: Continuous Convolutions for Point Cloud Processing . Alexandre Boulch Computers &amp; Graphics Journal, 2020 . 2019 . Zero-Shot Semantic Segmentation . Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick Pérez Neural Information Processing Systems (NeurIPS), 2019 . . Addressing Failure Prediction by Learning Model Confidence . Charles Corbière, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez Neural Information Processing Systems (NeurIPS), 2019 . . DADA: Depth-aware Domain Adaptation in Semantic Segmentation . Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Pérez International Conference on Computer Vision (ICCV), 2019 . . Boosting Few-Shot Visual Learning With Self-Supervision . Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pérez, and Matthieu Cord International Conference on Computer Vision (ICCV), 2019 . . Unsupervised Image Matching and Object Discovery as Optimization . Huy V. Vo, Francis Bach, Minsu Cho, Kai Han, Yann Lecun, Patrick Pérez and Jean Ponce Computer Vision and Pattern Recognition (CVPR), 2019 . . AdvEnt: Adversarial Entropy minimization for domain adaptation in semantic segmentation . Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2019 .",
          "url": "https://valeoai.github.io/blog/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "About",
          "content": "Overview . valeo.ai is an international team based in Paris, conducting AI research for Valeo automotive applications, in collaboration with world-class academics. Our main research is towards better, clearer &amp; safer automotive AI. . You can find out more about our research through our papers, released code, tweets, and this blog. . Blog purpose . The aim of this blog is to provide an accessible, general-audience medium for valeo.ai researchers to communicate research publications and findings, as well as perspectives on the field. Posts are written by students and research scientists at valeo.ai. They are intended to provide insights and walk-throughs for our findings and results, both to experts and the general audience. . Translating Posts . If you wish to translate our blog posts, please contact the authors of the posts, as they own the copyright, and copy the editor, andrei.bursuc@valeo.com in your email. . Acknowledgments . This site is built with fastpages and hosted on Github. The design is based upon the Jekyll theme Minima. .",
          "url": "https://valeoai.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  

  

  
  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://valeoai.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  


  
  
      ,"page0": {
          "title": "Multi-sensor perception",
          "content": "Multi-sensor perception . Automated driving relies first on a diverse range of sensors, like Valeo’s fish-eye cameras, LiDARs, radars and ultrasonics. Exploiting at best the outputs of each of these sensors at any instant is fundamental to understand the complex environment of the vehicle and gain robustness. To this end, we explore various machine learning approaches where sensors are considered either in isolation (as radar in Carrada at ICPR’20) or collectively (as in xMUDA at CVPR’20). . Publications . Raw High-Definition Radar for Multi-Task Learning . Julien Rebut, Arthur Ouaknine, Waqas Malik, and Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2022 . . Cross-modal Learning for Domain Adaptation in 3D Semantic Segmentation . Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Émilie Wirbel, and Patrick Pérez IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022 . . Multi-View Radar Semantic Segmentation . Arthur Ouaknine, Alasdair Newson, Patrick Pérez, Florence Tupin and Julien Rebut International Conference on Computer Vision (ICCV), 2021 . . CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations . Arthur Ouaknine, Alasdair Newson, Julien Rebut, Florence Tupin and Patrick Pérez International Conference on Pattern Recognition (ICPR), 2020 . . PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving . Thibault Buhet, Emilie Wirbel, Andrei Bursuc and Xavier Perrotton Conference on Robot Learning (CoRL), 2020 . . Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems . Isabelle Leang, Ganesh Sistu, Fabian Burger, Andrei Bursuc, and Senthil Yogamani IEEE International Conference on Intelligent Transportation Systems (ITSC), 2020 . . xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation . Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Émilie Wirbel, and Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2020 .",
          "url": "https://valeoai.github.io/blog/projects/multi-sensor",
          "relUrl": "/projects/multi-sensor",
          "date": ""
      }
      
  

  
      ,"page1": {
          "title": "3D perception",
          "content": "3D perception . Each sensor delivers information about the 3D world around the vehicle. Making sense of this information in terms of drivable space and important objects (road users, curb, obstacles, street furnitures) in 3D is required for the driving system to plan and act in the safest and most confortable way. This encompasses several challenging tasks, in particular detection and segmentation of objects in point clouds as in FKAConv at ACCV’20. . Publications . POCO: Point Convolution for Surface Reconstruction . Alexandre Boulch and Renaud Marlet Computer Vision and Pattern Recognition (CVPR), 2022 . . Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data . Corentin Sautier, Gilles Puy, Spyros Gidaris, Alexandre Boulch, Andrei Bursuc, and Renaud Marlet Computer Vision and Pattern Recognition (CVPR), 2022 . . NeeDrop: Unsupervised Shape Representation from Sparse Point Clouds using Needle Dropping . Alexandre Boulch, Pierre-Alain Langlois, Gilles Puy, and Renaud Marlet International Conference on 3D Vision (3DV), 2021 . . Generative Zero-Shot Learning for Semantic Segmentation of 3D Point Clouds . Bjoern Michele, Alexandre Boulch, Gilles Puy, Maxime Bucher, and Renaud Marlet International Conference on 3D Vision (3DV), 2021 . . PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds . Anh-Quan Cao, Gilles Puy, Alexandre Boulch, and Renaud Marlet International Conference on Computer Vision (ICCV), 2021 . . STaRFlow: A SpatioTemporal Recurrent Cell for Lightweight Multi-Frame Optical Flow Estimation . Pierre Godet, Alexandre Boulch, Aurelien Plyer and Guy Le Besnerais International Conference on Pattern Recognition (ICPR), 2020 . . FKAConv: Feature-Kernel Alignment for Point Cloud Convolutiont . Alexandre Boulch, Gilles Puy, and Renaud Marlet Asian Conference on Computer Vision (ACCV), 2020 . . FLOT: Scene Flow on Point Clouds guided by Optimal Transport . Gilles Puy, Alexandre Boulch, and Renaud Marlet European Conference on Computer Vision (ECCV), 2020 . . ConvPoint: Continuous Convolutions for Point Cloud Processing . Alexandre Boulch Computers &amp; Graphics Journal, 2020 .",
          "url": "https://valeoai.github.io/blog/projects/3d-perception",
          "relUrl": "/projects/3d-perception",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Frugal learning",
          "content": "Frugal learning . Collecting diverse enough data, and annotating it precisely, is complex, costly and time-consuming. To reduce dramatically these needs, we explore various alternatives to fully-supervised learning, e.g, training that is unsupervised (as rOSD at ECCCV’20), self-supervised (as BoWNet at CVPR’20), semi-supervised, active, zero-shot (as ZS3 at NeurIPS’19) or few-shot. We also investigate training with fully-synthetic data (in combination with unsupervised domain adaptation) and with GAN-augmented data (as Semantic Palette at CVPR’21). . Publications . Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data . Corentin Sautier, Gilles Puy, Spyros Gidaris, Alexandre Boulch, Andrei Bursuc, and Renaud Marlet Computer Vision and Pattern Recognition (CVPR), 2022 . . CSG0: Continual Urban Scene Generation with Zero Forgetting . Himalaya Jain (*), Tuan-Hung Vu (*), Patrick Pérez and Matthieu Cord (* equal contrib.) Computer Vision and Pattern Recognition (CVPR) Workshop, 2022 . . Large-Scale Unsupervised Object Discovery . Huy V. Vo, Elena Sizikova, Cordelia Schmid, Patrick Pérez and Jean Ponce Advances in Neural Information Processing Systems (NeurIPS), 2021 . . Handling new target classes in semantic segmentation with domain adaptation . Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick Pérez Computer Vision and Image Understanding (CVIU), 2021 . . Localizing Objects with Self-Supervised Transformers and no Labels . Oriane Siméoni, Gilles Puy, Huy V. Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Renaud Marlet, Jean Ponce British Machine Vision Conference (BMVC), 2021 . . Generative Zero-Shot Learning for Semantic Segmentation of 3D Point Clouds . Bjoern Michele, Alexandre Boulch, Gilles Puy, Maxime Bucher, and Renaud Marlet International Conference on 3D Vision (3DV), 2021 . . Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning . Spyros Gidaris, Andrei Bursuc, Gilles Puy, Nikos Komodakis, Patrick Pérez, and Matthieu Cord Computer Vision and Pattern Recognition (CVPR), 2021 . . Semantic Palette: Guiding Scene Generation with Class Proportions . Guillaume Le Moing, Tuan-Hung Vu, Himalaya Jain, Patrick Pérez and Matthieu Cord Computer Vision and Pattern Recognition (CVPR), 2021 . . Artificial Dummies for Urban Dataset Augmentation . Antonín Vobecký, David Hurych, Michal Uřičář, Patrick Pérez, and Josef Šivic AAAI Conference on Artificial Intelligence (AAAI), 2021 . . Toward Unsupervised, Multi-Object Discovery in Large-Scale Image Collections . Huy V. Vo, Patrick Pérez and Jean Ponce European Conference on Computer Vision (ECCV), 2020 . . Learning Representations by Predicting Bags of Visual Words . Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pérez, and Matthieu Cord Computer Vision and Pattern Recognition (CVPR), 2020 . . This dataset does not exist: training models from generated images . Victor Besnier, Himalaya Jain, Andrei Bursuc, Matthieu Cord, and Patrick Pérez International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2020 . . Zero-Shot Semantic Segmentation . Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick Pérez Neural Information Processing Systems (NeurIPS), 2019 . . Boosting Few-Shot Visual Learning With Self-Supervision . Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pérez, and Matthieu Cord International Conference on Computer Vision (ICCV), 2019 . . Unsupervised Image Matching and Object Discovery as Optimization . Huy V. Vo, Francis Bach, Minsu Cho, Kai Han, Yann Lecun, Patrick Pérez and Jean Ponce Computer Vision and Pattern Recognition (CVPR), 2019 .",
          "url": "https://valeoai.github.io/blog/projects/limited-supervision",
          "relUrl": "/projects/limited-supervision",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "Domain adaptation",
          "content": "Domain adaptation . Deep learning and reinforcement learning are key technologies for autonomous driving. One of the challenges they face is to adapt to conditions which differ from those met during training. To improve systems’ performance in such situations, we explore so-called “domain adaptation” techniques, as in AdvEnt at CVPR’19 and DADA its extension at ICCV’19. We propose new solutions to more practical DA scenarios in MTAF (ICCV&#39;21) to handle multiple target domains and in BUDA (CVIU&#39;21) to handle new target classes. In xMUDA (CVPR&#39;20), we introduce a new framework to tackle the challenging adaptation problem on both 2D image and 3D point-cloud spaces. . Publications . Multi-Head Distillation for Continual Unsupervised Domain Adaptation in Semantic Segmentation . Antoine Saporta, Arthur Douillard, Tuan-Hung Vu, Patrick Pérez and Matthieu Cord Computer Vision and Pattern Recognition (CVPR) Workshop, 2022 . . Cross-modal Learning for Domain Adaptation in 3D Semantic Segmentation . Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Émilie Wirbel, and Patrick Pérez IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022 . . Handling new target classes in semantic segmentation with domain adaptation . Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick Pérez Computer Vision and Image Understanding (CVIU), 2021 . . Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation . Antoine Saporta, Tuan-Hung Vu, Matthieu Cord and Patrick Pérez International Conference on Computer Vision (ICCV), 2021 . . Semantic Palette: Guiding Scene Generation with Class Proportions . Guillaume Le Moing, Tuan-Hung Vu, Himalaya Jain, Patrick Pérez and Matthieu Cord Computer Vision and Pattern Recognition (CVPR), 2021 . . Confidence Estimation via Auxiliary Models . Charles Corbière, Nicolas Thome, Antoine Saporta, Tuan-Hung Vu, Matthieu Cord, and Patrick Pérez IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021 . . ESL: Entropy-guided Self-supervised Learning for Domain Adaptation in Semantic Segmentation . Antoine Saporta, Tuan-Hung Vu, Matthieu Cord and Patrick Pérez Computer Vision and Pattern Recognition Workshop on Scalability in Autonomous Driving, 2020 . . xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation . Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Émilie Wirbel, and Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2020 . . DADA: Depth-aware Domain Adaptation in Semantic Segmentation . Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Pérez International Conference on Computer Vision (ICCV), 2019 . . AdvEnt: Adversarial Entropy minimization for domain adaptation in semantic segmentation . Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Pérez Computer Vision and Pattern Recognition (CVPR), 2019 .",
          "url": "https://valeoai.github.io/blog/projects/domain-adaptation",
          "relUrl": "/projects/domain-adaptation",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Reliability",
          "content": "Reliability . When the unexpected happens, when the weather badly degrades, when a sensor gets blocked, the embarked perception system should diagnose the situation and react accordingly, e.g., by calling an alternative system or the human driver. With this in mind, we investigate ways to improve the robustness of neural nets to input variations, including to adversarial attacks, and to predict automatically the performance and the confidence of their predictions as in ConfidNet at NeurIPS’19. . Publications . Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation . Victor Besnier, Andrei Bursuc, Alexandre Briot, and David Picard International Conference on Computer Vision (ICCV), 2021 . . StyleLess layer: Improving robustness for real-world driving . Julien Rebut, Andrei Bursuc, and Patrick Pérez IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021 . . Confidence Estimation via Auxiliary Models . Charles Corbière, Nicolas Thome, Antoine Saporta, Tuan-Hung Vu, Matthieu Cord, and Patrick Pérez IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021 . . TRADI: Tracking deep neural network weight distributions . Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson, and Isabelle Bloch European Conference on Computer Vision (ECCV), 2020 . . Addressing Failure Prediction by Learning Model Confidence . Charles Corbière, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez Neural Information Processing Systems (NeurIPS), 2019 .",
          "url": "https://valeoai.github.io/blog/projects/reliability",
          "relUrl": "/projects/reliability",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "Driving in action",
          "content": "Driving in action . Getting from sensory inputs to car control goes either through a modular stack (perception &gt; localization &gt; forecast &gt; planning &gt; actuation) or, more radically, through a single end-to-end model. We work on both strategies, more specificaly on action forecasting, automatic interpretation of decisions taken by a driving system, and reinforcement / imitation learning for end-to-end systems (as in RL work at CVPR’20). . Publications . STEEX: Steering Counterfactual Explanations with Semantics . Paul Jacob, Éloi Zablocki, Hédi Ben-Younes, Mickaël Chen, Patrick Pérez, Matthieu Cord European Conference on Computer Vision, 2022 . . Explainability of deep vision-based autonomous driving systems: Review and challenges . Éloi Zablocki*, Hédi Ben-Younes*, Patrick Pérez, Matthieu Cord International Journal of Computer Vision, 2022 . . Driving behavior explanation with multi-level fusion . Hédi Ben-Younes*, Éloi Zablocki*, Patrick Pérez, Matthieu Cord Pattern Recognition (PR), 2021 . . PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving . Thibault Buhet, Emilie Wirbel, Andrei Bursuc and Xavier Perrotton Conference on Robot Learning (CoRL), 2020 . . End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances . Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde Computer Vision and Pattern Recognition (CVPR), 2020 . . VRUNet: Multi-Task Learning Model for Intent Prediction of Vulnerable Road Users . Adithya Ranga, Filippo Giruzzi, Jagdish Bhanushali, Emilie Wirbel, Patrick Pérez, Tuan-Hung Vu, Xavier Perotton Electronic Imaging, 2020 .",
          "url": "https://valeoai.github.io/blog/projects/driving",
          "relUrl": "/projects/driving",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "Core Deep Learning",
          "content": "Core Deep Learning . Deep learning being now a key component of AD systems, it is important to get a better understanding of its inner workings, in particular the link between the specifics of the learning optimization and the key properities (performance, regularity, robustness, generalization) of the trained models. Among other things, we investigate the impact of popular batch normalization on standard learning procedures and the ability to learn through unsupervised distillation. . Publications . QuEST: Quantized Embedding Space for Transferring Knowledge . Himalaya Jain, Spyros Gidaris, Nikos Komodakis, Patrick Pérez, and Matthieu Cord European Conference on Computer Vision (ECCV), 2020 .",
          "url": "https://valeoai.github.io/blog/projects/deep-learning",
          "relUrl": "/projects/deep-learning",
          "date": ""
      }
      
  



  
      ,"page0": {
          "title": "ConvPoint: Continuous Convolutions for Point Cloud Processing",
          "content": "ConvPoint: Continuous Convolutions for Point Cloud Processing . Alexandre Boulch &nbsp;&nbsp; . CaG 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; Slides&nbsp;&nbsp; . . Abstract . Point clouds are unstructured and unordered data, as opposed to images. Thus, most machine learning approach developed for image cannot be directly transferred to point clouds. In this paper, we propose a generalization of discrete convolutional neural networks (CNNs) in order to deal with point clouds by replacing discrete kernels by continuous ones. This formulation is simple, allows arbitrary point cloud sizes and can easily be used for designing neural networks similarly to 2D CNNs. We present experimental results with various architectures, highlighting the flexibility of the proposed approach. We obtain competitive results compared to the state-of-the-art on shape classification, part segmentation and semantic segmentation for large-scale point clouds. . . BibTeX . @article{boulch2020convpoint, title={ConvPoint: Continuous convolutions for point cloud processing}, author={Boulch, Alexandre}, journal={Computers &amp; Graphics}, year={2020}, publisher={Elsevier} } . .",
          "url": "https://valeoai.github.io/blog/publications/2020_convpoint/",
          "relUrl": "/publications/2020_convpoint/",
          "date": ""
      }
      
  

  
      ,"page1": {
          "title": "STaRFlow: A SpatioTemporal Recurrent Cell for Lightweight Multi-Frame Optical Flow Estimation",
          "content": "STaRFlow: A SpatioTemporal Recurrent Cell for Lightweight Multi-Frame Optical Flow Estimation . Pierre Godet &nbsp;&nbsp; Alexandre Boulch &nbsp;&nbsp; Aurelien Plyer &nbsp;&nbsp; Guy Le Besnerais&nbsp;&nbsp; . ICPR 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . We present a new lightweight CNN-based algorithm for multi-frame optical flow estimation. Our solution introduces a double recurrence over spatial scale and time through repeated use of a generic “STaR” (SpatioTemporal Recurrent) cell. It includes (i) a temporal recurrence based on conveying learned features rather than optical flow estimates; (ii) an occlusion detection process which is coupled with optical flow estimation and therefore uses a very limited number of extra parameters. The resulting STaRFlow algorithm gives state-of-the-art performances on MPI Sintel and Kitti2015 and involves significantly less parameters than all other methods with comparable results. . . BibTeX . @inproceedings{godet2021starflow, title={STaRFlow: A SpatioTemporal Recurrent Cell for Lightweight Multi-Frame Optical Flow Estimation}, author={Godet, Pierre and Boulch, Alexandre and Plyer, Aur{ &#39;e}lien and Le Besnerais, Guy}, booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, pages={2462--2469}, year={2021}, organization={IEEE} } . .",
          "url": "https://valeoai.github.io/blog/publications/2020_starflow/",
          "relUrl": "/publications/2020_starflow/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Generative Zero-Shot Learning for Semantic Segmentation of 3D Point Clouds",
          "content": "Generative Zero-Shot Learning for Semantic Segmentation of 3D Point Clouds . Bjoern Michele&nbsp;&nbsp; Alexandre Boulch&nbsp;&nbsp; Gilles Puy &nbsp;&nbsp; Maxime Bucher&nbsp;&nbsp; Renaud Marlet&nbsp;&nbsp; . 3DV 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; Slides&nbsp;&nbsp; . . Abstract . While there has been a number of studies on Zero-Shot Learning (ZSL) for 2D images, its application to 3D data is still recent and scarce, with just a few methods limited to classification. We present the first generative approach for both ZSL and Generalized ZSL (GZSL) on 3D data, that can handle both classification and, for the first time, semantic segmentation. We show that it reaches or outperforms the state of the art on ModelNet40 classification for both inductive ZSL and inductive GZSL. For semantic segmentation, we created three benchmarks for evaluating this new ZSL task, using S3DIS, ScanNet and SemanticKITTI. Our experiments show that our method outperforms strong baselines, which we additionally propose for this task. . . BibTeX . @inproceedings{michele2021generative, title={Generative Zero-Shot Learning for Semantic Segmentation of {3D} Point Cloud}, author={Michele, Bj{ &quot;o}rn and Boulch, Alexandre and Puy, Gilles and Bucher, Maxime and Marlet, Renaud}, booktitle={International Conference on 3D Vision (3DV)}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/3dgenz/",
          "relUrl": "/publications/3dgenz/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "Driving behavior explanation with multi-level fusion",
          "content": "Driving behavior explanation with multi-level fusion . Hédi Ben-Younes &nbsp;&nbsp; Éloi Zablocki &nbsp;&nbsp; Patrick Pérez &nbsp;&nbsp; Matthieu Cord . Pattern Recognition 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . In this era of active development of autonomous vehicles, it becomes crucial to provide driving systems with the capacity to explain their decisions. In this work, we focus on generating high-level driving explanations as the vehicle drives. We present BEEF, for BEhavior Explanation with Fusion, a deep architecture which explains the behavior of a trajectory prediction model. Supervised by annotations of human driving decisions justifications, BEEF learns to fuse features from multiple levels. Leveraging recent advances in the multi-modal fusion literature, BEEF is carefully designed to model the correlations between high-level decisions features and mid-level perceptual features. The flexibility and efficiency of our approach are validated with extensive experiments on the HDD and BDD-X datasets. . . BibTeX . @article{beef2021, author = {Hedi Ben{-}Younes and { &#39;{E}}loi Zablocki and Patrick P{ &#39;{e}}rez and Matthieu Cord}, title = {Driving Behavior Explanation with Multi-level Fusion}, journal = {Pattern Recognition (PR)}, year = {2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/beef/",
          "relUrl": "/publications/beef/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "NeeDrop: Unsupervised Shape Representation from Sparse Point Clouds using Needle Dropping",
          "content": "NeeDrop: Unsupervised Shape Representation from Sparse Point Clouds using Needle Dropping . Alexandre Boulch   Pierre-Alain Langlois    Gilles Puy    Renaud Marlet   &lt;/h3&gt; . 3DV 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . There has been recently a growing interest for implicit shape representations. Contrary to explicit representations, they have no resolution limitations and they easily deal with a wide variety of surface topologies. To learn these implicit representations, current approaches rely on a certain level of shape supervision (e.g., inside/outside information or distance-to-shape knowledge), or at least require a dense point cloud (to approximate well enough the distance-to-shape). In contrast, we introduce NeeDrop, a self-supervised method for learning shape representations from possibly extremely sparse point clouds. Like in Buffon’s needle problem, we “drop” (sample) needles on the point cloud and consider that, statistically, close to the surface, the needle end points lie on opposite sides of the surface. No shape knowledge is required and the point cloud can be highly sparse, e.g., as lidar point clouds acquired by vehicles. Previous self-supervised shape representation approaches fail to produce good-quality results on this kind of data. We obtain quantitative results on par with existing supervised approaches on shape reconstruction datasets and show promising qualitative results on hard autonomous driving datasets such as KITTI. . . BibTeX . @inproceedings{boulch2021needrop, title={NeeDrop: Self-supervised Shape Representation from Sparse Point Clouds using Needle Dropping}, author={Boulch, Alexandre and Langlois, Pierre-Alain and Puy, Gilles and Marlet, Renaud}, booktitle={International Conference on 3D Vision (3DV)}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/needrop/",
          "relUrl": "/publications/needrop/",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation",
          "content": "Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation . Victor Besnier&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Alexandre Briot&nbsp;&nbsp; David Picard . ICCV 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . In this paper, we tackle the detection of out-of-distribution (OOD) objects in semantic segmentation. By analyzing the literature, we found that current methods are either accurate or fast but not both which limits their usability in real world applications. To get the best of both aspects, we propose to mitigate the common shortcomings by following four design principles: decoupling the OOD detection from the segmentation task, observing the entire segmentation network instead of just its output, generating training data for the OOD detector by leveraging blind spots in the segmentation network and focusing the generated data on localized regions in the image to simulate OOD objects. Our main contribution is a new OOD detection architecture called ObsNet associated with a dedicated training scheme based on Local Adversarial Attacks (LAA). We validate the soundness of our approach across numerous ablation studies. We also show it obtains top performances both in speed and accuracy when compared to ten recent methods of the literature on three different datasets. . . . BibTeX . @inproceedings{besnier2021triggering, title = {Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation}, author = {Besnier, Victor and Bursuc, Andrei and Picard, David and Briot Alexandre}, booktitle={Proceedings of the IEEE International Conference on Computer Vision}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/obsnet/",
          "relUrl": "/publications/obsnet/",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "StyleLess layer: Improving robustness for real-world driving",
          "content": "StyleLess layer: Improving robustness for real-world driving . Julien Rebut &nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Patrick Pérez . IROS 2021 . Paper&nbsp;&nbsp; . . Abstract . Deep Neural Networks (DNNs) are a critical component for self-driving vehicles. They achieve impressive performance by reaping information from high amounts of labeled data. Yet, the full complexity of the real world cannot be encapsulated in the training data, no matter how big the dataset, and DNNs can hardly generalize to unseen conditions. Robustness to various image corruptions, caused by changing weather conditions or sensor degradation and aging, is crucial for safety when such vehicles are deployed in the real world. We address this problem through a novel type of layer, dubbed StyleLess, which enables DNNs to learn robust and informative features that can cope with varying external conditions. We propose multiple variations of this layer that can be integrated in most of the architectures and trained jointly with the main task. We validate our contribution on typical autonomous-driving tasks (detection, semantic segmentation), showing that in most cases, this approach improves predictive performance on unseen conditions (fog, rain), while preserving performance on seen conditions and objects. . . BibTeX . @inproceedings{rebut2021styleless, title={StyleLess layer: Improving robustness for real-world driving}, author={Rebut, Julien and Bursuc, Andrei and P{ &#39;e}rez, Patrick}, booktitle={IROS}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/styleless/",
          "relUrl": "/publications/styleless/",
          "date": ""
      }
      
  

  
      ,"page7": {
          "title": "CSG0: Continual Urban Scene Generation with Zero Forgetting",
          "content": "CSG0: Continual Urban Scene Generation with Zero Forgetting . Himalaya Jain (*)&nbsp;&nbsp; Tuan-Hung Vu (*)&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Matthieu Cord (* equal contrib.) . CVPRW 2022 . Paper&nbsp;&nbsp; . . Abstract . With the rapid advances in generative adversarial networks (GANs), the visual quality of synthesised scenes keeps improving, including for complex urban scenes with applications to automated driving. We address in this work a continual scene generation setup in which GANs are trained on a stream of distinct domains; ideally, the learned models should eventually be able to generate new scenes in all seen domains. This setup reflects the real-life scenario where data are continuously acquired in different places at different times. In such a continual setup, we aim for learning with zero forgetting, i.e., with no degradation in synthesis quality over earlier domains due to catastrophic forgetting. To this end, we introduce a novel framework that not only (i) enables seamless knowledge transfer in continual training but also (ii) guarantees zero forgetting with a small overhead cost. While being more memory efficient, thanks to continual learning, our model obtains better synthesis quality as compared against the brute-force solution that trains one full model for each domain. Especially, under extreme low-data regimes, our approach outperforms the brute-force one by a large margin. . . BibTeX . @inproceedings{jain2022csg0, title={CSG0: Continual Urban Scene Generation with Zero Forgetting}, author={Jain, Himalaya and Vu, Tuan-Hung and P{ &#39;e}rez, Patrick and Cord, Matthieu}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshop}, year={2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/csg0/",
          "relUrl": "/publications/csg0/",
          "date": ""
      }
      
  

  
      ,"page8": {
          "title": "Diverse Probabilistic Trajectory Forecasting with Admissibility Constraints",
          "content": "Diverse Probabilistic Trajectory Forecasting with Admissibility Constraints . Laura Calem &nbsp;&nbsp; Hedi Ben-Younes &nbsp;&nbsp; Patrick Pérez &nbsp;&nbsp; Nicolas Thome . ICPR 2022 . . . Abstract . Predicting multiple trajectories for road users is important for driving automation systems: ego-vehicle motion planning indeed requires a clear view of the possible motions of the surrounding agents. However, the generative models used for multiple-trajectory forecasting suffer from a lack of diversity in their proposals. To avoid this form of collapse, we propose a novel method for structured prediction of diverse trajectories. To this end, we complement an underlying pretrained generative model with a diversity component, based on a determinantal point process (DPP). We balance and structure this diversity with the inclusion of knowledge-based quality constraints, independent from the underlying generative model. We combine these two novel components with a gating operation, ensuring that the predictions are both diverse and within the drivable area. We demonstrate on the nuScenes driving dataset the relevance of our compound approach, which yields significant improvements in the diversity and the quality of the generated trajectories. . . BibTeX . @inproceedings{calem22diva, title={Diverse Probabilistic Trajectory Forecasting with Admissibility Constraints}, author={Laura Calem and Hedi Ben{-}Younes and Patrick P{ &#39;{e}}rez and Nicolas Thome},, booktitle={International Conference on Pattern Recognition (ICPR)} year={2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/diva/",
          "relUrl": "/publications/diva/",
          "date": ""
      }
      
  

  
      ,"page9": {
          "title": "Multi-Head Distillation for Continual Unsupervised Domain Adaptation in Semantic Segmentation",
          "content": "Multi-Head Distillation for Continual Unsupervised Domain Adaptation in Semantic Segmentation . Antoine Saporta&nbsp;&nbsp; Arthur Douillard&nbsp;&nbsp; Tuan-Hung Vu&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Matthieu Cord . CVPRW 2022 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Unsupervised Domain Adaptation (UDA) is a transfer learning task which aims at training on an unlabeled target domain by leveraging a labeled source domain. Beyond the traditional scope of UDA with a single source domain and a single target domain, real-world perception systems face a variety of scenarios to handle, from varying lighting conditions to many cities around the world. In this context, UDAs with several domains increase the challenges with the addition of distribution shifts within the different target domains. This work focuses on a novel framework for learning UDA, continuous UDA, in which models operate on multiple target domains discovered sequentially, without access to previous target domains. We propose MuHDi, for Multi-Head Distillation, a method that solves the catastrophic forgetting problem, inherent in continual learning tasks. MuHDi performs distillation at multiple levels from the previous model as well as an auxiliary target-specialist segmentation head. We report both extensive ablation and experiments on challenging multi-target UDA semantic segmentation benchmarks to validate the proposed learning scheme and architecture. . . BibTeX . @inproceedings{saporta2022muhdi, title={Multi-Head Distillation for Continual Unsupervised Domain Adaptation in Semantic Segmentation}, author={Saporta, Antoine and Douillard, Arthur and Vu, Tuan-Hung and P{ &#39;e}rez, Patrick and Cord, Matthieu}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshop}, year={2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/muhdi/",
          "relUrl": "/publications/muhdi/",
          "date": ""
      }
      
  

  
      ,"page10": {
          "title": "POCO: Point Convolution for Surface Reconstruction",
          "content": "POCO: Point Convolution for Surface Reconstruction . Alexandre Boulch&nbsp;&nbsp;Renaud Marlet . CVPR 2022 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . . Abstract . Implicit neural networks have been successfully used for surface reconstruction from point clouds. However, many of them face scalability issues as they encode the isosurface function of a whole object or scene into a single latent vector. To overcome this limitation, a few approaches infer latent vectors on a coarse regular 3D grid or on 3D patches, and interpolate them to answer occupancy queries. In doing so, they loose the direct connection with the input points sampled on the surface of objects, and they attach information uniformly in space rather than where it matters the most, i.e., near the surface. Besides, relying on fixed patch sizes may require discretization tuning. To address these issues, we propose to use point cloud convolutions and compute latent vectors at each input point. We then perform a learning-based interpolation on nearest neighbors using inferred weights. Experiments on both object and scene datasets show that our approach significantly outperforms other methods on most classical metrics, producing finer details and better reconstructing thinner volumes. &lt;/a&gt; . . . BibTeX . @inproceedings{boulch2022poco, title={POCO: Point Convolution for Surface Reconstruction}, author={Boulch, Alexandre and Marlet, Renaud}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, year={2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/poco/",
          "relUrl": "/publications/poco/",
          "date": ""
      }
      
  

  
      ,"page11": {
          "title": "Raw High-Definition Radar for Multi-Task Learning",
          "content": "Raw High-Definition Radar for Multi-Task Learning . Julien Rebut&nbsp;&nbsp;Arthur Ouaknine&nbsp;&nbsp;Waqas Walik&nbsp;&nbsp;Patrick Pérez . CVPR 2022 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . . Abstract . With their robustness to adverse weather conditions and ability to measure speeds, radar sensors have been part of the automotive landscape for more than two decades. Recent progress toward High Definition (HD) Imaging radar has driven the angular resolution below the degree, thus approaching laser scanning performance. However, the amount of data a HD radar delivers and the computational cost to estimate the angular positions remain a challenge. In this paper, we propose a novel HD radar sensing model, FFT-RadNet, that eliminates the overhead of computing the range-azimuth-Doppler 3D tensor, learning instead to recover angles from a range-Doppler spectrum. FFT-RadNet is trained both to detect vehicles and to segment free driving space. On both tasks, it competes with the most recent radar-based models while requiring less compute and memory. Also, we collected and annotated 2-hour worth of raw data from synchronized automotive-grade sensors (camera, laser, HD radar) in various environments (city street, highway, countryside road). This unique dataset, nick-named RADIal for &quot;Radar, Lidar et al.&quot;, is available at this https URL. . . . BibTeX . @inproceedings{rebut2022radial, title={Raw High-Definition Radar for Multi-Task Learning}, author={Rebut, Julien and Ouaknine, Arthur and Malik, Waqas and P{ &#39;e}rez, Patrick}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, year={2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/radial/",
          "relUrl": "/publications/radial/",
          "date": ""
      }
      
  

  
      ,"page12": {
          "title": "STEEX: Steering Counterfactual Explanations with Semantics",
          "content": "STEEX: Steering Counterfactual Explanations with Semantics . Paul Jacob &nbsp;&nbsp; Éloi Zablocki &nbsp;&nbsp; Hédi Ben-Younes &nbsp;&nbsp; Mickaël Chen &nbsp;&nbsp; Patrick Pérez &nbsp;&nbsp; Matthieu Cord . ECCV 2022 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . As deep learning models are increasingly used in safety-critical applications, explainability and trustworthiness become major concerns. For simple images, such as low-resolution face portraits, synthesizing visual counterfactual explanations has recently been proposed as a way to uncover the decision mechanisms of a trained classification model. In this work, we address the problem of producing counterfactual explanations for high-quality images and complex scenes. Leveraging recent semantic-to-image models, we propose a new generative counterfactual explanation framework that produces plausible and sparse modifications which preserve the overall scene structure. Furthermore, we introduce the concept of &quot;region-targeted counterfactual explanations&quot;, and a corresponding framework, where users can guide the generation of counterfactuals by specifying a set of semantic regions of the query image the explanation must be about. Extensive experiments are conducted on challenging datasets including high-quality portraits (CelebAMask-HQ) and driving scenes (BDD100k). . . BibTeX . @inproceedings{steex2022, author = {Paul Jacob and { &#39;{E}}loi Zablocki and Hedi Ben{-}Younes and Micka{ &quot;{e}}l Chen and Patrick P{ &#39;{e}}rez and Matthieu Cord}, title = {STEEX: Steering Counterfactual Explanations with Semantics}, booktitle = {ECCV 2020}, publisher = {Springer}, year = {2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/steex/",
          "relUrl": "/publications/steex/",
          "date": ""
      }
      
  

  
      ,"page13": {
          "title": "Explainability of deep vision-based autonomous driving systems: Review and challenges",
          "content": "Explainability of deep vision-based autonomous driving systems: Review and challenges . Éloi Zablocki &nbsp;&nbsp; Hédi Ben-Younes &nbsp;&nbsp; Patrick Pérez &nbsp;&nbsp; Matthieu Cord . IJCV 2022 . Paper&nbsp;&nbsp; Blog &nbsp;&nbsp; . . Abstract . This survey reviews explainability methods for vision-based self-driving systems trained with behavior cloning. The concept of explainability has several facets and the need for explainability is strong in driving, a safety-critical application. Gathering contributions from several research fields, namely computer vision, deep learning, autonomous driving, explainable AI (X-AI), this survey tackles several points. First, it discusses definitions, context, and motivation for gaining more interpretability and explainability from self-driving systems, as well as the challenges that are specific to this application. Second, methods providing explanations to a black-box self-driving system in a post-hoc fashion are comprehensively organized and detailed. Third, approaches from the literature that aim at building more interpretable self-driving systems by design are presented and discussed in detail. Finally, remaining open-challenges and potential future research directions are identified and examined. . . BibTeX . @article{xai-driving-survey-2022, author = loi Zablocki and Hedi Ben{-}Younes and Patrick P{ &#39;{e}}rez and Matthieu Cord}, title = {Explainability of deep vision-based autonomous driving systems: Review and challenges}, journal = {IJCV}, year = {2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/xai-survey/",
          "relUrl": "/publications/xai-survey/",
          "date": ""
      }
      
  

  
      ,"page14": {
          "title": "Cross-modal Learning for Domain Adaptation in 3D Semantic Segmentation",
          "content": "Cross-modal Learning for Domain Adaptation in 3D Semantic Segmentation . Maximilian Jaritz&nbsp;&nbsp; Tuan-Hung Vu&nbsp;&nbsp; Raoul de Charette&nbsp;&nbsp; Émilie Wirbel&nbsp;&nbsp; Patrick Pérez . T-PAMI 2022 . Paper&nbsp;&nbsp; . . Abstract . Domain adaptation is an important task to enable learning when labels are scarce. While most works focus only on the image modality, there are many important multi-modal datasets. In order to leverage multi-modality for domain adaptation, we propose cross-modal learning, where we enforce consistency between the predictions of two modalities via mutual mimicking. We constrain our network to make correct predictions on labeled data and consistent predictions across modalities on unlabeled target-domain data. Experiments in unsupervised and semi-supervised domain adaptation settings prove the effectiveness of this novel domain adaptation strategy. Specifically, we evaluate on the task of 3D semantic segmentation from either the 2D image, the 3D point cloud or from both. We leverage recent driving datasets to produce a wide variety of domain adaptation scenarios including changes in scene layout, lighting, sensor setup and weather, as well as the synthetic-to-real setup. Our method significantly improves over previous uni-modal adaptation baselines on all adaption scenarios. Code will be made available upon publication. . . BibTeX . @article{jaritz2022xmossda, title={Cross-modal Learning for Domain Adaptation in 3D Semantic Segmentation}, author={Jaritz, Maximilian and Vu, Tuan-Hung and Charette, Raoul de and Wirbel, Emilie and P{ &#39;e}rez, Patrick}, journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, year={2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/xmossda/",
          "relUrl": "/publications/xmossda/",
          "date": ""
      }
      
  

  
      ,"page15": {
          "title": "AdvEnt: Adversarial Entropy minimization for domain adaptation in semantic segmentation",
          "content": "AdvEnt: Adversarial Entropy minimization for domain adaptation in semantic segmentation . Tuan-Hung Vu&nbsp;&nbsp; Himalaya Jain&nbsp;&nbsp; Maxime Bucher&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . CVPR 2019 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; Blog &nbsp;&nbsp; . . Abstract . Semantic segmentation is a key problem for many computer vision tasks. While approaches based on convolutional neural networks constantly break new records on different benchmarks, generalizing well to diverse testing environments remains a major challenge. In numerous real world applications, there is indeed a large gap between data distributions in train and test domains, which results in severe performance loss at run-time. In this work, we address the task of unsupervised domain adaptation in semantic segmentation with losses based on the entropy of the pixel-wise predictions. To this end, we propose two novel, complementary methods using (i) an entropy loss and (ii) an adversarial loss respectively. We demonstrate state-of-the-art performance in semantic segmentation on two challenging synthetic-2-real set-ups and show that the approach can also be used for detection. . . Video . . . . BibTeX . @inproceedings{vu2018advent, title={ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation}, author={Vu, Tuan-Hung and Jain, Himalaya and Bucher, Maxime and Cord, Mathieu and P{ &#39;e}rez, Patrick}, booktitle={CVPR}, year={2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/advent/",
          "relUrl": "/publications/advent/",
          "date": ""
      }
      
  

  
      ,"page16": {
          "title": "Boosting Few-Shot Visual Learning With Self-Supervision",
          "content": "Boosting Few-Shot Visual Learning With Self-Supervision . Spyros Gidaris&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Nikos Komodakis&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Matthieu Cord . ICCV 2019 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Few-shot learning and self-supervised learning address different facets of the same problem: how to train a model with little or no labeled data. Few-shot learning aims for optimization methods and models that can learn efficiently to recognize patterns in the low data regime. Self-supervised learning focuses instead on unlabeled data and looks into it for the supervisory signal to feed high capacity deep neural networks. In this work we exploit the complementarity of these two domains and propose an approach for improving few-shot learning through self-supervision. We use self-supervision as an auxiliary task in a few-shot learning pipeline, enabling feature extractors to learn richer and more transferable visual representations while still using few annotated samples. Through self-supervision, our approach can be naturally extended towards using diverse unlabeled data from other datasets in the few-shot setting. We report consistent improvements across an array of architectures, datasets and self-supervision techniques. We provide the implementation code at: https://github.com/valeoai/BF3S . . . BibTeX . @inproceedings{gidaris2019boosting, title={Boosting few-shot visual learning with self-supervision}, author={Gidaris, Spyros and Bursuc, Andrei and Komodakis, Nikos and P{ &#39;e}rez, Patrick and Cord, Matthieu}, booktitle={Proceedings of the IEEE International Conference on Computer Vision}, pages={8059--8068}, year={2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/bf3s/",
          "relUrl": "/publications/bf3s/",
          "date": ""
      }
      
  

  
      ,"page17": {
          "title": "Learning Representations by Predicting Bags of Visual Words",
          "content": "Learning Representations by Predicting Bags of Visual Words . Spyros Gidaris&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Nikos Komodakis&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Matthieu Cord . CVPR 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Self-supervised representation learning targets to learn convnet-based image representations from unlabeled data. Inspired by the success of NLP methods in this area, in this work we propose a self-supervised approach based on spatially dense image descriptions that encode discrete visual concepts, here called visual words. To build such discrete representations, we quantize the feature maps of a first pre-trained self-supervised convnet, over a k-means based vocabulary. Then, as a self-supervised task, we train another convnet to predict the histogram of visual words of an image (i.e., its Bag-of-Words representation) given as input a perturbed version of that image. The proposed task forces the convnet to learn perturbation-invariant and context-aware image features, useful for downstream image understanding tasks. We extensively evaluate our method and demonstrate very strong empirical results, e.g., our pre-trained self-supervised representations transfer better on detection task and similarly on classification over classes &quot;unseen&quot; during pre-training, when compared to the supervised case. This also shows that the process of image discretization into visual words can provide the basis for very powerful self-supervised approaches in the image domain, thus allowing further connections to be made to related methods from the NLP domain that have been extremely successful so far. . . . BibTeX . @inproceedings{gidaris2020learning, title={Learning Representations by Predicting Bags of Visual Words}, author={Gidaris, Spyros and Bursuc, Andrei and Komodakis, Nikos and P{ &#39;e}rez, Patrick and Cord, Matthieu}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={6928--6938}, year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/bownet/",
          "relUrl": "/publications/bownet/",
          "date": ""
      }
      
  

  
      ,"page18": {
          "title": "Handling new target classes in semantic segmentation with domain adaptation",
          "content": "Handling new target classes in semantic segmentation with domain adaptation . Maxime Bucher&nbsp;&nbsp; Tuan-Hung Vu&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . CVIU 2021 . Paper&nbsp;&nbsp; . . Abstract . In this work, we define and address a novel domain adaptation (DA) problem in semantic scene segmentation, where the target domain not only exhibits a data distribution shift w.r.t. the source domain, but also includes novel classes that do not exist in the latter. Different to &quot;open-set&quot; and &quot;universal domain adaptation&quot;, which both regard all objects from new classes as &quot;unknown&quot;, we aim at explicit test-time prediction for these new classes. To reach this goal, we propose a framework that leverages domain adaptation and zero-shot learning techniques to enable &quot;boundless&quot; adaptation in the target domain. It relies on a novel architecture, along with a dedicated learning scheme, to bridge the source-target domain gap while learning how to map new classes&#39; labels to relevant visual representations. The performance is further improved using self-training on target-domain pseudo-labels. For validation, we consider different domain adaptation set-ups, namely synthetic-2-real, country-2-country and dataset-2-dataset. Our framework outperforms the baselines by significant margins, setting competitive standards on all benchmarks for the new task. . . . BibTeX . @inproceedings{bucher2021buda, title={Handling new target classes in semantic segmentation with domain adaptation}, author={Bucher, Maxime and Tuan-Hung, VU and Cord, Matthieu and P{ &#39;e}rez, Patrick}, journal={Computer Vision and Image Understanding}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/buda/",
          "relUrl": "/publications/buda/",
          "date": ""
      }
      
  

  
      ,"page19": {
          "title": "CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations",
          "content": "CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations . Arthur Ouaknine&nbsp;&nbsp;Alasdair Newson&nbsp;&nbsp; Julien Rebut&nbsp;&nbsp; Florence Tupin&nbsp;&nbsp; Patrick Pérez . ICPR 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; Blog &nbsp;&nbsp; . . Abstract . High quality perception is essential for autonomous driving (AD) systems. To reach the accuracy and robustness that are required by such systems, several types of sensors must be combined. Currently, mostly cameras and laser scanners (lidar) are deployed to build a representation of the world around the vehicle. While radar sensors have been used for a long time in the automotive industry, they are still under-used for AD despite their appealing characteristics (notably, their ability to measure the relative speed of obstacles and to operate even in adverse weather conditions). To a large extent, this situation is due to the relative lack of automotive datasets with real radar signals that are both raw and annotated. In this work, we introduce CARRADA, a dataset of synchronized camera and radar recordings with range-angle-Doppler annotations. We also present a semi-automatic annotation approach, which was used to annotate the dataset, and a radar semantic segmentation baseline, which we evaluate on several metrics. . . BibTeX . @INPROCEEDINGS{9413181, author={Ouaknine, Arthur and Newson, Alasdair and Rebut, Julien and Tupin, Florence and Pérez, Patrick}, booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, title={CARRADA Dataset: Camera and Automotive Radar with Range- Angle- Doppler Annotations}, year={2021}, pages={5068-5075}, doi={10.1109/ICPR48806.2021.9413181} } . .",
          "url": "https://valeoai.github.io/blog/publications/carrada/",
          "relUrl": "/publications/carrada/",
          "date": ""
      }
      
  

  
      ,"page20": {
          "title": "Addressing Failure Prediction by Learning Model Confidence",
          "content": "Addressing Failure Prediction by Learning Model Confidence . Charles Corbière&nbsp;&nbsp; Nicolas Thome&nbsp;&nbsp; Avner Bar-Hen&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . NeurIPS 2019 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; BibTeX&nbsp;&nbsp; . . Abstract . Assessing reliably the confidence of a deep neural net and predicting its failures is of primary importance for the practical deployment of these models. In this paper, we propose a new target criterion for model confidence, corresponding to the True Class Probability (TCP). We show how using the TCP is more suited than relying on the classic Maximum Class Probability (MCP). We provide in addition theoretical guarantees for TCP in the context of failure prediction. Since the true class is by essence unknown at test time, we propose to learn TCP criterion on the training set, introducing a specific learning scheme adapted to this context. Extensive experiments are conducted for validating the relevance of the proposed approach. We study various network architectures, small and large scale datasets for image classification and semantic segmentation. We show that our approach consistently outperforms several strong methods, from MCP to Bayesian uncertainty, as well as recent approaches specifically designed for failure prediction. . . BibTeX . @incollection{NIPS2019_8556, title = {Addressing Failure Prediction by Learning Model Confidence}, author = {Corbi `{e}re, Charles and THOME, Nicolas and Bar-Hen, Avner and Cord, Matthieu and &#39;{e}rez, Patrick}, booktitle = {Advances in Neural Information Processing Systems 32}, pages = {2902--2913}, year = {2019}, } . .",
          "url": "https://valeoai.github.io/blog/publications/confidnet/",
          "relUrl": "/publications/confidnet/",
          "date": ""
      }
      
  

  
      ,"page21": {
          "title": "Confidence Estimation via Auxiliary Models",
          "content": "Confidence Estimation via Auxiliary Models . Charles Corbière&nbsp;&nbsp; Nicolas Thome&nbsp;&nbsp; Antoine Saporta&nbsp;&nbsp; Tuan-Hung Vu&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . TPAMI 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; BibTeX&nbsp;&nbsp; . . Abstract . Reliably quantifying the confidence of deep neural classifiers is a challenging yet fundamental requirement for deployingsuch models in safety-critical applications. In this paper, we introduce a novel target criterion for model confidence, namely the true class probability (TCP). We show that TCP offers better properties for confidence estimation than standard maximum class probability (MCP). Since the true class is by essence unknown at test time, we propose to learn TCP criterion from data with an auxiliary model, introducing a specific learning scheme adapted to this context. We evaluate our approach on the task of failure prediction and ofself-training with pseudo-labels for domain adaptation, which both necessitate effective confidence estimates. Extensive experiments are conducted for validating the relevance of the proposed approach in each task. We study various network architectures andexperiment with small and large datasets for image classification and semantic segmentation. In every tested benchmark, our approach outperforms strong baselines . . BibTeX . @article{corbiere2021confidence, author={Corbiere, Charles and Thome, Nicolas and Saporta, Antoine and Vu, Tuan-Hung and Cord, Matthieu and Perez, Patrick}, journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, title={Confidence Estimation via Auxiliary Models}, year={2021}, volume={}, number={}, pages={1-1}, doi={10.1109/TPAMI.2021.3085983}} } . .",
          "url": "https://valeoai.github.io/blog/publications/confidnet_conda/",
          "relUrl": "/publications/confidnet_conda/",
          "date": ""
      }
      
  

  
      ,"page22": {
          "title": "DADA: Depth-aware Domain Adaptation in Semantic Segmentation",
          "content": "DADA: Depth-aware Domain Adaptation in Semantic Segmentation . Tuan-Hung Vu&nbsp;&nbsp; Himalaya Jain&nbsp;&nbsp; Maxime Bucher&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . ICCV 2019 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Unsupervised domain adaptation (UDA) is important for applications where large scale annotation of representative data is challenging. For semantic segmentation in particular, it helps deploy on real &quot;target domain&quot; data models that are trained on annotated images from a different &quot;source domain&quot;, notably a virtual environment. To this end, most previous works consider semantic segmentation as the only mode of supervision for source domain data, while ignoring other, possibly available, information like depth. In this work, we aim at exploiting at best such a privileged information while training the UDA model. We propose a unified depth-aware UDA framework that leverages in several complementary ways the knowledge of dense depth in the source domain. As a result, the performance of the trained semantic segmentation model on the target domain is boosted. Our novel approach indeed achieves state-of-the-art performance on different challenging synthetic-2-real benchmarks. . . . BibTeX . @inproceedings{vu2019dada, title={Dada: Depth-aware domain adaptation in semantic segmentation}, author={Vu, Tuan-Hung and Jain, Himalaya and Bucher, Maxime and Cord, Matthieu and P{ &#39;e}rez, Patrick}, booktitle={Proceedings of the IEEE International Conference on Computer Vision}, pages={7364--7373}, year={2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/dada/",
          "relUrl": "/publications/dada/",
          "date": ""
      }
      
  

  
      ,"page23": {
          "title": "Artificial Dummies for Urban Dataset Augmentation",
          "content": "Artificial Dummies for Urban Dataset Augmentation . Antonín Vobecký&nbsp;&nbsp; David Hurych&nbsp;&nbsp; Michal Uříčář&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Josef Šivic . AAAI 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Existing datasets for training pedestrian detectors in images suffer from limited appearance and pose variation. The most challenging scenarios are rarely included because they are too difficult to capture due to safety reasons, or they are very unlikely to happen. The strict safety requirements in assisted and autonomous driving applications call for an extra high detection accuracy also in these rare situations. Having the ability to generate people images in arbitrary poses, with arbitrary appearances and embedded in different background scenes with varying illumination and weather conditions, is a crucial component for the development and testing of such applications. The contributions of this paper are three-fold. First, we describe an augmentation method for controlled synthesis of urban scenes containing people, thus producing rare or neverseen situations. This is achieved with a data generator (called DummyNet) with disentangled control of the pose, the appearance, and the target background scene. Second, the proposed generator relies on novel network architecture and associated loss that takes into account the segmentation of the foreground person and its composition into the background scene. Finally, we demonstrate that the data generated by our DummyNet improve performance of several existing person detectors across various datasets as well as in challenging situations, such as night-time conditions, where only a limited amount of training data is available. In the setup with only day-time data available, we improve the night-time detector by 17% log-average miss rate over the detector trained with the day-time data only . . . BibTeX . @inproceedings{vobecky2021artificial, title={Artificial Dummies for Urban Dataset Augmentation}, author={Vobeck{ &#39;y}, Anton{ &#39;i}n and Hurych, David and U{ vr}i{ vc}{ &#39;a}{ vr}, Michal and P{ &#39;e}rez, Patrick and Sivic, Josef}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, pages={0--0}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/dummynet/",
          "relUrl": "/publications/dummynet/",
          "date": ""
      }
      
  

  
      ,"page24": {
          "title": "Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems",
          "content": "Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems . Isabelle Liang&nbsp;&nbsp; Ganesh Sistu&nbsp;&nbsp; Fabian Burger&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Senthil Yogamani&nbsp;&nbsp; . ITSC 2020 . Paper&nbsp;&nbsp; . . Abstract . Deep multi-task networks are of particular interest for autonomous driving systems. They can potentially strike an excellent trade-off between predictive performance, hardware constraints and efficient use of information from multiple types of annotations and modalities. However, training such models is non-trivial and requires balancing learning over all tasks as their respective losses display different scales, ranges and dynamics across training. Multiple task weighting methods that adjust the losses in an adaptive way have been proposed recently on different datasets and combinations of tasks, making it difficult to compare them. In this work, we review and systematically evaluate nine task weighting strategies on common grounds on three automotive datasets (KITTI, Cityscapes and WoodScape). We then propose a novel method combining evolutionary meta-learning and task-based selective backpropagation, for computing task weights leading to reliable network training. Our method outperforms state-of-the-art methods by a significant margin on a two-task application. . . . Results . . Comparison of various task-weighting methods for two-task network training. . Task weights and asynchronous backpropagation frequencies computed by several task-weighting methods. . BibTeX . @article{leang2020dynamic, title={Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems}, author={Leang, Isabelle and Sistu, Ganesh and Burger, Fabian and Bursuc, Andrei and Yogamani, Senthil}, journal={arXiv preprint arXiv:2001.02223}, year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/dynamic-mtl/",
          "relUrl": "/publications/dynamic-mtl/",
          "date": ""
      }
      
  

  
      ,"page25": {
          "title": "End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances",
          "content": "End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances . Marin Toromanoff&nbsp;&nbsp; Emilie Wirbel&nbsp;&nbsp; Fabien Moutarde . CVPR 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Reinforcement Learning (RL) aims at learning an optimal behavior policy from its own experiments and not rule-based control methods. However, there is no RL algorithm yet capable of handling a task as difficult as urban driving. We present a novel technique, coined implicit affordances, to effectively leverage RL for urban driving thus including lane keeping, pedestrians and vehicles avoidance, and traffic light detection. To our knowledge we are the first to present a successful RL agent handling such a complex task especially regarding the traffic light detection. Furthermore, we have demonstrated the effectiveness of our method by winning the Camera Only track of the CARLA challenge. . . . Video . . . . BibTeX . @inproceedings{toromanoff2020end, title={End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances}, author={Toromanoff, Marin and Wirbel, Emilie and Moutarde, Fabien}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={7153--7162}, year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/e2e-rl-driving/",
          "relUrl": "/publications/e2e-rl-driving/",
          "date": ""
      }
      
  

  
      ,"page26": {
          "title": "ESL: Entropy-guided Self-supervised Learning for Domain Adaptation in Semantic Segmentation",
          "content": "ESL: Entropy-guided Self-supervised Learning for Domain Adaptation in Semantic Segmentation . Antoine Saporta&nbsp;&nbsp;Tuan-Hung Vu&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . CVPRW 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . While fully-supervised deep learning yields good models for urban scene semantic segmentation, these models struggle to generalize to new environments with different lighting or weather conditions for instance. In addition, producing the extensive pixel-level annotations that the task requires comes at a great cost. Unsupervised domain adaptation (UDA) is one approach that tries to address these issues in order to make such systems more scalable. In particular, self-supervised learning (SSL) has recently become an effective strategy for UDA in semantic segmentation. At the core of such methods liespseudo-labeling&#39;, that is, the practice of assigning high-confident class predictions as pseudo-labels, subsequently used as true labels, for target data. To collect pseudo-labels, previous works often rely on the highest softmax score, which we here argue as an unfavorable confidence measurement. . . BibTeX . @inproceedings{saporta2020esl, title={Esl: Entropy-guided self-supervised learning for domain adaptation in semantic segmentation}, author={Saporta, Antoine and Vu, Tuan-Hung and Cord, Matthieu and P{ &#39;e}rez, Patrick}, booktitle={CVPRW}, year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/esl/",
          "relUrl": "/publications/esl/",
          "date": ""
      }
      
  

  
      ,"page27": {
          "title": "FKAConv: Feature-Kernel Alignment for Point Cloud Convolutiont",
          "content": "FKAConv: Feature-Kernel Alignment for Point Cloud Convolutiont . Alexandre Boulch &nbsp;&nbsp; Gilles Puy &nbsp;&nbsp; Renaud Marlet&nbsp;&nbsp; . ACCV 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Recent state-of-the-art methods for point cloud processing are based on the notion of point convolution, for which several approaches have been proposed. In this paper, inspired by discrete convolution in image processing, we provide a formulation to relate and analyze a number of point convolution methods. We also propose our own convolution variant, that separates the estimation of geometry-less kernel weights and their alignment to the spatial support of features. Additionally, we define a point sampling strategy for convolution that is both effective and fast. Finally, using our convolution and sampling strategy, we show competitive results on classification and semantic segmentation benchmarks while being time and memory efficient. . . BibTeX . @inproceedings{boulch2020fka, title={FKAConv: Feature-Kernel Alignment for Point Cloud Convolutions}, author={Boulch, Alexandre and Puy, Gilles and Marlet, Renaud}, booktitle={15th Asian Conference on Computer Vision (ACCV 2020)}, year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/fkaconv/",
          "relUrl": "/publications/fkaconv/",
          "date": ""
      }
      
  

  
      ,"page28": {
          "title": "FLOT: Scene Flow on Point Clouds guided by Optimal Transport",
          "content": "FLOT: Scene Flow on Point Clouds guided by Optimal Transport . Gilles Puy &nbsp;&nbsp; Alexandre Boulch &nbsp;&nbsp; Renaud Marlet . ECCV 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . We propose and study a method called FLOT that estimates scene flow on point clouds. We start the design of FLOT by noticing that scene flow estimation on point clouds reduces to estimating a permutation matrix in a perfect world. Inspired by recent works on graph matching, we build a method to find these correspondences by borrowing tools from optimal transport. Then, we relax the transport constraints to take into account real-world imperfections. The transport cost between two points is given by the pairwise similarity between deep features extracted by a neural network trained under full supervision using synthetic datasets. Our main finding is that FLOT can perform as well as the best existing methods on synthetic and real-world datasets while requiring much less parameters and without using multiscale analysis. Our second finding is that, on the training datasets considered, most of the performance can be explained by the learned transport cost. This yields a simpler method, FLOT0, which is obtained using a particular choice of optimal transport parameters and performs nearly as well as FLOT. . . BibTeX . @inproceedings{puy20flot, title={FLOT: Scene Flow on Point Clouds Guided by Optimal Transport}, author={Puy, Gilles and Boulch, Alexandre and Marlet, Renaud}, booktitle={European Conference on Computer Vision} year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/flot/",
          "relUrl": "/publications/flot/",
          "date": ""
      }
      
  

  
      ,"page29": {
          "title": "This dataset does not exist: training models from generated images",
          "content": "This dataset does not exist: training models from generated images . Victor Besnier&nbsp;&nbsp; Himalaya Jain&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . ICASSP 2020 . Paper&nbsp;&nbsp; . . Abstract . Current generative networks are increasingly proficient in generating high-resolution realistic images. These generative networks, especially the conditional ones, can potentially become a great tool for providing new image datasets. This naturally brings the question: Can we train a classifier only on the generated data? This potential availability of nearly unlimited amounts of training data challenges standard practices for training machine learning models, which have been crafted across the years for limited and fixed size datasets. In this work we investigate this question and its related challenges. We identify ways to improve significantly the performance over naive training on randomly generated images with regular heuristics. We propose three standalone techniques that can be applied at different stages of the pipeline, i.e., data generation, training on generated data, and deploying on real data. We evaluate our proposed approaches on a subset of the ImageNet dataset and show encouraging results compared to classifiers trained on real images. . . . Results . . Effect of applying HSM at different iterations during classifier training. The first image of each category is sampled randomly. The other two images are generated from HSM-computed codes at two different steps during training. The difference between the images shows that the effect of HSM is specific to the classifier. . Results for ImageNet-10 real test images. Performance of classifiers trained on generated images with all combinations of the proposed methods. Each classifier is trained for $150$ epochs (except Long training, where we let DS run for $150$ epochs) over a set of $N = 13K$ images; in case of continuous sampling we replace $50 %$ (i.e., $6,500$) of the images every epoch, while fixed dataset is the usual setup where no images are replaced during training. In all setups we use $N$ images per epoch. First column, without applying any of the proposed methods, is the baseline. Each of the proposed methods individually shows improvement over the baseline. The combination of the methods further improves the results. Effect of replacement fraction $r$ in DS. Classification accuracy using DS on real images with varying $r$, i.e., fraction of the dataset being replaced with new images every epoch. The figure shows plots for DS with and without BNA. . . BibTeX . @inproceedings{besnier2020dataset, title={This dataset does not exist: training models from generated images}, author={Besnier, Victor and Jain, Himalaya and Bursuc, Andrei and Cord, Matthieu and P{ &#39;e}rez, Patrick}, booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages={1--5}, year={2020}, organization={IEEE} } . .",
          "url": "https://valeoai.github.io/blog/publications/gan-dataset/",
          "relUrl": "/publications/gan-dataset/",
          "date": ""
      }
      
  

  
      ,"page30": {
          "title": "Large-Scale Unsupervised Object Discovery",
          "content": "Large-Scale Unsupervised Object Discovery . Huy V. Vo&nbsp;&nbsp; Elena Sizikova&nbsp;&nbsp; Cordelia Schmid&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Jean Ponce . NeurIPS 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Existing approaches to unsupervised object discovery (UOD) do not scale up to large datasets without approximations that compromise their performance. We propose a novel formulation of UOD as a ranking problem, amenable to the arsenal of distributed methods available for eigenvalue problems and link analysis. Through the use of self-supervised features, we also demonstrate the first effective fully unsupervised pipeline for UOD. Extensive experiments on COCO [42] and OpenImages [35] show that, in the single-object discovery setting where a single prominent object is sought in each image, the proposed LOD (Large-scale Object Discovery) approach is on par with, or better than the state of the art for mediumscale datasets (up to 120K images), and over 37% better than the only other algorithms capable of scaling up to 1.7M images. In the multi-object discovery setting where multiple objects are sought in each image, the proposed LOD is over 14% better in average precision (AP) than all other methods for datasets ranging from 20K to 1.7M images. Using self-supervised features, we also show that the proposed method obtains state-of-the-art UOD performance on OpenImages. . . . BibTeX . @inproceedings{Vo21LOD, title = {Large-Scale Unsupervised Object Discovery}, author = {Vo, Huy V. and Sizikova, Elena and Schmid, Cordelia and P{ &#39;e}rez, Patrick and Ponce, Jean}, booktitle = {Advances in Neural Information Processing Systems 34 ({NeurIPS})} year = {2021}, } . .",
          "url": "https://valeoai.github.io/blog/publications/lod/",
          "relUrl": "/publications/lod/",
          "date": ""
      }
      
  

  
      ,"page31": {
          "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
          "content": "Localizing Objects with Self-Supervised Transformers and no Labels . Oriane Siméoni &nbsp;&nbsp; Gilles Puy &nbsp;&nbsp; Huy V. Vo &nbsp;&nbsp; Simon Roburin &nbsp;&nbsp; Spyros Gidaris &nbsp;&nbsp; Andrei Bursuc &nbsp;&nbsp; Patrick Pérez &nbsp;&nbsp; Renaud Marlet &nbsp;&nbsp; Jean Ponce . BMVC 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Localizing objects in image collections without supervision can help to avoid expensive annotation campaigns. We propose a simple approach to this problem, that leverages the activation features of a vision transformer pre-trained in a self-supervised manner. Our method, LOST, does not require any external object proposal nor any exploration of the image collection; it operates on a single image. Yet, we outperform state-of-the-art object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. We also show that training a class-agnostic detector on the discovered objects boosts results by another 7 points. Moreover, we show promising results on the unsupervised object discovery task. . . BibTeX . @inproceedings{LOST, title = {Localizing Objects with Self-Supervised Transformers and no Labels}, author = {Oriane Sim &#39;eoni and Gilles Puy and Huy V. Vo and Simon Roburin and Spyros Gidaris and Andrei Bursuc and Patrick P &#39;erez and Renaud Marlet and Jean Ponce}, journal = {Proceedings of the British Machine Vision Conference (BMVC)}, month = {November}, year = {2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/lost/",
          "relUrl": "/publications/lost/",
          "date": ""
      }
      
  

  
      ,"page32": {
          "title": "Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation",
          "content": "Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation . Antoine Saporta&nbsp;&nbsp;Tuan-Hung Vu&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . ICCV 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . In this work, we address the task of unsupervised domain adaptation (UDA) for semantic segmentation in presence of multiple target domains: The objective is to train a single model that can handle all these domains at test time. Such a multi-target adaptation is crucial for a variety of scenarios that real-world autonomous systems must handle. It is a challenging setup since one faces not only the domain gap between the labeled source set and the unlabeled target set, but also the distribution shifts existing within the latter among the different target domains. To this end, we introduce two adversarial frameworks: (i) multi-discriminator, which explicitly aligns each target domain to its counterparts, and (ii) multi-target knowledge transfer, which learns a target-agnostic model thanks to a multi-teacher/single-student distillation mechanism.The evaluation is done on four newly-proposed multi-target benchmarks for UDA in semantic segmentation. In all tested scenarios, our approaches consistently outperform baselines, setting competitive standards for the novel task. . . BibTeX . @inproceedings{saporta2021mtaf, title={Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation}, author={Saporta, Antoine and Vu, Tuan-Hung and Cord, Mathieu and P{ &#39;e}rez, Patrick}, booktitle={ICCV}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/mtaf/",
          "relUrl": "/publications/mtaf/",
          "date": ""
      }
      
  

  
      ,"page33": {
          "title": "Multi-View Radar Semantic Segmentation",
          "content": "Multi-View Radar Semantic Segmentation . Arthur Ouaknie&nbsp;&nbsp;Alasdair Newson&nbsp;&nbsp;Patrick Pérez&nbsp;&nbsp; Florence Tupin&nbsp;&nbsp; Julien Rebut . ICCV 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; Blog &nbsp;&nbsp; . . Abstract . Understanding the scene around the ego-vehicle is key to assisted and autonomous driving. Nowadays, this is mostly conducted using cameras and laser scanners, despite their reduced performances in adverse weather conditions. Automotive radars are low-cost active sensors that measure properties of surrounding objects, including their relative speed, and have the key advantage of not being impacted by rain, snow or fog. However, they are seldom used for scene understanding due to the size and complexity of radar raw data and the lack of annotated datasets. Fortunately, recent open-sourced datasets have opened up research on classification, object detection and semantic segmentation with raw radar signals using end-to-end trainable models. In this work, we propose several novel architectures, and their associated losses, which analyse multiple &quot;views&quot; of the range-angle-Doppler radar tensor to segment it semantically. Experiments conducted on the recent CARRADA dataset demonstrate that our best model outperforms alternative models, derived either from the semantic segmentation of natural images or from radar scene understanding, while requiring significantly fewer parameters. . . BibTeX . @inproceedings{ouaknine_2021_multi-view, title={Multi-View Radar Semantic Segmentation}, author={Ouaknine, Arthur and Alasdair, Newson and P{ &#39;e}rez, Patrick and Tupin, Florence and Rebut, Julien}, booktitle={ICCV}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/mvrss/",
          "relUrl": "/publications/mvrss/",
          "date": ""
      }
      
  

  
      ,"page34": {
          "title": "Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning",
          "content": "Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning . Spyros Gidaris&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Gilles Puy&nbsp;&nbsp; Nikos Komodakis Patrick Pérez&nbsp;&nbsp; Matthieu Cord . CVPR 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . . Abstract . Learning image representations without human supervision is an important and active research field. Several recent approaches have successfully leveraged the idea of making such a representation invariant under different types of perturbations, especially via contrastive-based instance discrimination training. Although effective visual representations should indeed exhibit such invariances, there are other important characteristics, such as encoding contextual reasoning skills, for which alternative reconstruction-based approaches might be better suited. With this in mind, we propose a teacher-student scheme to learn representations by training a convnet to reconstruct a bag-of-visual-words (BoW) representation of an image, given as input a perturbed version of that same image. Our strategy performs an online training of both the teacher network (whose role is to generate the BoW targets) and the student network (whose role is to learn representations), along with an online update of the visual-words vocabulary (used for the BoW targets). This idea effectively enables fully online BoW-guided unsupervised learning. Extensive experiments demonstrate the interest of our BoW-based strategy which surpasses previous state-of-the-art methods (including contrastive-based ones) in several applications. For instance, in downstream tasks such Pascal object detection, Pascal classification and Places205 classification, our method improves over all prior unsupervised approaches, thus establishing new state-of-the-art results that are also significantly better even than those of supervised pre-training. . . . Results . . Evaluation of ImageNet pre-trained ResNet50 models. The &#39;&#39;Epochs&#39;&#39; and &#39;&#39;Batch&#39;&#39; columns provide the number of pre-training epochs and the batch size of each model respectively. The first section includes models pre-trained with a similar number of epochs as our model (second section). We boldfaced the best results among all sections as well as of only the top two. For the linear classification tasks, we provide the top-1 accuracy. For object detection, we fine-tuned Faster R-CNN (R50-C4) on VOC $ texttt{trainval07+12}$ and report detection AP scores by testing on $ texttt{test07}$. For semi-supervised learning, we fine-tune the pre-trained models on $1 %$ and $10 %$ of ImageNet and report the top-5 accuracy. Note that, in this case the &#39;&#39;Supervised&#39;&#39; entry results come from (Zhai et al.) and are obtained by supervised training using only $1 %$ or $10 %$ of the labelled data. All the classification results are computed with single-crop testing. $^ dagger$: results computed by us. . BibTeX . @inproceedings{gidaris2021obow, title={Learning Representations by Predicting Bags of Visual Words}, author={Gidaris, Spyros and Bursuc, Andrei and Puy, Gilles and Komodakis, Nikos and Cord, Matthieu and P{ &#39;e}rez, Patrick}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/obow/",
          "relUrl": "/publications/obow/",
          "date": ""
      }
      
  

  
      ,"page35": {
          "title": "Unsupervised Image Matching and Object Discovery as Optimization",
          "content": "Unsupervised Image Matching and Object Discovery as Optimization . Huy V. Vo&nbsp;&nbsp; Francis Bach&nbsp;&nbsp; Minsu Cho&nbsp;&nbsp; Kai Han&nbsp;&nbsp; Yann LeCun&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Jean Ponce . CVPR 2019 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Learning with complete or partial supervision is power- ful but relies on ever-growing human annotation efforts. As a way to mitigate this serious problem, as well as to serve specific applications, unsupervised learning has emerged as an important field of research. In computer vision, unsu- pervised learning comes in various guises. We focus here on the unsupervised discovery and matching of object cate- gories among images in a collection, following the work of Cho et al. [12]. We show that the original approach can be reformulated and solved as a proper optimization problem. Experiments on several benchmarks establish the merit of our approach. . . . BibTeX . @inproceedings{Vo19UOD, title = {Unsupervised image matching and object discovery as optimization}, author = {Vo, Huy V. and Bach, Francis and Cho, Minsu and Han, Kai and LeCun, Yann and P{ &#39;e}rez, Patrick and Ponce, Jean}, booktitle = {CVPR}, year = {2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/osd/",
          "relUrl": "/publications/osd/",
          "date": ""
      }
      
  

  
      ,"page36": {
          "title": "PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds",
          "content": "PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds . Anh-Quan Cao &nbsp;&nbsp; Gilles Puy &nbsp;&nbsp; Alexandre Boulch &nbsp;&nbsp; Renaud Marlet . ICCV 2021 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Rigid registration of point clouds with partial overlaps is a longstanding problem usually solved in two steps: (a) finding correspondences between the point clouds; (b) filtering these correspondences to keep only the most reliable ones to estimate the transformation. Recently, several deep nets have been proposed to solve these steps jointly. We built upon these works and propose PCAM: a neural network whose key element is a pointwise product of cross-attention matrices that permits to mix both low-level geometric and high-level contextual information to find point correspondences. These cross-attention matrices also permits the exchange of context information between the point clouds, at each layer, allowing the network construct better matching features within the overlapping regions. The experiments show that PCAM achieves state-of-the-art results among methods which, like us, solve steps (a) and (b) jointly via deepnets. . . BibTeX . @inproceedings{cao21pcam, title={PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds}, author={Cao, Anh-Quan and Puy, Gilles and Boulch, Alexandre and Marlet, Renaud}, booktitle={International Conference on Computer Vision}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/pcam/",
          "relUrl": "/publications/pcam/",
          "date": ""
      }
      
  

  
      ,"page37": {
          "title": "PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving",
          "content": "PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving . Thibault Buhet &nbsp;&nbsp; Emilie Wirbel&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Xavier Perrotton . CoRL 2020 . Paper&nbsp;&nbsp; . . Abstract . To navigate safely in urban environments, an autonomous vehicle (*ego vehicle*) must understand and anticipate its surroundings, in particular the behavior and intents of other road users (*neighbors*). Most of the times, multiple decision choices are acceptable for all road users (e.g., turn right or left, or different ways of avoiding an obstacle), leading to a highly uncertain and multi-modal decision space. We focus here on predicting multiple feasible future trajectories for both ego vehicle and neighbors through a probabilistic framework. We rely on a conditional imitation learning algorithm, conditioned by a navigation command for the ego vehicle (e.g., *turn right*). Our model processes ego vehicle front-facing camera images and bird-eye view grid, computed from Lidar point clouds, with detections of past and present objects, in order to generate multiple trajectories for both ego vehicle and its neighbors. Our approach is computationally efficient and relies only on on-board sensors. We evaluate our method offline on the publicly available dataset nuScenes, achieving state-of-the-art performance, investigate the impact of our architecture choices on online simulated experiments and show preliminary insights for real vehicle control. . . . Results . Comparison on nuScenes 4s prediction with published results of DESIRE-plan, ESP and PRECOG from (Rhinehart et al., 2019) (results from their Table II, with a fixed 5 agents training), over minMSD metric. . Closed loop error locations for urban and track test data (internal data), visualized for PLOP and constant velocity baseline. We note that braking behind a vehicle can induce multiple high speed errors and stack multiple red dots on the same location. Points of interest (traffic lights, roundabout, stop signs) are highlighted on the map. . Video . . . . BibTeX . @article{buhet2020plop, title={PLOP: Probabilistic poLynomial Objects trajectory Planning for autonomous driving}, author={Buhet, Thibault and Wirbel, Emilie and Bursuc, Andrei and Perrotton, Xavier}, journal={Conference on Robot Learning (CoRL)}, year={2020} } . . . References . Rhinehart, N., McAllister, R., Kitani, K., &amp; Levine, S. (2019). Precog: Prediction conditioned on goals in visual multi-agent settings. Iccv. |",
          "url": "https://valeoai.github.io/blog/publications/plop/",
          "relUrl": "/publications/plop/",
          "date": ""
      }
      
  

  
      ,"page38": {
          "title": "QuEST: Quantized Embedding Space for Transferring Knowledge",
          "content": "QuEST: Quantized Embedding Space for Transferring Knowledge . Himalaya Jain&nbsp;&nbsp; Spyros Gidaris&nbsp;&nbsp; Nikos Komodakis&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Matthieu Cord . ECCV 2020 . Paper&nbsp;&nbsp; . . Abstract . Knowledge distillation refers to the process of training a student network to achieve better accuracy by learning from a pre-trained teacher network. Most of the existing knowledge distillation methods direct the student to follow the teacher by matching the teacher&#39;s output, feature maps or their distribution. In this work, we propose a novel way to achieve this goal: by distilling the knowledge through a quantized visual words space. According to our method, the teacher&#39;s feature maps are first quantized to represent the main visual concepts (i.e., visual words) encompassed in these maps and then the student is asked to predict those visual word representations. Despite its simplicity, we show that our approach is able to yield results that improve the state of the art on knowledge distillation for model compression and transfer learning scenarios. To that end, we provide an extensive evaluation across several network architectures and most commonly used benchmark datasets. . . . Video . . . . BibTeX . @article{jain2019quest, title={QUEST: Quantized embedding space for transferring knowledge}, author={Jain, Himalaya and Gidaris, Spyros and Komodakis, Nikos and P{ &#39;e}rez, Patrick and Cord, Matthieu}, journal={arXiv preprint arXiv:1912.01540}, year={2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/quest/",
          "relUrl": "/publications/quest/",
          "date": ""
      }
      
  

  
      ,"page39": {
          "title": "Toward Unsupervised, Multi-Object Discovery in Large-Scale Image Collections",
          "content": "Toward Unsupervised, Multi-Object Discovery in Large-Scale Image Collections . Huy V. Vo&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Jean Ponce . ECCV 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . This paper addresses the problem of discovering the objects present in a collection of images without any supervision. We build on the optimization approach of Vo et al. [34] with several key novelties: (1) We propose a novel saliency-based region proposal algorithm that achieves significantly higher overlap with ground-truth objects than other competitive methods. This procedure leverages off-the-shelf CNN features trained on classification tasks without any bounding box information, but is otherwise unsupervised. (2) We exploit the inherent hierarchical structure of proposals as an effective regularizer for the approach to object discovery of [34], boosting its performance to significantly improve over the state of the art on several standard benchmarks. (3) We adopt a two-stage strategy to select promising proposals using small random sets of images before using the whole image collection to discover the objects it depicts, allowing us to tackle, for the first time (to the best of our knowledge), the discovery of multiple objects in each one of the pictures making up datasets with up to 20,000 images, an over five-fold increase compared to existing methods, and a first step toward true large-scale unsupervised image interpretation. . . . BibTeX . @inproceedings{Vo20rOSD, title = {Toward unsupervised, multi-object discovery in large-scale image collections}, author = {Vo, Huy V. and P{ &#39;e}rez, Patrick and Ponce, Jean}, booktitle = {Proceedings of the European Conference on Computer Vision ({ECCV})}, year = {2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/rosd/",
          "relUrl": "/publications/rosd/",
          "date": ""
      }
      
  

  
      ,"page40": {
          "title": "Semantic Palette: Guiding Scene Generation with Class Proportions ",
          "content": "Semantic Palette: Guiding Scene Generation with Class Proportions . Guillaume Le Moing&nbsp;&nbsp; Tuan-Hung Vu&nbsp;&nbsp; Himalaya Jain&nbsp;&nbsp; Patrick Pérez &nbsp;&nbsp; Matthieu Cord . CVPR 2021 . . . Abstract . Despite the recent progress of generative adversarial networks (GANs) at synthesizing photo-realistic images, producing complex urban scenes remains a challenging problem. Previous works break down scene generation into two consecutive phases: unconditional semantic layout synthesis and image synthesis conditioned on layouts. In this work, we propose to condition layout generation as well for higher semantic control: given a vector of class proportions, we generate layouts with matching composition. To this end, we introduce a conditional framework with novel architecture designs and learning objectives, which effectively accommodates class proportions to guide the scene generation process. The proposed architecture also allows partial layout editing with interesting applications. Thanks to the semantic control, we can produce layouts close to the real distribution, helping enhance the whole scene generation process. On different metrics and urban scene benchmarks, our models outperform existing baselines. Moreover, we demonstrate the merit of our approach for data augmentation: semantic segmenters trained on real layout-image pairs along with additional ones generated by our approach outperform models only trained on real pairs. . . Video . . . . BibTeX . @inproceedings{lemoing2021semanticpalette, title={Semantic Palette: Guiding Scene Generation with Class Proportions}, author={Le Moing, Guillaume and Vu, Tuan-Hung and Jain, Himalaya and P{ &#39;e}rez, Patrick and Cord, Mathieu}, booktitle={CVPR}, year={2021} } . .",
          "url": "https://valeoai.github.io/blog/publications/semanticpalette/",
          "relUrl": "/publications/semanticpalette/",
          "date": ""
      }
      
  

  
      ,"page41": {
          "title": "Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data",
          "content": "Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data . Corentin Sautier &nbsp;&nbsp; Gilles Puy &nbsp;&nbsp; Spyros Gidaris &nbsp;&nbsp; Alexandre Boulch &nbsp;&nbsp; Andrei Bursuc &nbsp;&nbsp; Renaud Marlet . CVPR 2022 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Segmenting or detecting objects in sparse Lidar point clouds are two important tasks in autonomous driving to allow a vehicle to act safely in its 3D environment. The best performing methods in 3D semantic segmentation or object detection rely on a large amount of annotated data. Yet annotating 3D Lidar data for these tasks is tedious and costly. In this context, we propose a self-supervised pre-training method for 3D perception models that is tailored to autonomous driving data. Specifically, we leverage the availability of synchronized and calibrated image and Lidar sensors in autonomous driving setups for distilling self-supervised pre-trained image representations into 3D models. Hence, our method does not require any point cloud nor image annotations. The key ingredient of our method is the use of superpixels which are used to pool 3D point features and 2D pixel features in visually similar regions. We then train a 3D network on the self-supervised task of matching these pooled point features with the corresponding pooled image pixel features. The advantages of contrasting regions obtained by superpixels are that: (1) grouping together pixels and points of visually coherent regions leads to a more meaningful contrastive task that produces features well adapted to 3D semantic segmentation and 3D object detection; (2) all the different regions have the same weight in the contrastive loss regardless of the number of 3D points sampled in these regions; (3) it mitigates the noise produced by incorrect matching of points and pixels due to occlusions between the different sensors. Extensive experiments on autonomous driving datasets demonstrate the ability of our image-to-Lidar distillation strategy to produce 3D representations that transfer well on semantic segmentation and object detection tasks. . . BibTeX . @inproceedings{sautier22slidr, title={Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data}, author={Corentin Sautier and Gilles Puy and Spyros Gidaris and Alexandre Boulch and Andrei Bursuc and Renaud Marlet}, booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)} year={2022} } . .",
          "url": "https://valeoai.github.io/blog/publications/slidr/",
          "relUrl": "/publications/slidr/",
          "date": ""
      }
      
  

  
      ,"page42": {
          "title": "TRADI: Tracking deep neural network weight distributions",
          "content": "TRADI: Tracking deep neural network weight distributions . Gianni Franchi&nbsp;&nbsp; Andrei Bursuc&nbsp;&nbsp; Emanuel Aldea&nbsp;&nbsp; Severine Dubuisson&nbsp;&nbsp; Isabelle Bloch . ECCV 2020 . Paper&nbsp;&nbsp; . . Abstract . During training, the weights of a Deep Neural Network (DNN) are optimized from a random initialization towards a nearly optimum value minimizing a loss function. Only this final state of the weights is typically kept for testing, while the wealth of information on the geometry of the weight space, accumulated over the descent towards the minimum is discarded. In this work we propose to make use of this knowledge and leverage it for computing the distributions of the weights of the DNN. This can be further used for estimating the epistemic uncertainty of the DNN by sampling an ensemble of networks from these distributions. To this end we introduce a method for tracking the trajectory of the weights during optimization, that does not require any changes in the architecture nor on the training procedure. We evaluate our method on standard classification and regression benchmarks, and on out-of-distribution detection for classification and semantic segmentation. We achieve competitive results, while preserving computational efficiency in comparison to other popular approaches. . . . Results . . Results on a synthetic regression task comparing MC dropout, Deep Ensembles, and TRADI. $x$-axis: spatial coordinate of the Gaussian process. Black lines: ground truth curve. Blue points: training points. Orange areas: estimated variance. . Distinguishing in- and out-of-distribution data for semantic segmentation (CamVid, StreetHazards, BDD Anomaly) and image classification (MNIST/notMNIST). . Qualitative results on CamVid-OOD. Columns: $(a)$ input image and ground truth; $(b)-(d)$ predictions and confidence scores by MC Dropout, Deep Ensembles, and TRADI. Rows: $(1)$ input and confidence maps; $(2)$ class predictions; $(3)$ zoomed-in area on input and confidence maps . BibTeX . @article{franchi2019tradi, title={TRADI: Tracking deep neural network weight distributions}, author={Franchi, Gianni and Bursuc, Andrei and Aldea, Emanuel and Dubuisson, S{ &#39;e}verine and Bloch, Isabelle}, journal={arXiv preprint arXiv:1912.11316}, year={2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/tradi/",
          "relUrl": "/publications/tradi/",
          "date": ""
      }
      
  

  
      ,"page43": {
          "title": "VRUNet: Multi-Task Learning Model for Intent Prediction of Vulnerable Road Users",
          "content": "VRUNet: Multi-Task Learning Model for Intent Prediction of Vulnerable Road Users . Adithya Ranga&nbsp;&nbsp; Filippo Giruzzi&nbsp;&nbsp; Jagdish Bhanushali&nbsp;&nbsp; Emilie Wirbel&nbsp;&nbsp; Patrick Pérez&nbsp;&nbsp; Tuan-Hung Vu&nbsp;&nbsp; Xavier Perotton&nbsp;&nbsp; . Electronic Imaging 2020 . Paper&nbsp;&nbsp; . . Abstract . Advanced perception and path planning are at the core for any self-driving vehicle. Autonomous vehicles need to understand the scene and intentions of other road users for safe motion planning. For urban use cases it is very important to perceive and predict the intentions of pedestrians, cyclists, scooters, etc., classified as vulnerable road users (VRU). Intent is a combination of pedestrian activities and long term trajectories defining their future motion. In this paper we propose a multi-task learning model to predict pedestrian actions, crossing intent and forecast their future path from video sequences. We have trained the model on naturalistic driving open-source JAAD [1] dataset, which is rich in behavioral annotations and real world scenarios. Experimental results show state-of-the-art performance on JAAD dataset and how we can benefit from jointly learning and predicting actions and trajectories using 2D human pose features and scene context. . . BibTeX . @article{ranga2020vrunet, title={VRUNet: Multi-Task Learning Model for Intent Prediction of Vulnerable Road Users}, author={Ranga, Adithya and Giruzzi, Filippo and Bhanushali, Jagdish and Wirbel, Emilie and P{ &#39;e}rez, Patrick and Vu, Tuan-Hung and Perotton, Xavier}, journal={Electronic Imaging}, year={2020}} . .",
          "url": "https://valeoai.github.io/blog/publications/vrunet/",
          "relUrl": "/publications/vrunet/",
          "date": ""
      }
      
  

  
      ,"page44": {
          "title": "xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation",
          "content": "xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation . Maximilian Jaritz&nbsp;&nbsp; Tuan-Hung Vu&nbsp;&nbsp; Raoul de Charette&nbsp;&nbsp; Émilie Wirbel&nbsp;&nbsp; Patrick Pérez . CVPR 2020 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Unsupervised Domain Adaptation (UDA) is crucial to tackle the lack of annotations in a new domain. There are many multi-modal datasets, but most UDA approaches are uni-modal. In this work, we explore how to learn from multi-modality and propose cross-modal UDA (xMUDA) where we assume the presence of 2D images and 3D point clouds for 3D semantic segmentation. This is challenging as the two input spaces are heterogeneous and can be impacted differently by domain shift. In xMUDA, modalities learn from each other through mutual mimicking, disentangled from the segmentation objective, to prevent the stronger modality from adopting false predictions from the weaker one. We evaluate on new UDA scenarios including day-to-night, country-to-country and dataset-to-dataset, leveraging recent autonomous driving datasets. xMUDA brings large improvements over uni-modal UDA on all tested scenarios, and is complementary to state-of-the-art UDA techniques. Code is available at: https://github.com/valeoai/xmuda . . Video . . . . BibTeX . @inproceedings{jaritz2020xmuda, title={xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation}, author={Jaritz, Maximilian and Vu, Tuan-Hung and Charette, Raoul de and Wirbel, Emilie and P{ &#39;e}rez, Patrick}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={12605--12614}, year={2020} } . .",
          "url": "https://valeoai.github.io/blog/publications/xmuda/",
          "relUrl": "/publications/xmuda/",
          "date": ""
      }
      
  

  
      ,"page45": {
          "title": "Zero-Shot Semantic Segmentation",
          "content": "Zero-Shot Semantic Segmentation . Maxime Bucher&nbsp;&nbsp; Tuan-Hung Vu&nbsp;&nbsp; Matthieu Cord&nbsp;&nbsp; Patrick Pérez . NeurIPS 2019 . Paper&nbsp;&nbsp; Code &nbsp;&nbsp; . . Abstract . Semantic segmentation models are limited in their ability to scale to large numbers of object classes. In this paper, we introduce the new task of zero-shot semantic segmentation: learning pixel-wise classifiers for never-seen object categories with zero training examples. To this end, we present a novel architecture, ZS3Net, combining a deep visual segmentation model with an approach to generate visual representations from semantic word embeddings. By this way, ZS3Net addresses pixel classification tasks where both seen and unseen categories are faced at test time (so called &quot;generalized&quot; zero-shot classification). Performance is further improved by a self-training step that relies on automatic pseudo-labeling of pixels from unseen classes. On the two standard segmentation datasets, Pascal-VOC and Pascal-Context, we propose zero-shot benchmarks and set competitive baselines. For complex scenes as ones in the Pascal-Context dataset, we extend our approach by using a graph-context encoding to fully leverage spatial context priors coming from class-wise segmentation maps. . . . BibTeX . @inproceedings{bucher2019zero, title={Zero-shot semantic segmentation}, author={Bucher, Maxime and Tuan-Hung, VU and Cord, Matthieu and P{ &#39;e}rez, Patrick}, booktitle={Advances in Neural Information Processing Systems}, pages={468--479}, year={2019} } . .",
          "url": "https://valeoai.github.io/blog/publications/zs3/",
          "relUrl": "/publications/zs3/",
          "date": ""
      }
      
  


}